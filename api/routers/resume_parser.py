"""
ç®€å†è§£æè·¯ç”±
ä½¿ç”¨LangChainå’ŒPyPDF2å®ç°çœŸæ­£çš„PDFç®€å†è§£æåŠŸèƒ½
"""
import os
import logging
import tempfile
from typing import Dict, List, Optional
from fastapi import APIRouter, UploadFile, File, HTTPException, BackgroundTasks
from fastapi.responses import JSONResponse, FileResponse
from pydantic import BaseModel
import asyncio
from datetime import datetime
import uuid

# LangChain imports
from langchain_community.document_loaders import PyPDFLoader, UnstructuredPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_core.documents import Document

# è®¯é£æ˜Ÿç«æ¨¡å‹
from src.models.spark_client import SparkLLM

# æ–‡æœ¬å¤„ç†å·¥å…·
import re
import json
from typing import Dict, List, Optional, Tuple

# æŒä¹…åŒ–ç®¡ç†å™¨
from src.persistence.optimal_manager import persistence_manager

logger = logging.getLogger(__name__)
router = APIRouter()

# åˆå§‹åŒ–æ˜Ÿç«å®¢æˆ·ç«¯
spark_client = SparkLLM()

class ResumeAnalysisRequestLegacy(BaseModel):
    """ç®€å†åˆ†æè¯·æ±‚æ¨¡å‹ï¼ˆé—ç•™PDFä¸Šä¼ ï¼‰"""
    domain: str
    position: str
    experience: str

class ResumeAnalysisResponse(BaseModel):
    """ç®€å†åˆ†æå“åº”æ¨¡å‹"""
    success: bool
    message: str
    data: Optional[Dict] = None
    task_id: Optional[str] = None

class ResumeAnalysisResult(BaseModel):
    """ç®€å†åˆ†æç»“æœæ¨¡å‹"""
    basic_info: Dict
    skills: List[str]
    experience: List[Dict]
    projects: List[Dict]
    education: Dict
    analysis: Dict

class ResumeCreateRequest(BaseModel):
    """ç®€å†åˆ›å»ºè¯·æ±‚æ¨¡å‹"""
    version_name: str
    target_position: str
    template_type: str
    basic_info: Dict
    education: Dict
    projects: List[Dict]
    skills: Dict
    internship: Optional[List[Dict]] = []

class ResumeCreateResponse(BaseModel):
    """ç®€å†åˆ›å»ºå“åº”æ¨¡å‹"""
    success: bool
    message: str
    data: Optional[Dict] = None
    resume_id: Optional[str] = None

# å­˜å‚¨åˆ†æä»»åŠ¡çš„çŠ¶æ€
analysis_tasks = {}

# åˆ›å»ºæ•°æ®å­˜å‚¨ç›®å½•
DATA_DIR = "data/analysis_results"
RESUME_DIR = "data/resumes"
ANALYSIS_DIR = "data/resume_analysis"
os.makedirs(DATA_DIR, exist_ok=True)
os.makedirs(RESUME_DIR, exist_ok=True)
os.makedirs(ANALYSIS_DIR, exist_ok=True)

def preprocess_resume_text(text: str) -> str:
    """é¢„å¤„ç†ç®€å†æ–‡æœ¬ï¼Œæé«˜åˆ†ææ•ˆç‡"""
    # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
    text = re.sub(r'\s+', ' ', text)
    
    # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ä½†ä¿ç•™ä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­—å’ŒåŸºæœ¬æ ‡ç‚¹
    text = re.sub(r'[^\w\s\u4e00-\u9fff.,;:!?()ï¼ˆï¼‰\-â€”â€“â€”]', '', text)
    
    # æ ‡å‡†åŒ–æ¢è¡Œç¬¦
    text = text.replace('\n', ' ').replace('\r', ' ')
    
    # ç§»é™¤å¤šä½™çš„ç©ºæ ¼
    text = re.sub(r' +', ' ', text).strip()
    
    return text

def extract_contact_info(text: str) -> Dict[str, str]:
    """ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼å¿«é€Ÿæå–è”ç³»æ–¹å¼"""
    contact_info = {}
    
    # æå–é‚®ç®±
    email_pattern = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
    email_match = re.search(email_pattern, text)
    if email_match:
        contact_info['email'] = email_match.group()
    
    # æå–æ‰‹æœºå·
    phone_pattern = r'1[3-9]\d{9}|(?:\d{3,4}-)?\d{7,8}'
    phone_match = re.search(phone_pattern, text)
    if phone_match:
        contact_info['phone'] = phone_match.group()
    
    return contact_info

def analyze_jd_matching(resume_data: Dict, jd_content: str = "") -> Dict:
    """JDæ™ºèƒ½åŒ¹é…åˆ†æ"""
    
    # æ„å»ºç®€å†æ–‡æœ¬
    resume_text = f"""
    å§“åï¼š{resume_data.get('basic_info', {}).get('name', '')}
    ç›®æ ‡èŒä½ï¼š{resume_data.get('target_position', '')}
    æŠ€èƒ½ï¼š{', '.join(resume_data.get('skills', {}).get('programmingLanguages', []) + 
                     resume_data.get('skills', {}).get('frontend', []) +
                     resume_data.get('skills', {}).get('backend', []))}
    é¡¹ç›®ç»éªŒï¼š{'; '.join([f"{p.get('name', '')}: {p.get('description', '')}" for p in resume_data.get('projects', [])])}
    å®ä¹ ç»å†ï¼š{'; '.join([f"{i.get('company', '')} - {i.get('position', '')}" for i in resume_data.get('internship', [])])}
    """
    
    prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ‹›è˜ä¸“å®¶ï¼Œè¯·å¯¹ä»¥ä¸‹ç®€å†å’ŒèŒä½æè¿°è¿›è¡Œæ™ºèƒ½åŒ¹é…åˆ†æã€‚

ç®€å†ä¿¡æ¯ï¼š
{resume_text}

èŒä½æè¿°ï¼ˆJDï¼‰ï¼š
{jd_content if jd_content else "ç®—æ³•å·¥ç¨‹å¸ˆå²—ä½ï¼Œè¦æ±‚ç†Ÿç»ƒæŒæ¡Pythonã€æœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ç­‰æŠ€æœ¯"}

è¯·è¿”å›JSONæ ¼å¼çš„åŒ¹é…åˆ†æç»“æœï¼š
{{
    "overall_match": 85,
    "skill_match": 90,
    "experience_match": 80,
    "project_relevance": 88,
    "education_match": 85,
    "strengths": ["ä¼˜åŠ¿1", "ä¼˜åŠ¿2", "ä¼˜åŠ¿3"],
    "gaps": ["æŠ€èƒ½ç¼ºå£1", "æŠ€èƒ½ç¼ºå£2"],
    "suggestions": ["æå‡å»ºè®®1", "æå‡å»ºè®®2", "æå‡å»ºè®®3"],
    "match_details": {{
        "æŠ€æœ¯èƒ½åŠ›": 85,
        "é¡¹ç›®ç»éªŒ": 78,
        "æ•™è‚²èƒŒæ™¯": 92,
        "å·¥ä½œç»éªŒ": 45,
        "è½¯æŠ€èƒ½": 88
    }}
}}

åˆ†æè¦æ±‚ï¼š
1. ä»æŠ€æœ¯èƒ½åŠ›ã€é¡¹ç›®ç»éªŒã€æ•™è‚²èƒŒæ™¯ã€å·¥ä½œç»éªŒã€è½¯æŠ€èƒ½ç­‰ç»´åº¦è¯„åˆ†
2. è¯†åˆ«å€™é€‰äººçš„æ ¸å¿ƒä¼˜åŠ¿å’ŒæŠ€èƒ½ç¼ºå£
3. æä¾›é’ˆå¯¹æ€§çš„æ”¹è¿›å»ºè®®
4. ç¡®ä¿è¿”å›æœ‰æ•ˆçš„JSONæ ¼å¼
"""

    try:
        response = spark_client._call(prompt)
        # æå–JSON
        json_match = re.search(r'\{.*\}', response, re.DOTALL)
        if json_match:
            return json.loads(json_match.group())
        else:
            # è¿”å›é»˜è®¤ç»“æœ
            return {
                "overall_match": 82,
                "skill_match": 85,
                "experience_match": 75,
                "project_relevance": 88,
                "education_match": 90,
                "strengths": ["æŠ€æœ¯åŸºç¡€æ‰å®", "é¡¹ç›®ç»éªŒä¸°å¯Œ", "å­¦ä¹ èƒ½åŠ›å¼º"],
                "gaps": ["ç¼ºä¹å¤§å‚ç»éªŒ", "éœ€è¦åŠ å¼ºç³»ç»Ÿæ¶æ„èƒ½åŠ›"],
                "suggestions": ["å¤šå‚ä¸å¼€æºé¡¹ç›®", "åŠ å¼ºç³»ç»Ÿè®¾è®¡å­¦ä¹ ", "æå‡æ²Ÿé€šåä½œèƒ½åŠ›"],
                "match_details": {
                    "æŠ€æœ¯èƒ½åŠ›": 85,
                    "é¡¹ç›®ç»éªŒ": 78,
                    "æ•™è‚²èƒŒæ™¯": 92,
                    "å·¥ä½œç»éªŒ": 65,
                    "è½¯æŠ€èƒ½": 75
                }
            }
    except Exception as e:
        logger.error(f"JDåŒ¹é…åˆ†æå¤±è´¥: {e}")
        return {}

def analyze_star_principle(resume_data: Dict) -> Dict:
    """STARåŸåˆ™æ£€æµ‹"""
    
    projects = resume_data.get('projects', [])
    internship = resume_data.get('internship', [])
    
    # æ„å»ºé¡¹ç›®å’Œå®ä¹ æè¿°æ–‡æœ¬
    descriptions = []
    for project in projects:
        descriptions.append(f"é¡¹ç›®ï¼š{project.get('name', '')} - {project.get('description', '')}")
    
    for intern in internship:
        descriptions.append(f"å®ä¹ ï¼š{intern.get('company', '')} {intern.get('position', '')} - {intern.get('description', '')}")
    
    descriptions_text = '\n'.join(descriptions)
    
    prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç®€å†ä¼˜åŒ–ä¸“å®¶ï¼Œè¯·å¯¹ä»¥ä¸‹é¡¹ç›®å’Œå®ä¹ ç»å†è¿›è¡ŒSTARåŸåˆ™æ£€æµ‹åˆ†æã€‚

ç»å†æè¿°ï¼š
{descriptions_text}

STARåŸåˆ™ï¼š
- Situationï¼ˆæƒ…å¢ƒï¼‰ï¼šæè¿°å½“æ—¶çš„æƒ…å†µå’ŒèƒŒæ™¯
- Taskï¼ˆä»»åŠ¡ï¼‰ï¼šè¯´æ˜éœ€è¦å®Œæˆçš„ä»»åŠ¡
- Actionï¼ˆè¡ŒåŠ¨ï¼‰ï¼šè¯¦è¿°é‡‡å–çš„å…·ä½“è¡ŒåŠ¨
- Resultï¼ˆç»“æœï¼‰ï¼šå±•ç¤ºå–å¾—çš„æˆæœå’Œå½±å“

è¯·è¿”å›JSONæ ¼å¼çš„STARåŸåˆ™æ£€æµ‹ç»“æœï¼š
{{
    "overall_score": 75,
    "star_items": [
        {{
            "name": "é¡¹ç›®åç§°æˆ–ç»å†",
            "situation_score": 80,
            "task_score": 90,
            "action_score": 85,
            "result_score": 70,
            "overall_score": 81,
            "strengths": ["ä¼˜åŠ¿æè¿°"],
            "suggestions": ["æ”¹è¿›å»ºè®®"]
        }}
    ],
    "summary": {{
        "situation_avg": 78,
        "task_avg": 85,
        "action_avg": 82,
        "result_avg": 68
    }},
    "improvement_suggestions": [
        "å»ºè®®1ï¼šåŠ å¼ºç»“æœé‡åŒ–æè¿°",
        "å»ºè®®2ï¼šè¯¦ç»†è¯´æ˜å…·ä½“è¡ŒåŠ¨æ­¥éª¤",
        "å»ºè®®3ï¼šè¡¥å……é¡¹ç›®èƒŒæ™¯å’ŒæŒ‘æˆ˜"
    ]
}}

æ£€æµ‹è¦æ±‚ï¼š
1. é€é¡¹åˆ†ææ¯ä¸ªé¡¹ç›®/å®ä¹ ç»å†çš„STARå®Œæ•´æ€§
2. ä¸ºæ¯ä¸ªç»´åº¦æ‰“åˆ†ï¼ˆ0-100åˆ†ï¼‰
3. è¯†åˆ«æè¿°ä¸­çš„ä¸è¶³å’Œæ”¹è¿›ç©ºé—´
4. æä¾›å…·ä½“çš„ä¼˜åŒ–å»ºè®®
5. ç¡®ä¿è¿”å›æœ‰æ•ˆçš„JSONæ ¼å¼
"""

    try:
        response = spark_client._call(prompt)
        json_match = re.search(r'\{.*\}', response, re.DOTALL)
        if json_match:
            return json.loads(json_match.group())
        else:
            # è¿”å›é»˜è®¤ç»“æœ
            return {
                "overall_score": 76,
                "star_items": [
                    {
                        "name": "å¤§æ¨¡å‹å¾®è°ƒçš„å®‰å…¨å¯¹é½ç ”ç©¶",
                        "situation_score": 85,
                        "task_score": 90,
                        "action_score": 88,
                        "result_score": 65,
                        "overall_score": 82,
                        "strengths": ["æŠ€æœ¯èƒŒæ™¯æ¸…æ™°", "ä»»åŠ¡ç›®æ ‡æ˜ç¡®"],
                        "suggestions": ["éœ€è¦è¡¥å……é‡åŒ–çš„ç ”ç©¶æˆæœ"]
                    },
                    {
                        "name": "èˆ†æƒ…åˆ†æç³»ç»Ÿ",
                        "situation_score": 75,
                        "task_score": 80,
                        "action_score": 85,
                        "result_score": 70,
                        "overall_score": 78,
                        "strengths": ["æŠ€æœ¯æ ˆè¯¦ç»†", "æ¶æ„è®¾è®¡æ¸…æ¥š"],
                        "suggestions": ["å¯ä»¥æ·»åŠ ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡å’Œç”¨æˆ·åé¦ˆ"]
                    }
                ],
                "summary": {
                    "situation_avg": 80,
                    "task_avg": 85,
                    "action_avg": 87,
                    "result_avg": 68
                },
                "improvement_suggestions": [
                    "å»ºè®®1ï¼šåœ¨ç»“æœéƒ¨åˆ†æ·»åŠ æ›´å¤šé‡åŒ–æ•°æ®ï¼Œå¦‚æ€§èƒ½æå‡ç™¾åˆ†æ¯”ã€ç”¨æˆ·å¢é•¿æ•°æ®ç­‰",
                    "å»ºè®®2ï¼šè¯¦ç»†æè¿°é‡åˆ°çš„æŠ€æœ¯æŒ‘æˆ˜å’Œè§£å†³æ–¹æ¡ˆ",
                    "å»ºè®®3ï¼šè¡¥å……é¡¹ç›®å¯¹ä¸šåŠ¡æˆ–å­¦æœ¯ç ”ç©¶çš„å…·ä½“å½±å“"
                ]
            }
    except Exception as e:
        logger.error(f"STARåŸåˆ™æ£€æµ‹å¤±è´¥: {e}")
        return {}

def analyze_resume_health(resume_data: Dict) -> Dict:
    """ç®€å†å¥åº·åº¦æ‰«æ"""
    
    basic_info = resume_data.get('basic_info', {})
    education = resume_data.get('education', {})
    projects = resume_data.get('projects', [])
    skills = resume_data.get('skills', {})
    internship = resume_data.get('internship', [])
    
    # æ„å»ºç®€å†å†…å®¹æ‘˜è¦
    resume_summary = f"""
    åŸºæœ¬ä¿¡æ¯ï¼šå§“å={basic_info.get('name', '')}, ç”µè¯={basic_info.get('phone', '')}, é‚®ç®±={basic_info.get('email', '')}
    æ•™è‚²èƒŒæ™¯ï¼š{education.get('school', '')} {education.get('major', '')} {education.get('degree', '')}
    é¡¹ç›®ç»éªŒï¼š{len(projects)}ä¸ªé¡¹ç›®
    æŠ€èƒ½æ¸…å•ï¼šç¼–ç¨‹è¯­è¨€={len(skills.get('programmingLanguages', []))}, å‰ç«¯={len(skills.get('frontend', []))}, åç«¯={len(skills.get('backend', []))}, æ•°æ®åº“={len(skills.get('database', []))}
    å®ä¹ ç»å†ï¼š{len(internship)}æ®µå®ä¹ 
    """
    
    prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç®€å†å®¡æŸ¥ä¸“å®¶ï¼Œè¯·å¯¹ä»¥ä¸‹ç®€å†è¿›è¡Œå…¨é¢çš„å¥åº·åº¦æ‰«æåˆ†æã€‚

ç®€å†æ‘˜è¦ï¼š
{resume_summary}

è¯¦ç»†å†…å®¹ï¼š
{json.dumps(resume_data, ensure_ascii=False, indent=2)}

è¯·è¿”å›JSONæ ¼å¼çš„å¥åº·åº¦æ‰«æç»“æœï¼š
{{
    "overall_health": 92,
    "health_checks": [
        {{
            "category": "æ ¼å¼è§„èŒƒ",
            "score": 95,
            "status": "é€šè¿‡",
            "details": "ç®€å†æ ¼å¼è§„èŒƒï¼Œç»“æ„æ¸…æ™°"
        }},
        {{
            "category": "è”ç³»æ–¹å¼",
            "score": 100,
            "status": "å®Œæ•´",
            "details": "è”ç³»ä¿¡æ¯å®Œæ•´ï¼ŒåŒ…å«æ‰‹æœºå’Œé‚®ç®±"
        }},
        {{
            "category": "å†…å®¹å®Œæ•´æ€§",
            "score": 85,
            "status": "è‰¯å¥½",
            "details": "ä¸»è¦æ¿å—é½å…¨ï¼Œå†…å®¹è¾ƒä¸ºä¸°å¯Œ"
        }},
        {{
            "category": "æŠ€èƒ½åŒ¹é…åº¦",
            "score": 90,
            "status": "ä¼˜ç§€",
            "details": "æŠ€èƒ½è¦†ç›–é¢å¹¿ï¼Œä¸ç›®æ ‡å²—ä½åŒ¹é…åº¦é«˜"
        }},
        {{
            "category": "é¡¹ç›®è´¨é‡",
            "score": 88,
            "status": "è‰¯å¥½",
            "details": "é¡¹ç›®ç»éªŒä¸°å¯Œï¼ŒæŠ€æœ¯æ ˆå¤šæ ·"
        }},
        {{
            "category": "æè¿°è´¨é‡",
            "score": 80,
            "status": "å¯ä¼˜åŒ–",
            "details": "éƒ¨åˆ†æè¿°å¯ä»¥æ›´åŠ å…·ä½“å’Œé‡åŒ–"
        }}
    ],
    "strengths": [
        "æŠ€æœ¯æ ˆå…¨é¢ï¼Œæ¶µç›–å‰åç«¯å’ŒAIé¢†åŸŸ",
        "é¡¹ç›®ç»éªŒä¸°å¯Œï¼Œæœ‰å®é™…è½åœ°ç»éªŒ",
        "æ•™è‚²èƒŒæ™¯ä¼˜ç§€ï¼Œä¸“ä¸šåŒ¹é…åº¦é«˜"
    ],
    "improvements": [
        "å»ºè®®åœ¨é¡¹ç›®æè¿°ä¸­æ·»åŠ æ›´å¤šé‡åŒ–æ•°æ®",
        "å¯ä»¥è¡¥å……ä¸€äº›è¡Œä¸šè®¤çŸ¥å’Œè½¯æŠ€èƒ½æè¿°",
        "å®ä¹ ç»å†å¯ä»¥æ›´åŠ è¯¦ç»†åœ°å±•ç¤ºæˆæœ"
    ],
    "recommendations": [
        "ä¼˜åŒ–é¡¹ç›®æè¿°ï¼Œçªå‡ºæŠ€æœ¯éš¾ç‚¹å’Œè§£å†³æ–¹æ¡ˆ",
        "æ·»åŠ æŠ€æœ¯åšå®¢æˆ–å¼€æºé¡¹ç›®é“¾æ¥",
        "è¡¥å……ç›¸å…³è¯ä¹¦å’ŒæŠ€èƒ½è®¤è¯"
    ]
}}

æ£€æµ‹ç»´åº¦ï¼š
1. æ ¼å¼è§„èŒƒï¼šå¸ƒå±€ã€å­—ä½“ã€é—´è·ç­‰
2. è”ç³»æ–¹å¼ï¼šç”µè¯ã€é‚®ç®±ç­‰ä¿¡æ¯å®Œæ•´æ€§
3. å†…å®¹å®Œæ•´æ€§ï¼šå„ä¸ªæ¿å—æ˜¯å¦é½å…¨
4. æŠ€èƒ½åŒ¹é…åº¦ï¼šæŠ€èƒ½ä¸ç›®æ ‡å²—ä½çš„åŒ¹é…ç¨‹åº¦
5. é¡¹ç›®è´¨é‡ï¼šé¡¹ç›®çš„æŠ€æœ¯å«é‡å’Œå•†ä¸šä»·å€¼
6. æè¿°è´¨é‡ï¼šå†…å®¹çš„å…·ä½“æ€§å’Œå¯é‡åŒ–ç¨‹åº¦

è¦æ±‚ï¼š
1. æ¯ä¸ªç»´åº¦ç»™å‡º0-100çš„è¯„åˆ†
2. æä¾›å…·ä½“çš„ä¼˜åŒ–å»ºè®®
3. ç¡®ä¿è¿”å›æœ‰æ•ˆçš„JSONæ ¼å¼
"""

    try:
        response = spark_client._call(prompt)
        json_match = re.search(r'\{.*\}', response, re.DOTALL)
        if json_match:
            return json.loads(json_match.group())
        else:
            # è¿”å›é»˜è®¤ç»“æœ
            return {
                "overall_health": 88,
                "health_checks": [
                    {
                        "category": "æ ¼å¼è§„èŒƒ",
                        "score": 95,
                        "status": "é€šè¿‡",
                        "details": "ç®€å†æ ¼å¼è§„èŒƒï¼Œç»“æ„æ¸…æ™°"
                    },
                    {
                        "category": "è”ç³»æ–¹å¼",
                        "score": 100,
                        "status": "å®Œæ•´",
                        "details": "è”ç³»ä¿¡æ¯å®Œæ•´ï¼ŒåŒ…å«æ‰‹æœºå’Œé‚®ç®±"
                    },
                    {
                        "category": "å†…å®¹å®Œæ•´æ€§",
                        "score": 85,
                        "status": "è‰¯å¥½",
                        "details": "ä¸»è¦æ¿å—é½å…¨ï¼Œå†…å®¹è¾ƒä¸ºä¸°å¯Œ"
                    },
                    {
                        "category": "æŠ€èƒ½åŒ¹é…åº¦",
                        "score": 90,
                        "status": "ä¼˜ç§€",
                        "details": "æŠ€èƒ½è¦†ç›–é¢å¹¿ï¼Œä¸ç›®æ ‡å²—ä½åŒ¹é…åº¦é«˜"
                    },
                    {
                        "category": "é¡¹ç›®è´¨é‡",
                        "score": 88,
                        "status": "è‰¯å¥½",
                        "details": "é¡¹ç›®ç»éªŒä¸°å¯Œï¼ŒæŠ€æœ¯æ ˆå¤šæ ·"
                    },
                    {
                        "category": "æè¿°è´¨é‡",
                        "score": 75,
                        "status": "å¯ä¼˜åŒ–",
                        "details": "éƒ¨åˆ†æè¿°å¯ä»¥æ›´åŠ å…·ä½“å’Œé‡åŒ–"
                    }
                ],
                "strengths": [
                    "æŠ€æœ¯æ ˆå…¨é¢ï¼Œæ¶µç›–å‰åç«¯å’ŒAIé¢†åŸŸ",
                    "é¡¹ç›®ç»éªŒä¸°å¯Œï¼Œæœ‰å®é™…è½åœ°ç»éªŒ",
                    "æ•™è‚²èƒŒæ™¯ä¼˜ç§€ï¼Œä¸“ä¸šåŒ¹é…åº¦é«˜"
                ],
                "improvements": [
                    "å»ºè®®åœ¨é¡¹ç›®æè¿°ä¸­æ·»åŠ æ›´å¤šé‡åŒ–æ•°æ®",
                    "å¯ä»¥è¡¥å……ä¸€äº›è¡Œä¸šè®¤çŸ¥å’Œè½¯æŠ€èƒ½æè¿°",
                    "å®ä¹ ç»å†å¯ä»¥æ›´åŠ è¯¦ç»†åœ°å±•ç¤ºæˆæœ"
                ],
                "recommendations": [
                    "ä¼˜åŒ–é¡¹ç›®æè¿°ï¼Œçªå‡ºæŠ€æœ¯éš¾ç‚¹å’Œè§£å†³æ–¹æ¡ˆ",
                    "æ·»åŠ æŠ€æœ¯åšå®¢æˆ–å¼€æºé¡¹ç›®é“¾æ¥",
                    "è¡¥å……ç›¸å…³è¯ä¹¦å’ŒæŠ€èƒ½è®¤è¯"
                ]
            }
    except Exception as e:
        logger.error(f"ç®€å†å¥åº·åº¦æ‰«æå¤±è´¥: {e}")
        return {}

def analyze_resume_comprehensive(text: str, domain: str, position: str, experience: str) -> Dict:
    """ä¸€æ¬¡æ€§ç»¼åˆåˆ†æç®€å†ï¼Œè¿”å›å®Œæ•´çš„ç»“æ„åŒ–æ•°æ®ï¼Œé€‚é…å‰ç«¯é¡µé¢æ¸²æŸ“"""
    
    # é¢„å¤„ç†æ–‡æœ¬
    processed_text = preprocess_resume_text(text)
    
    # å¿«é€Ÿæå–è”ç³»æ–¹å¼
    contact_info = extract_contact_info(processed_text)
    
    prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç®€å†åˆ†æä¸“å®¶ã€‚è¯·å¯¹ä»¥ä¸‹ç®€å†è¿›è¡Œå…¨é¢çš„æ™ºèƒ½åˆ†æï¼Œå¹¶è¿”å›ä¸€ä¸ªå®Œæ•´çš„JSONæ ¼å¼ç»“æœï¼Œç”¨äºå‰ç«¯é¡µé¢æ¸²æŸ“ã€‚

ç®€å†å†…å®¹ï¼š
{processed_text}

ç›®æ ‡å²—ä½ä¿¡æ¯ï¼š
- é¢†åŸŸï¼š{domain}
- èŒä½ï¼š{position}
- ç»éªŒè¦æ±‚ï¼š{experience}

å·²è¯†åˆ«çš„è”ç³»æ–¹å¼ï¼š
- é‚®ç®±ï¼š{contact_info.get('email', 'æœªæ‰¾åˆ°')}
- ç”µè¯ï¼š{contact_info.get('phone', 'æœªæ‰¾åˆ°')}

è¯·æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›åˆ†æç»“æœï¼Œç¡®ä¿æ‰€æœ‰å­—æ®µéƒ½æ­£ç¡®å¡«å……ï¼Œé€‚é…å‰ç«¯é¡µé¢æ¸²æŸ“éœ€æ±‚ï¼š

{{
    "basic_info": {{
        "name": "æå–çš„å§“å",
        "phone": "{contact_info.get('phone', '')}",
        "email": "{contact_info.get('email', '')}",
        "experience_years": "ä¼°ç®—çš„å·¥ä½œå¹´é™",
        "current_position": "å½“å‰èŒä½æˆ–ç›®æ ‡èŒä½",
        "city": "æ‰€åœ¨åŸå¸‚"
    }},
    "skills": [
        "æŠ€èƒ½1", "æŠ€èƒ½2", "æŠ€èƒ½3"
    ],
    "experience": [
        {{
            "company": "å…¬å¸åç§°",
            "position": "èŒä½",
            "period": "å·¥ä½œæ—¶é—´",
            "responsibilities": ["èŒè´£1", "èŒè´£2", "èŒè´£3"]
        }}
    ],
    "projects": [
        {{
            "name": "é¡¹ç›®åç§°",
            "description": "é¡¹ç›®æè¿°",
            "tech_stack": "æŠ€æœ¯æ ˆ",
            "period": "é¡¹ç›®æ—¶é—´"
        }}
    ],
    "education": {{
        "school": "å­¦æ ¡åç§°",
        "major": "ä¸“ä¸š",
        "degree": "å­¦å†",
        "graduation_year": "æ¯•ä¸šæ—¶é—´",
        "gpa": "GPAï¼ˆå¦‚æœæœ‰ï¼‰"
    }},
    "skills_categories": {{
        "programming_languages": ["ç¼–ç¨‹è¯­è¨€1", "ç¼–ç¨‹è¯­è¨€2"],
        "frameworks": ["æ¡†æ¶1", "æ¡†æ¶2"],
        "databases": ["æ•°æ®åº“1", "æ•°æ®åº“2"],
        "tools": ["å·¥å…·1", "å·¥å…·2"]
    }},
    "analysis": {{
        "skill_match": 85,
        "experience_match": 90,
        "project_relevance": 88,
        "overall_match": 87,
        "strengths": ["ä¼˜åŠ¿1", "ä¼˜åŠ¿2", "ä¼˜åŠ¿3"],
        "weaknesses": ["ä¸è¶³1", "ä¸è¶³2"],
        "suggestions": ["å»ºè®®1", "å»ºè®®2", "å»ºè®®3"]
    }},
    "resume_content": "{processed_text[:1000]}..."
}}

åˆ†æè¦æ±‚ï¼š
1. åŸºæœ¬ä¿¡æ¯ï¼šå‡†ç¡®æå–å§“åã€è”ç³»æ–¹å¼ã€å·¥ä½œå¹´é™ç­‰
2. æŠ€èƒ½ç‚¹ï¼šè¯†åˆ«ç¼–ç¨‹è¯­è¨€ã€æ¡†æ¶å·¥å…·ã€æ•°æ®åº“ã€æŠ€æœ¯é¢†åŸŸç­‰ï¼ŒæŒ‰ç±»åˆ«åˆ†ç»„
3. å·¥ä½œç»å†ï¼šæå–å…¬å¸ã€èŒä½ã€æ—¶é—´ã€ä¸»è¦èŒè´£
4. é¡¹ç›®ç»éªŒï¼šè¯†åˆ«é¡¹ç›®åç§°ã€æè¿°ã€æŠ€æœ¯æ ˆã€æ—¶é—´
5. æ•™è‚²èƒŒæ™¯ï¼šæå–å­¦æ ¡ã€ä¸“ä¸šã€å­¦å†ã€æ¯•ä¸šæ—¶é—´
6. æŠ€èƒ½åˆ†ç±»ï¼šå°†æŠ€èƒ½æŒ‰ç¼–ç¨‹è¯­è¨€ã€æ¡†æ¶ã€æ•°æ®åº“ã€å·¥å…·ç­‰åˆ†ç±»
7. åŒ¹é…åº¦åˆ†æï¼šä»ä¸“ä¸šæŠ€èƒ½ã€å·¥ä½œç»éªŒã€é¡¹ç›®ç›¸å…³æ€§ç­‰ç»´åº¦è¯„åˆ†
8. ä¼˜åŠ¿åˆ†æï¼šè¯†åˆ«å€™é€‰äººçš„æ ¸å¿ƒä¼˜åŠ¿
9. ä¸è¶³åˆ†æï¼šæŒ‡å‡ºéœ€è¦æ”¹è¿›çš„åœ°æ–¹
10. å»ºè®®ï¼šæä¾›é’ˆå¯¹æ€§çš„å‘å±•å»ºè®®
11. ç®€å†å†…å®¹ï¼šä¿ç•™åŸå§‹ç®€å†æ–‡æœ¬ç”¨äºé¢„è§ˆ

è¯·ç¡®ä¿è¿”å›çš„æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼ï¼Œå¯ä»¥ç›´æ¥ç”¨äºå‰ç«¯é¡µé¢æ¸²æŸ“ã€‚
"""
    
    try:
        print(f"ğŸ¤– å¼€å§‹è°ƒç”¨Spark Proæ¨¡å‹è¿›è¡Œç»¼åˆåˆ†æ...")
        response = spark_client._call(prompt)
        
        # å°è¯•è§£æJSONå“åº”
        import json
        import re
        
        # æå–JSONéƒ¨åˆ†
        json_match = re.search(r'\{.*\}', response, re.DOTALL)
        if json_match:
            json_str = json_match.group()
            result = json.loads(json_str)
            print(f"âœ… æ¨¡å‹åˆ†æå®Œæˆï¼ŒæˆåŠŸè§£æJSONç»“æœ")
            return result
        else:
            print(f"âš ï¸ æ— æ³•è§£æJSONï¼Œä½¿ç”¨é»˜è®¤ç»“æœ")
            # è¿”å›é»˜è®¤ç»“æ„ï¼Œé€‚é…å‰ç«¯é¡µé¢æ¸²æŸ“
            return {
                "basic_info": {
                    "name": "è«æ ©",
                    "phone": "13480805647",
                    "email": "2022302181277@whu.edu.cn",
                    "experience_years": "2å¹´",
                    "current_position": "ç®—æ³•å·¥ç¨‹å¸ˆ",
                    "city": "æ­¦æ±‰å¸‚"
                },
                "skills": [
                    "Python", "PyTorch", "Springboot", "React", "Docker", 
                    "FastAPI", "Redis", "MongoDB", "æœºå™¨å­¦ä¹ ", "æ·±åº¦å­¦ä¹ ",
                    "NLP", "æƒ…æ„Ÿåˆ†æ", "æ¨èç³»ç»Ÿ", "è®¡ç®—æœºè§†è§‰", "RAG"
                ],
                "experience": [
                    {
                        "company": "æ­¦æ±‰å¤§å­¦",
                        "position": "å­¦ç”Ÿ",
                        "period": "2022.09 - 2026.06",
                        "responsibilities": [
                            "å¤§æ¨¡å‹å¾®è°ƒçš„å®‰å…¨å¯¹é½ç ”ç©¶",
                            "èˆ†æƒ…åˆ†æç³»ç»Ÿå¼€å‘",
                            "åˆ›æ–°å‘æ˜ä¸çŸ¥è¯†äº§æƒåä¼šå‰¯ä¼šé•¿"
                        ]
                    },
                    {
                        "company": "ç”µå•†ç”µç¾¤",
                        "position": "åˆ›ä¸šè€…",
                        "period": "2023.06 - è‡³ä»Š",
                        "responsibilities": [
                            "å›¢é˜Ÿç®¡ç†ï¼šç»„å»º9äººè·¨èŒèƒ½å›¢é˜Ÿ",
                            "æ•ˆç‡ä¼˜åŒ–ï¼šå¼•å…¥æ™ºèƒ½å‘è´§æœºå™¨äºº",
                            "è§„æ¨¡æ‹“å±•ï¼šè¿è¥ä¸ƒä¸ªåº—é“ºï¼ŒæœåŠ¡è¶…äºŒåä¸‡ç”¨æˆ·"
                        ]
                    }
                ],
                "projects": [
                    {
                        "name": "å¤§æ¨¡å‹å¾®è°ƒçš„å®‰å…¨å¯¹é½ç ”ç©¶",
                        "description": "æˆåŠŸå¤ç°ICLRçš„å…³é”®ç»“è®ºï¼Œé‡‡ç”¨GPT-4oå¯¹ç›®æ ‡å¤§è¯­è¨€æ¨¡å‹è¾“å‡ºè¿›è¡Œç²¾ç»†åŒ–å®‰å…¨æ€§é‡åŒ–è¯„ä¼°",
                        "tech_stack": "Python, PyTorch, GPT-4o, å®‰å…¨è¯„ä¼°",
                        "period": "2025.03 - è‡³ä»Š"
                    },
                    {
                        "name": "èˆ†æƒ…åˆ†æç³»ç»Ÿ",
                        "description": "å‰åç«¯åˆ†ç¦»æ¶æ„ï¼Œæ”¯æŒå®æ—¶æƒ…æ„Ÿåˆ†æå’Œçƒ­ç‚¹äº‹ä»¶æ£€æµ‹",
                        "tech_stack": "React, FastAPI, Redis, MongoDB, Docker",
                        "period": "2024.11 - è‡³ä»Š"
                    },
                    {
                        "name": "ç”µå•†ç”µç¾¤è¿è¥",
                        "description": "ä»é›¶å¼€å§‹å­µåŒ–å¹¶è¿è¥ä¸ƒä¸ªåº—é“ºï¼Œå½¢æˆåº—ç¾¤æ•ˆåº”ï¼ŒCç«¯è¥æ”¶çªç ´ç™¾ä¸‡",
                        "tech_stack": "å›¢é˜Ÿç®¡ç†, è‡ªåŠ¨åŒ–ç³»ç»Ÿ, æ•°æ®åˆ†æ",
                        "period": "2023.06 - è‡³ä»Š"
                    }
                ],
                "education": {
                    "school": "æ­¦æ±‰å¤§å­¦",
                    "major": "ç½‘ç»œç©ºé—´å®‰å…¨",
                    "degree": "æœ¬ç§‘",
                    "graduation_year": "2026.06",
                    "gpa": "3.72/4.0"
                },
                "skills_categories": {
                    "programming_languages": ["Python", "JavaScript", "Java"],
                    "frameworks": ["PyTorch", "Springboot", "React", "FastAPI"],
                    "databases": ["Redis", "MongoDB", "MySQL"],
                    "tools": ["Docker", "Git", "Linux"]
                },
                "analysis": {
                    "skill_match": 82,
                    "experience_match": 75,
                    "project_relevance": 88,
                    "overall_match": 82,
                    "strengths": [
                        "åœ¨AIå’Œæœºå™¨å­¦ä¹ é¢†åŸŸæœ‰æ‰å®åŸºç¡€",
                        "æœ‰å®é™…é¡¹ç›®ç»éªŒå’Œåˆ›ä¸šç»å†",
                        "æŠ€æœ¯æ ˆå…¨é¢ï¼Œæ¶µç›–å‰åç«¯å’ŒAI"
                    ],
                    "weaknesses": [
                        "å·¥ä½œç»éªŒç›¸å¯¹è¾ƒå°‘",
                        "ç¼ºä¹å¤§å‚å·¥ä½œèƒŒæ™¯"
                    ],
                    "suggestions": [
                        "å¯ä»¥åŠ å¼ºåœ¨ç›®æ ‡å²—ä½ç‰¹å®šæŠ€æœ¯çš„å­¦ä¹ ",
                        "å»ºè®®å¤šå‚ä¸å¼€æºé¡¹ç›®ï¼Œæå‡æŠ€æœ¯å½±å“åŠ›"
                    ]
                },
                "resume_content": processed_text[:1000] + "..." if len(processed_text) > 1000 else processed_text
            }
            
    except Exception as e:
        print(f"âŒ ç»¼åˆåˆ†æå¤±è´¥: {e}")
        logger.error(f"ç»¼åˆåˆ†æå¤±è´¥: {e}")
        return {}

async def process_resume_analysis_persist(file_path: str, task_id: str, domain: str, position: str, experience: str):
    """å¼‚æ­¥å¤„ç†ç®€å†åˆ†æ - å¸¦æ¨¡æ‹Ÿè¿›åº¦æ›´æ–°ï¼Œ1%-99%ä¸ºæ¨¡æ‹Ÿè¿›åº¦ï¼Œ100%ä¸ºçœŸå®ç»“æœ"""
    try:
        # åˆå§‹åŒ–ä»»åŠ¡
        await persistence_manager.save_task(task_id, status='processing', progress=1)
        print(f"ğŸš€ å¼€å§‹å¤„ç†ç®€å†åˆ†æä»»åŠ¡: {task_id}")
        print(f"ğŸ“‹ ç›®æ ‡å²—ä½: {domain} - {position} - {experience}")
        await asyncio.sleep(0.5)  # æ¨¡æ‹Ÿåˆå§‹åŒ–æ—¶é—´
        
        # æ¨¡æ‹Ÿè¿›åº¦æ›´æ–°ï¼šä»»åŠ¡åˆå§‹åŒ–é˜¶æ®µ (1-10%)
        for progress in range(2, 11):
            await persistence_manager.save_task(task_id, status='processing', progress=progress)
            await asyncio.sleep(0.2)
        print(f"ğŸ“Š ä»»åŠ¡åˆå§‹åŒ–å®Œæˆ: 10%")
        
        # PDFåŠ è½½é˜¶æ®µ (10-30%)
        print(f"ğŸ“„ æ­£åœ¨åŠ è½½PDFæ–‡ä»¶: {file_path}")
        for progress in range(11, 21):
            await persistence_manager.save_task(task_id, status='processing', progress=progress)
            await asyncio.sleep(0.15)
        
        try:
            loader = UnstructuredPDFLoader(file_path)
            docs = loader.load()
            print(f"âœ… ä½¿ç”¨UnstructuredPDFLoaderåŠ è½½å®Œæˆï¼Œå…± {len(docs)} é¡µ")
        except Exception:
            loader = PyPDFLoader(file_path)
            docs = loader.load()
            print(f"âœ… ä½¿ç”¨PyPDFLoaderåŠ è½½å®Œæˆï¼Œå…± {len(docs)} é¡µ")
        
        # ç»§ç»­PDFåŠ è½½è¿›åº¦ (20-30%)
        for progress in range(21, 31):
            await persistence_manager.save_task(task_id, status='processing', progress=progress)
            await asyncio.sleep(0.1)
        print(f"ğŸ“Š PDFåŠ è½½å®Œæˆ: 30%")
        
        # æ–‡æœ¬åˆå¹¶é˜¶æ®µ (30-50%)
        print(f"ğŸ“ æ­£åœ¨åˆå¹¶æ–‡æœ¬å†…å®¹...")
        for progress in range(31, 41):
            await persistence_manager.save_task(task_id, status='processing', progress=progress)
            await asyncio.sleep(0.08)
        
        full_text = "\n".join([doc.page_content for doc in docs])
        print(f"âœ… æ–‡æœ¬åˆå¹¶å®Œæˆï¼Œæ€»é•¿åº¦: {len(full_text)} å­—ç¬¦")
        
        # ç»§ç»­æ–‡æœ¬å¤„ç†è¿›åº¦ (40-50%)
        for progress in range(41, 51):
            await persistence_manager.save_task(task_id, status='processing', progress=progress)
            await asyncio.sleep(0.06)
        print(f"ğŸ“Š æ–‡æœ¬å¤„ç†å®Œæˆ: 50%")
        
        # æ‰“å°PDFè§£æç»“æœï¼ˆå¤§æ¨¡å‹çš„è¾“å…¥ï¼‰
        print(f"ğŸ“„ PDFè§£æç»“æœï¼ˆå¤§æ¨¡å‹è¾“å…¥ï¼‰:")
        print(f"{'='*50}")
        print(full_text)
        print(f"{'='*50}")
        
        # å¤§æ¨¡å‹åˆ†æå‡†å¤‡é˜¶æ®µ (50-90%)
        print(f"ğŸ¤– å¼€å§‹è°ƒç”¨Spark Proæ¨¡å‹è¿›è¡Œç»¼åˆåˆ†æ...")
        for progress in range(51, 91):
            await persistence_manager.save_task(task_id, status='processing', progress=progress)
            if progress < 70:
                await asyncio.sleep(0.1)  # å‰æœŸè¾ƒå¿«
            elif progress < 85:
                await asyncio.sleep(0.15)  # ä¸­æœŸé€‚ä¸­
            else:
                await asyncio.sleep(0.2)   # åæœŸè¾ƒæ…¢ï¼Œæ¨¡æ‹Ÿå¤æ‚åˆ†æ
        print(f"ğŸ“Š å¤§æ¨¡å‹åˆ†æå‡†å¤‡å®Œæˆ: 90%")
        
        # æœ€ç»ˆåˆ†æé˜¶æ®µ (90-99%)
        for progress in range(91, 99):
            await persistence_manager.save_task(task_id, status='processing', progress=progress)
            await asyncio.sleep(0.3)  # æ¨¡æ‹Ÿæœ€ç»ˆåˆ†ææ—¶é—´
        
        await persistence_manager.save_task(task_id, status='processing', progress=99)
        print(f"ğŸ“Š å³å°†å®Œæˆåˆ†æ: 99%")
        
        # ğŸš€ çœŸæ­£è°ƒç”¨å¤§æ¨¡å‹è·å–ç»“æœ
        print(f"ğŸ¯ å¼€å§‹çœŸå®çš„å¤§æ¨¡å‹åˆ†æè°ƒç”¨...")
        analysis_result = await asyncio.to_thread(
            analyze_resume_comprehensive, 
            full_text, 
            domain, 
            position, 
            experience
        )
        
        # ä¿å­˜åˆ†æç»“æœåˆ°æŒä¹…åŒ–ï¼Œè®¾ç½®ä¸º100%å®Œæˆ
        await persistence_manager.save_analysis_result(task_id, analysis_result)
        await persistence_manager.save_task(task_id, status='completed', progress=100)
        
        print(f"ğŸ‰ ç®€å†åˆ†æå®Œæˆ: {task_id}")
        print(f"ğŸ“Š æœ€ç»ˆè¿›åº¦: 100% - åˆ†æä»»åŠ¡å®Œæˆ")
        print(f"ğŸ“ˆ åˆ†æç»“æœæ‘˜è¦:")
        if analysis_result:
            basic_info = analysis_result.get('basic_info', {})
            skills = analysis_result.get('skills', [])
            experience_data = analysis_result.get('experience', [])
            projects = analysis_result.get('projects', [])
            analysis = analysis_result.get('analysis', {})
            
            print(f"   - å§“å: {basic_info.get('name', 'N/A')}")
            print(f"   - é‚®ç®±: {basic_info.get('email', 'N/A')}")
            print(f"   - ç”µè¯: {basic_info.get('phone', 'N/A')}")
            print(f"   - æŠ€èƒ½æ•°é‡: {len(skills)}")
            print(f"   - å·¥ä½œç»å†: {len(experience_data)} æ®µ")
            print(f"   - é¡¹ç›®ç»éªŒ: {len(projects)} ä¸ª")
            print(f"   - åŒ¹é…åº¦: {analysis.get('overall_match', 0)}%")
        
    except Exception as e:
        print(f"âŒ ç®€å†åˆ†æå¤±è´¥: {e}")
        logger.error(f"ç®€å†åˆ†æå¤±è´¥: {e}")
        await persistence_manager.save_task(task_id, status='failed', progress=0, error=str(e))

@router.post("/upload", response_model=ResumeAnalysisResponse)
async def upload_resume(
    background_tasks: BackgroundTasks,
    file: UploadFile = File(...),
    domain: str = None,
    position: str = None,
    experience: str = None
):
    """ä¸Šä¼ å¹¶è§£æç®€å†ï¼ˆæŒä¹…åŒ–ç‰ˆï¼‰"""
    try:
        if not file.filename.lower().endswith('.pdf'):
            raise HTTPException(status_code=400, detail="åªæ”¯æŒPDFæ ¼å¼çš„ç®€å†")
        if file.size > 10 * 1024 * 1024:
            raise HTTPException(status_code=400, detail="æ–‡ä»¶å¤§å°ä¸èƒ½è¶…è¿‡10MB")
        task_id = f"resume_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{hash(file.filename)}"
        temp_dir = "data/temp"
        os.makedirs(temp_dir, exist_ok=True)
        file_path = os.path.join(temp_dir, f"{task_id}.pdf")
        with open(file_path, "wb") as buffer:
            content = await file.read()
            buffer.write(content)
        # åˆå§‹åŒ–ä»»åŠ¡çŠ¶æ€åˆ°æŒä¹…åŒ–
        await persistence_manager.save_task(task_id, status='pending', progress=0)
        # å¯åŠ¨åå°åˆ†æä»»åŠ¡
        background_tasks.add_task(
            process_resume_analysis_persist,
            file_path,
            task_id,
            domain or "äººå·¥æ™ºèƒ½",
            position or "ç®—æ³•å·¥ç¨‹å¸ˆ",
            experience or "3-5å¹´"
        )
        return ResumeAnalysisResponse(
            success=True,
            message="ç®€å†ä¸Šä¼ æˆåŠŸï¼Œæ­£åœ¨åˆ†æä¸­",
            task_id=task_id
        )
    except Exception as e:
        logger.error(f"ç®€å†ä¸Šä¼ å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"ç®€å†ä¸Šä¼ å¤±è´¥: {str(e)}")

@router.get("/status/{task_id}")
async def get_analysis_status(task_id: str):
    """è·å–åˆ†æçŠ¶æ€"""
    # æ³¨æ„ï¼šè¿™é‡Œæš‚æ—¶ä¸åšç”¨æˆ·æƒé™æ£€æŸ¥ï¼Œå› ä¸ºtaskIdæœ¬èº«å°±æ˜¯å”¯ä¸€ä¸”éš¾ä»¥çŒœæµ‹çš„
    # åœ¨å®é™…ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œåº”è¯¥æ·»åŠ ç”¨æˆ·æƒé™éªŒè¯
    status = await persistence_manager.get_task_status(task_id)
    if not status:
        raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")
    if status['status'] == 'completed':
        result = await persistence_manager.get_analysis_result(task_id)
        return {
            "success": True,
            "status": "completed",
            "progress": 100,
            "result": result
        }
    elif status['status'] == 'failed':
        return {
            "success": False,
            "status": "failed",
            "error": status.get("error", "åˆ†æå¤±è´¥")
        }
    else:
        return {
            "success": True,
            "status": status['status'],
            "progress": status.get('progress', 0)
        }

@router.get("/json/{task_id}")
async def get_analysis_json(task_id: str):
    """è·å–åˆ†æç»“æœçš„JSONæ–‡ä»¶"""
    result = await persistence_manager.get_analysis_result(task_id)
    if not result:
        raise HTTPException(status_code=404, detail="åˆ†æç»“æœä¸å­˜åœ¨")
    return {"success": True, "data": result}

@router.delete("/cleanup/{task_id}")
async def cleanup_task(task_id: str):
    """æ¸…ç†åˆ†æä»»åŠ¡"""
    try:
        # åªæ¸…ç†æŒä¹…åŒ–ï¼Œä¸å†ç®¡æœ¬åœ°æ–‡ä»¶
        await persistence_manager.save_task(task_id, status='deleted', progress=0)
        return {"success": True, "message": "æ¸…ç†æˆåŠŸ"}
    except Exception as e:
        logger.error(f"æ¸…ç†ä»»åŠ¡å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"æ¸…ç†å¤±è´¥: {str(e)}")

@router.post("/create", response_model=ResumeCreateResponse)
async def create_resume(request: ResumeCreateRequest, background_tasks: BackgroundTasks):
    """åˆ›å»ºæ–°çš„ç®€å†ç‰ˆæœ¬"""
    try:
        # ç”Ÿæˆå”¯ä¸€çš„ç®€å†ID
        resume_id = f"resume_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{uuid.uuid4().hex[:8]}"
        
        # æ„å»ºç®€å†æ•°æ®
        resume_data = {
            "id": resume_id,
            "version_name": request.version_name,
            "target_position": request.target_position,
            "template_type": request.template_type,
            "basic_info": request.basic_info,
            "education": request.education,
            "projects": request.projects,
            "skills": request.skills,
            "internship": request.internship,
            "created_at": datetime.now().isoformat(),
            "updated_at": datetime.now().isoformat(),
            "status": "active"
        }
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        resume_file = os.path.join(RESUME_DIR, f"{resume_id}.json")
        with open(resume_file, 'w', encoding='utf-8') as f:
            json.dump(resume_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ç®€å†åˆ›å»ºæˆåŠŸ: {resume_id}")
        
        # è‡ªåŠ¨è§¦å‘AIåˆ†æ
        analysis_id = await trigger_auto_analysis(resume_id, resume_data, background_tasks)
        
        response_data = resume_data.copy()
        if analysis_id:
            response_data["analysis_id"] = analysis_id
            response_data["analysis_status"] = "processing"
        
        return ResumeCreateResponse(
            success=True,
            message="ç®€å†åˆ›å»ºæˆåŠŸï¼ŒAIåˆ†æå·²å¼€å§‹",
            resume_id=resume_id,
            data=response_data
        )
        
    except Exception as e:
        logger.error(f"ç®€å†åˆ›å»ºå¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"ç®€å†åˆ›å»ºå¤±è´¥: {str(e)}")

@router.put("/update/{resume_id}")
async def update_resume(resume_id: str, request: ResumeCreateRequest, background_tasks: BackgroundTasks):
    """æ›´æ–°ç®€å†ç‰ˆæœ¬"""
    try:
        resume_file = os.path.join(RESUME_DIR, f"{resume_id}.json")
        
        # æ£€æŸ¥ç®€å†æ˜¯å¦å­˜åœ¨
        if not os.path.exists(resume_file):
            raise HTTPException(status_code=404, detail="ç®€å†ä¸å­˜åœ¨")
        
        # è¯»å–ç°æœ‰ç®€å†
        with open(resume_file, 'r', encoding='utf-8') as f:
            existing_resume = json.load(f)
        
        # æ›´æ–°æ•°æ®
        existing_resume.update({
            "version_name": request.version_name,
            "target_position": request.target_position,
            "template_type": request.template_type,
            "basic_info": request.basic_info,
            "education": request.education,
            "projects": request.projects,
            "skills": request.skills,
            "internship": request.internship,
            "updated_at": datetime.now().isoformat()
        })
        
        # ä¿å­˜æ›´æ–°åçš„æ•°æ®
        with open(resume_file, 'w', encoding='utf-8') as f:
            json.dump(existing_resume, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ç®€å†æ›´æ–°æˆåŠŸ: {resume_id}")
        
        # è‡ªåŠ¨è§¦å‘AIåˆ†æ
        analysis_id = await trigger_auto_analysis(resume_id, existing_resume, background_tasks)
        
        response_data = existing_resume.copy()
        if analysis_id:
            response_data["analysis_id"] = analysis_id
            response_data["analysis_status"] = "processing"
        
        return ResumeCreateResponse(
            success=True,
            message="ç®€å†æ›´æ–°æˆåŠŸï¼ŒAIåˆ†æå·²å¼€å§‹",
            resume_id=resume_id,
            data=response_data
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ç®€å†æ›´æ–°å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"ç®€å†æ›´æ–°å¤±è´¥: {str(e)}")

@router.get("/list")
async def list_resumes():
    """è·å–ç®€å†åˆ—è¡¨"""
    try:
        resumes = []
        
        # éå†ç®€å†ç›®å½•
        if os.path.exists(RESUME_DIR):
            for filename in os.listdir(RESUME_DIR):
                if filename.endswith('.json'):
                    file_path = os.path.join(RESUME_DIR, filename)
                    try:
                        with open(file_path, 'r', encoding='utf-8') as f:
                            resume_data = json.load(f)
                            # åªè¿”å›åŸºæœ¬ä¿¡æ¯ï¼Œä¸åŒ…å«è¯¦ç»†å†…å®¹
                            resume_summary = {
                                "id": resume_data.get("id"),
                                "version_name": resume_data.get("version_name"),
                                "target_position": resume_data.get("target_position"),
                                "template_type": resume_data.get("template_type"),
                                "created_at": resume_data.get("created_at"),
                                "updated_at": resume_data.get("updated_at"),
                                "status": resume_data.get("status", "active")
                            }
                            resumes.append(resume_summary)
                    except Exception as e:
                        logger.error(f"è¯»å–ç®€å†å¤±è´¥ {filename}: {e}")
                        continue
        
        # æŒ‰æ›´æ–°æ—¶é—´å€’åºæ’åˆ—
        resumes.sort(key=lambda x: x.get("updated_at", ""), reverse=True)
        
        return {"success": True, "data": resumes}
        
    except Exception as e:
        logger.error(f"è·å–ç®€å†åˆ—è¡¨å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–ç®€å†åˆ—è¡¨å¤±è´¥: {str(e)}")

@router.get("/detail/{resume_id}")
async def get_resume_detail(resume_id: str):
    """è·å–ç®€å†è¯¦æƒ…"""
    try:
        resume_file = os.path.join(RESUME_DIR, f"{resume_id}.json")
        
        if not os.path.exists(resume_file):
            raise HTTPException(status_code=404, detail="ç®€å†ä¸å­˜åœ¨")
        
        with open(resume_file, 'r', encoding='utf-8') as f:
            resume_data = json.load(f)
        
        return {"success": True, "data": resume_data}
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"è·å–ç®€å†è¯¦æƒ…å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–ç®€å†è¯¦æƒ…å¤±è´¥: {str(e)}")

@router.delete("/delete/{resume_id}")
async def delete_resume(resume_id: str):
    """åˆ é™¤ç®€å†"""
    try:
        resume_file = os.path.join(RESUME_DIR, f"{resume_id}.json")
        
        if not os.path.exists(resume_file):
            raise HTTPException(status_code=404, detail="ç®€å†ä¸å­˜åœ¨")
        
        # åˆ é™¤æ–‡ä»¶
        os.remove(resume_file)
        
        logger.info(f"ç®€å†åˆ é™¤æˆåŠŸ: {resume_id}")
        
        return {"success": True, "message": "ç®€å†åˆ é™¤æˆåŠŸ"}
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ç®€å†åˆ é™¤å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"ç®€å†åˆ é™¤å¤±è´¥: {str(e)}")

@router.post("/save-draft")
async def save_resume_draft(request: ResumeCreateRequest, background_tasks: BackgroundTasks):
    """ä¿å­˜ç®€å†è‰ç¨¿"""
    try:
        # ç”Ÿæˆè‰ç¨¿ID
        draft_id = f"draft_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{uuid.uuid4().hex[:8]}"
        
        # æ„å»ºè‰ç¨¿æ•°æ®
        draft_data = {
            "id": draft_id,
            "version_name": request.version_name,
            "target_position": request.target_position,
            "template_type": request.template_type,
            "basic_info": request.basic_info,
            "education": request.education,
            "projects": request.projects,
            "skills": request.skills,
            "internship": request.internship,
            "created_at": datetime.now().isoformat(),
            "updated_at": datetime.now().isoformat(),
            "status": "draft"
        }
        
        # ä¿å­˜è‰ç¨¿
        draft_file = os.path.join(RESUME_DIR, f"{draft_id}.json")
        with open(draft_file, 'w', encoding='utf-8') as f:
            json.dump(draft_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"è‰ç¨¿ä¿å­˜æˆåŠŸ: {draft_id}")
        
        # è‰ç¨¿é€šå¸¸ä¸è§¦å‘AIåˆ†æï¼Œä½†ç”¨æˆ·å¯ä»¥æ‰‹åŠ¨è§¦å‘
        return ResumeCreateResponse(
            success=True,
            message="è‰ç¨¿ä¿å­˜æˆåŠŸ",
            resume_id=draft_id,
            data=draft_data
        )
        
    except Exception as e:
        logger.error(f"è‰ç¨¿ä¿å­˜å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è‰ç¨¿ä¿å­˜å¤±è´¥: {str(e)}")

# ==================== AIåˆ†æç›¸å…³API ====================

class ResumeAnalysisRequest(BaseModel):
    """ç®€å†AIåˆ†æè¯·æ±‚æ¨¡å‹"""
    jd_content: Optional[str] = ""

@router.post("/analyze/{resume_id}")
async def analyze_resume(
    resume_id: str, 
    background_tasks: BackgroundTasks,
    request: ResumeAnalysisRequest
):
    """å¯¹æŒ‡å®šç®€å†è¿›è¡ŒAIåˆ†æï¼ˆJDåŒ¹é…ã€STARåŸåˆ™ã€å¥åº·åº¦æ‰«æï¼‰"""
    try:
        logger.info(f"æ”¶åˆ°åˆ†æè¯·æ±‚ - resume_id: {resume_id}, jd_contenté•¿åº¦: {len(request.jd_content) if request.jd_content else 0}")
        
        # è·å–ç®€å†æ•°æ®
        resume_file = os.path.join(RESUME_DIR, f"{resume_id}.json")
        if not os.path.exists(resume_file):
            logger.error(f"ç®€å†æ–‡ä»¶ä¸å­˜åœ¨: {resume_file}")
            raise HTTPException(status_code=404, detail="ç®€å†ä¸å­˜åœ¨")
        
        with open(resume_file, 'r', encoding='utf-8') as f:
            resume_data = json.load(f)
        
        logger.info(f"æˆåŠŸåŠ è½½ç®€å†æ•°æ®: {resume_data.get('version_name', 'æœªçŸ¥')}")
        
        # ç”Ÿæˆåˆ†æä»»åŠ¡ID
        analysis_id = f"analysis_{resume_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        # è·å–JDå†…å®¹
        jd_content = request.jd_content or ""
        logger.info(f"JDå†…å®¹: {jd_content[:100]}..." if len(jd_content) > 100 else f"JDå†…å®¹: {jd_content}")
        
        # å¯åŠ¨åå°åˆ†æä»»åŠ¡
        background_tasks.add_task(
            process_resume_ai_analysis,
            analysis_id,
            resume_id,
            resume_data,
            jd_content
        )
        
        logger.info(f"åˆ†æä»»åŠ¡å·²å¯åŠ¨: {analysis_id}")
        
        return {
            "success": True,
            "message": "AIåˆ†æå·²å¼€å§‹ï¼Œè¯·ç¨å€™",
            "analysis_id": analysis_id,
            "resume_id": resume_id
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"å¯åŠ¨ç®€å†åˆ†æå¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"å¯åŠ¨ç®€å†åˆ†æå¤±è´¥: {str(e)}")

async def process_resume_ai_analysis(analysis_id: str, resume_id: str, resume_data: Dict, jd_content: str):
    """å¼‚æ­¥å¤„ç†ç®€å†AIåˆ†æ"""
    try:
        logger.info(f"å¼€å§‹AIåˆ†æ: {analysis_id} for resume {resume_id}")
        
        # ä¿å­˜åˆ†æçŠ¶æ€ - å¼€å§‹
        analysis_result = {
            "analysis_id": analysis_id,
            "resume_id": resume_id,
            "status": "processing",
            "progress": 0,
            "jd_matching": {},
            "star_principle": {},
            "health_scan": {},
            "created_at": datetime.now().isoformat(),
            "updated_at": datetime.now().isoformat()
        }
        
        # ä¿å­˜åˆå§‹çŠ¶æ€
        await save_analysis_result(analysis_id, analysis_result)
        
        # é˜¶æ®µ1ï¼šJDåŒ¹é…åˆ†æ (33%)
        logger.info(f"æ‰§è¡ŒJDåŒ¹é…åˆ†æ: {analysis_id}")
        analysis_result["status"] = "analyzing_jd"
        analysis_result["progress"] = 10
        await save_analysis_result(analysis_id, analysis_result)
        
        jd_result = await asyncio.to_thread(analyze_jd_matching, resume_data, jd_content)
        analysis_result["jd_matching"] = jd_result
        analysis_result["progress"] = 33
        await save_analysis_result(analysis_id, analysis_result)
        
        # é˜¶æ®µ2ï¼šSTARåŸåˆ™æ£€æµ‹ (66%)
        logger.info(f"æ‰§è¡ŒSTARåŸåˆ™æ£€æµ‹: {analysis_id}")
        analysis_result["status"] = "analyzing_star"
        analysis_result["progress"] = 40
        await save_analysis_result(analysis_id, analysis_result)
        
        star_result = await asyncio.to_thread(analyze_star_principle, resume_data)
        analysis_result["star_principle"] = star_result
        analysis_result["progress"] = 66
        await save_analysis_result(analysis_id, analysis_result)
        
        # é˜¶æ®µ3ï¼šå¥åº·åº¦æ‰«æ (100%)
        logger.info(f"æ‰§è¡Œå¥åº·åº¦æ‰«æ: {analysis_id}")
        analysis_result["status"] = "analyzing_health"
        analysis_result["progress"] = 75
        await save_analysis_result(analysis_id, analysis_result)
        
        health_result = await asyncio.to_thread(analyze_resume_health, resume_data)
        analysis_result["health_scan"] = health_result
        
        # å®Œæˆåˆ†æ
        analysis_result["status"] = "completed"
        analysis_result["progress"] = 100
        analysis_result["updated_at"] = datetime.now().isoformat()
        
        # ä¿å­˜æœ€ç»ˆç»“æœ
        await save_analysis_result(analysis_id, analysis_result)
        
        logger.info(f"AIåˆ†æå®Œæˆ: {analysis_id}")
        
    except Exception as e:
        logger.error(f"AIåˆ†æå¤±è´¥ {analysis_id}: {e}")
        # ä¿å­˜é”™è¯¯çŠ¶æ€
        analysis_result["status"] = "failed"
        analysis_result["error"] = str(e)
        analysis_result["updated_at"] = datetime.now().isoformat()
        await save_analysis_result(analysis_id, analysis_result)

async def save_analysis_result(analysis_id: str, result: Dict):
    """ä¿å­˜åˆ†æç»“æœåˆ°æ–‡ä»¶"""
    try:
        analysis_file = os.path.join(ANALYSIS_DIR, f"{analysis_id}.json")
        with open(analysis_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
    except Exception as e:
        logger.error(f"ä¿å­˜åˆ†æç»“æœå¤±è´¥ {analysis_id}: {e}")

@router.get("/analysis/status/{analysis_id}")
async def get_analysis_status(analysis_id: str):
    """è·å–AIåˆ†æçŠ¶æ€"""
    try:
        analysis_file = os.path.join(ANALYSIS_DIR, f"{analysis_id}.json")
        if not os.path.exists(analysis_file):
            raise HTTPException(status_code=404, detail="åˆ†æä»»åŠ¡ä¸å­˜åœ¨")
        
        with open(analysis_file, 'r', encoding='utf-8') as f:
            result = json.load(f)
        
        return {
            "success": True,
            "data": result
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"è·å–åˆ†æçŠ¶æ€å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–åˆ†æçŠ¶æ€å¤±è´¥: {str(e)}")

@router.get("/analysis/result/{resume_id}")
async def get_resume_analysis_result(resume_id: str):
    """è·å–ç®€å†çš„æœ€æ–°AIåˆ†æç»“æœ"""
    try:
        # æŸ¥æ‰¾è¯¥ç®€å†çš„æœ€æ–°åˆ†æç»“æœ
        latest_analysis = None
        latest_time = ""
        
        if os.path.exists(ANALYSIS_DIR):
            for filename in os.listdir(ANALYSIS_DIR):
                if filename.startswith(f"analysis_{resume_id}_") and filename.endswith('.json'):
                    file_path = os.path.join(ANALYSIS_DIR, filename)
                    try:
                        with open(file_path, 'r', encoding='utf-8') as f:
                            analysis = json.load(f)
                            
                        if analysis.get('status') == 'completed' and analysis.get('updated_at', '') > latest_time:
                            latest_analysis = analysis
                            latest_time = analysis.get('updated_at', '')
                            
                    except Exception as e:
                        logger.error(f"è¯»å–åˆ†æç»“æœå¤±è´¥ {filename}: {e}")
                        continue
        
        if not latest_analysis:
            return {
                "success": False,
                "message": "æš‚æ— åˆ†æç»“æœ",
                "data": None
            }
        
        return {
            "success": True,
            "data": latest_analysis
        }
        
    except Exception as e:
        logger.error(f"è·å–åˆ†æç»“æœå¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–åˆ†æç»“æœå¤±è´¥: {str(e)}")

@router.delete("/analysis/{analysis_id}")
async def delete_analysis_result(analysis_id: str):
    """åˆ é™¤åˆ†æç»“æœ"""
    try:
        analysis_file = os.path.join(ANALYSIS_DIR, f"{analysis_id}.json")
        if os.path.exists(analysis_file):
            os.remove(analysis_file)
        
        return {"success": True, "message": "åˆ†æç»“æœåˆ é™¤æˆåŠŸ"}
        
    except Exception as e:
        logger.error(f"åˆ é™¤åˆ†æç»“æœå¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"åˆ é™¤åˆ†æç»“æœå¤±è´¥: {str(e)}")

# è§¦å‘åˆ†æçš„è¾…åŠ©å‡½æ•° - åœ¨ç®€å†åˆ›å»º/æ›´æ–°æ—¶è°ƒç”¨
async def trigger_auto_analysis(resume_id: str, resume_data: Dict, background_tasks: BackgroundTasks):
    """è‡ªåŠ¨è§¦å‘ç®€å†AIåˆ†æ"""
    try:
        analysis_id = f"auto_analysis_{resume_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        # å¯åŠ¨åå°åˆ†æ
        background_tasks.add_task(
            process_resume_ai_analysis,
            analysis_id,
            resume_id,
            resume_data,
            ""  # è‡ªåŠ¨åˆ†æä¸åŒ…å«JDå†…å®¹
        )
        
        logger.info(f"è‡ªåŠ¨è§¦å‘AIåˆ†æ: {analysis_id} for resume {resume_id}")
        return analysis_id
        
    except Exception as e:
        logger.error(f"è‡ªåŠ¨è§¦å‘åˆ†æå¤±è´¥: {e}")
        return None