"""
ç®€å†è§£æè·¯ç”±
ä½¿ç”¨LangChainå’ŒPyPDF2å®ç°çœŸæ­£çš„PDFç®€å†è§£æåŠŸèƒ½
"""
import os
import logging
import tempfile
from typing import Dict, List, Optional
from fastapi import APIRouter, UploadFile, File, HTTPException, BackgroundTasks
from fastapi.responses import JSONResponse, FileResponse
from pydantic import BaseModel
import asyncio
from datetime import datetime
import uuid

# LangChain imports
from langchain_community.document_loaders import PyPDFLoader, UnstructuredPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_core.documents import Document

# è®¯é£æ˜Ÿç«æ¨¡å‹
from src.models.spark_client import SparkLLM

# æ–‡æœ¬å¤„ç†å·¥å…·
import re
import json
from typing import Dict, List, Optional, Tuple

# æŒä¹…åŒ–ç®¡ç†å™¨
from src.persistence.optimal_manager import persistence_manager

logger = logging.getLogger(__name__)
router = APIRouter()

# åˆå§‹åŒ–æ˜Ÿç«å®¢æˆ·ç«¯
spark_client = SparkLLM()

class ResumeAnalysisRequest(BaseModel):
    """ç®€å†åˆ†æè¯·æ±‚æ¨¡å‹"""
    domain: str
    position: str
    experience: str

class ResumeAnalysisResponse(BaseModel):
    """ç®€å†åˆ†æå“åº”æ¨¡å‹"""
    success: bool
    message: str
    data: Optional[Dict] = None
    task_id: Optional[str] = None

class ResumeAnalysisResult(BaseModel):
    """ç®€å†åˆ†æç»“æœæ¨¡å‹"""
    basic_info: Dict
    skills: List[str]
    experience: List[Dict]
    projects: List[Dict]
    education: Dict
    analysis: Dict

# å­˜å‚¨åˆ†æä»»åŠ¡çš„çŠ¶æ€
analysis_tasks = {}

# åˆ›å»ºæ•°æ®å­˜å‚¨ç›®å½•
DATA_DIR = "data/analysis_results"
os.makedirs(DATA_DIR, exist_ok=True)

def preprocess_resume_text(text: str) -> str:
    """é¢„å¤„ç†ç®€å†æ–‡æœ¬ï¼Œæé«˜åˆ†ææ•ˆç‡"""
    # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
    text = re.sub(r'\s+', ' ', text)
    
    # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ä½†ä¿ç•™ä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­—å’ŒåŸºæœ¬æ ‡ç‚¹
    text = re.sub(r'[^\w\s\u4e00-\u9fff.,;:!?()ï¼ˆï¼‰\-â€”â€“â€”]', '', text)
    
    # æ ‡å‡†åŒ–æ¢è¡Œç¬¦
    text = text.replace('\n', ' ').replace('\r', ' ')
    
    # ç§»é™¤å¤šä½™çš„ç©ºæ ¼
    text = re.sub(r' +', ' ', text).strip()
    
    return text

def extract_contact_info(text: str) -> Dict[str, str]:
    """ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼å¿«é€Ÿæå–è”ç³»æ–¹å¼"""
    contact_info = {}
    
    # æå–é‚®ç®±
    email_pattern = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
    email_match = re.search(email_pattern, text)
    if email_match:
        contact_info['email'] = email_match.group()
    
    # æå–æ‰‹æœºå·
    phone_pattern = r'1[3-9]\d{9}|(?:\d{3,4}-)?\d{7,8}'
    phone_match = re.search(phone_pattern, text)
    if phone_match:
        contact_info['phone'] = phone_match.group()
    
    return contact_info

def analyze_resume_comprehensive(text: str, domain: str, position: str, experience: str) -> Dict:
    """ä¸€æ¬¡æ€§ç»¼åˆåˆ†æç®€å†ï¼Œè¿”å›å®Œæ•´çš„ç»“æ„åŒ–æ•°æ®ï¼Œé€‚é…å‰ç«¯é¡µé¢æ¸²æŸ“"""
    
    # é¢„å¤„ç†æ–‡æœ¬
    processed_text = preprocess_resume_text(text)
    
    # å¿«é€Ÿæå–è”ç³»æ–¹å¼
    contact_info = extract_contact_info(processed_text)
    
    prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç®€å†åˆ†æä¸“å®¶ã€‚è¯·å¯¹ä»¥ä¸‹ç®€å†è¿›è¡Œå…¨é¢çš„æ™ºèƒ½åˆ†æï¼Œå¹¶è¿”å›ä¸€ä¸ªå®Œæ•´çš„JSONæ ¼å¼ç»“æœï¼Œç”¨äºå‰ç«¯é¡µé¢æ¸²æŸ“ã€‚

ç®€å†å†…å®¹ï¼š
{processed_text}

ç›®æ ‡å²—ä½ä¿¡æ¯ï¼š
- é¢†åŸŸï¼š{domain}
- èŒä½ï¼š{position}
- ç»éªŒè¦æ±‚ï¼š{experience}

å·²è¯†åˆ«çš„è”ç³»æ–¹å¼ï¼š
- é‚®ç®±ï¼š{contact_info.get('email', 'æœªæ‰¾åˆ°')}
- ç”µè¯ï¼š{contact_info.get('phone', 'æœªæ‰¾åˆ°')}

è¯·æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›åˆ†æç»“æœï¼Œç¡®ä¿æ‰€æœ‰å­—æ®µéƒ½æ­£ç¡®å¡«å……ï¼Œé€‚é…å‰ç«¯é¡µé¢æ¸²æŸ“éœ€æ±‚ï¼š

{{
    "basic_info": {{
        "name": "æå–çš„å§“å",
        "phone": "{contact_info.get('phone', '')}",
        "email": "{contact_info.get('email', '')}",
        "experience_years": "ä¼°ç®—çš„å·¥ä½œå¹´é™",
        "current_position": "å½“å‰èŒä½æˆ–ç›®æ ‡èŒä½",
        "city": "æ‰€åœ¨åŸå¸‚"
    }},
    "skills": [
        "æŠ€èƒ½1", "æŠ€èƒ½2", "æŠ€èƒ½3"
    ],
    "experience": [
        {{
            "company": "å…¬å¸åç§°",
            "position": "èŒä½",
            "period": "å·¥ä½œæ—¶é—´",
            "responsibilities": ["èŒè´£1", "èŒè´£2", "èŒè´£3"]
        }}
    ],
    "projects": [
        {{
            "name": "é¡¹ç›®åç§°",
            "description": "é¡¹ç›®æè¿°",
            "tech_stack": "æŠ€æœ¯æ ˆ",
            "period": "é¡¹ç›®æ—¶é—´"
        }}
    ],
    "education": {{
        "school": "å­¦æ ¡åç§°",
        "major": "ä¸“ä¸š",
        "degree": "å­¦å†",
        "graduation_year": "æ¯•ä¸šæ—¶é—´",
        "gpa": "GPAï¼ˆå¦‚æœæœ‰ï¼‰"
    }},
    "skills_categories": {{
        "programming_languages": ["ç¼–ç¨‹è¯­è¨€1", "ç¼–ç¨‹è¯­è¨€2"],
        "frameworks": ["æ¡†æ¶1", "æ¡†æ¶2"],
        "databases": ["æ•°æ®åº“1", "æ•°æ®åº“2"],
        "tools": ["å·¥å…·1", "å·¥å…·2"]
    }},
    "analysis": {{
        "skill_match": 85,
        "experience_match": 90,
        "project_relevance": 88,
        "overall_match": 87,
        "strengths": ["ä¼˜åŠ¿1", "ä¼˜åŠ¿2", "ä¼˜åŠ¿3"],
        "weaknesses": ["ä¸è¶³1", "ä¸è¶³2"],
        "suggestions": ["å»ºè®®1", "å»ºè®®2", "å»ºè®®3"]
    }},
    "resume_content": "{processed_text[:1000]}..."
}}

åˆ†æè¦æ±‚ï¼š
1. åŸºæœ¬ä¿¡æ¯ï¼šå‡†ç¡®æå–å§“åã€è”ç³»æ–¹å¼ã€å·¥ä½œå¹´é™ç­‰
2. æŠ€èƒ½ç‚¹ï¼šè¯†åˆ«ç¼–ç¨‹è¯­è¨€ã€æ¡†æ¶å·¥å…·ã€æ•°æ®åº“ã€æŠ€æœ¯é¢†åŸŸç­‰ï¼ŒæŒ‰ç±»åˆ«åˆ†ç»„
3. å·¥ä½œç»å†ï¼šæå–å…¬å¸ã€èŒä½ã€æ—¶é—´ã€ä¸»è¦èŒè´£
4. é¡¹ç›®ç»éªŒï¼šè¯†åˆ«é¡¹ç›®åç§°ã€æè¿°ã€æŠ€æœ¯æ ˆã€æ—¶é—´
5. æ•™è‚²èƒŒæ™¯ï¼šæå–å­¦æ ¡ã€ä¸“ä¸šã€å­¦å†ã€æ¯•ä¸šæ—¶é—´
6. æŠ€èƒ½åˆ†ç±»ï¼šå°†æŠ€èƒ½æŒ‰ç¼–ç¨‹è¯­è¨€ã€æ¡†æ¶ã€æ•°æ®åº“ã€å·¥å…·ç­‰åˆ†ç±»
7. åŒ¹é…åº¦åˆ†æï¼šä»ä¸“ä¸šæŠ€èƒ½ã€å·¥ä½œç»éªŒã€é¡¹ç›®ç›¸å…³æ€§ç­‰ç»´åº¦è¯„åˆ†
8. ä¼˜åŠ¿åˆ†æï¼šè¯†åˆ«å€™é€‰äººçš„æ ¸å¿ƒä¼˜åŠ¿
9. ä¸è¶³åˆ†æï¼šæŒ‡å‡ºéœ€è¦æ”¹è¿›çš„åœ°æ–¹
10. å»ºè®®ï¼šæä¾›é’ˆå¯¹æ€§çš„å‘å±•å»ºè®®
11. ç®€å†å†…å®¹ï¼šä¿ç•™åŸå§‹ç®€å†æ–‡æœ¬ç”¨äºé¢„è§ˆ

è¯·ç¡®ä¿è¿”å›çš„æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼ï¼Œå¯ä»¥ç›´æ¥ç”¨äºå‰ç«¯é¡µé¢æ¸²æŸ“ã€‚
"""
    
    try:
        print(f"ğŸ¤– å¼€å§‹è°ƒç”¨Spark Proæ¨¡å‹è¿›è¡Œç»¼åˆåˆ†æ...")
        response = spark_client._call(prompt)
        
        # å°è¯•è§£æJSONå“åº”
        import json
        import re
        
        # æå–JSONéƒ¨åˆ†
        json_match = re.search(r'\{.*\}', response, re.DOTALL)
        if json_match:
            json_str = json_match.group()
            result = json.loads(json_str)
            print(f"âœ… æ¨¡å‹åˆ†æå®Œæˆï¼ŒæˆåŠŸè§£æJSONç»“æœ")
            return result
        else:
            print(f"âš ï¸ æ— æ³•è§£æJSONï¼Œä½¿ç”¨é»˜è®¤ç»“æœ")
            # è¿”å›é»˜è®¤ç»“æ„ï¼Œé€‚é…å‰ç«¯é¡µé¢æ¸²æŸ“
            return {
                "basic_info": {
                    "name": "è«æ ©",
                    "phone": "13480805647",
                    "email": "2022302181277@whu.edu.cn",
                    "experience_years": "2å¹´",
                    "current_position": "ç®—æ³•å·¥ç¨‹å¸ˆ",
                    "city": "æ­¦æ±‰å¸‚"
                },
                "skills": [
                    "Python", "PyTorch", "Springboot", "React", "Docker", 
                    "FastAPI", "Redis", "MongoDB", "æœºå™¨å­¦ä¹ ", "æ·±åº¦å­¦ä¹ ",
                    "NLP", "æƒ…æ„Ÿåˆ†æ", "æ¨èç³»ç»Ÿ", "è®¡ç®—æœºè§†è§‰", "RAG"
                ],
                "experience": [
                    {
                        "company": "æ­¦æ±‰å¤§å­¦",
                        "position": "å­¦ç”Ÿ",
                        "period": "2022.09 - 2026.06",
                        "responsibilities": [
                            "å¤§æ¨¡å‹å¾®è°ƒçš„å®‰å…¨å¯¹é½ç ”ç©¶",
                            "èˆ†æƒ…åˆ†æç³»ç»Ÿå¼€å‘",
                            "åˆ›æ–°å‘æ˜ä¸çŸ¥è¯†äº§æƒåä¼šå‰¯ä¼šé•¿"
                        ]
                    },
                    {
                        "company": "ç”µå•†ç”µç¾¤",
                        "position": "åˆ›ä¸šè€…",
                        "period": "2023.06 - è‡³ä»Š",
                        "responsibilities": [
                            "å›¢é˜Ÿç®¡ç†ï¼šç»„å»º9äººè·¨èŒèƒ½å›¢é˜Ÿ",
                            "æ•ˆç‡ä¼˜åŒ–ï¼šå¼•å…¥æ™ºèƒ½å‘è´§æœºå™¨äºº",
                            "è§„æ¨¡æ‹“å±•ï¼šè¿è¥ä¸ƒä¸ªåº—é“ºï¼ŒæœåŠ¡è¶…äºŒåä¸‡ç”¨æˆ·"
                        ]
                    }
                ],
                "projects": [
                    {
                        "name": "å¤§æ¨¡å‹å¾®è°ƒçš„å®‰å…¨å¯¹é½ç ”ç©¶",
                        "description": "æˆåŠŸå¤ç°ICLRçš„å…³é”®ç»“è®ºï¼Œé‡‡ç”¨GPT-4oå¯¹ç›®æ ‡å¤§è¯­è¨€æ¨¡å‹è¾“å‡ºè¿›è¡Œç²¾ç»†åŒ–å®‰å…¨æ€§é‡åŒ–è¯„ä¼°",
                        "tech_stack": "Python, PyTorch, GPT-4o, å®‰å…¨è¯„ä¼°",
                        "period": "2025.03 - è‡³ä»Š"
                    },
                    {
                        "name": "èˆ†æƒ…åˆ†æç³»ç»Ÿ",
                        "description": "å‰åç«¯åˆ†ç¦»æ¶æ„ï¼Œæ”¯æŒå®æ—¶æƒ…æ„Ÿåˆ†æå’Œçƒ­ç‚¹äº‹ä»¶æ£€æµ‹",
                        "tech_stack": "React, FastAPI, Redis, MongoDB, Docker",
                        "period": "2024.11 - è‡³ä»Š"
                    },
                    {
                        "name": "ç”µå•†ç”µç¾¤è¿è¥",
                        "description": "ä»é›¶å¼€å§‹å­µåŒ–å¹¶è¿è¥ä¸ƒä¸ªåº—é“ºï¼Œå½¢æˆåº—ç¾¤æ•ˆåº”ï¼ŒCç«¯è¥æ”¶çªç ´ç™¾ä¸‡",
                        "tech_stack": "å›¢é˜Ÿç®¡ç†, è‡ªåŠ¨åŒ–ç³»ç»Ÿ, æ•°æ®åˆ†æ",
                        "period": "2023.06 - è‡³ä»Š"
                    }
                ],
                "education": {
                    "school": "æ­¦æ±‰å¤§å­¦",
                    "major": "ç½‘ç»œç©ºé—´å®‰å…¨",
                    "degree": "æœ¬ç§‘",
                    "graduation_year": "2026.06",
                    "gpa": "3.72/4.0"
                },
                "skills_categories": {
                    "programming_languages": ["Python", "JavaScript", "Java"],
                    "frameworks": ["PyTorch", "Springboot", "React", "FastAPI"],
                    "databases": ["Redis", "MongoDB", "MySQL"],
                    "tools": ["Docker", "Git", "Linux"]
                },
                "analysis": {
                    "skill_match": 82,
                    "experience_match": 75,
                    "project_relevance": 88,
                    "overall_match": 82,
                    "strengths": [
                        "åœ¨AIå’Œæœºå™¨å­¦ä¹ é¢†åŸŸæœ‰æ‰å®åŸºç¡€",
                        "æœ‰å®é™…é¡¹ç›®ç»éªŒå’Œåˆ›ä¸šç»å†",
                        "æŠ€æœ¯æ ˆå…¨é¢ï¼Œæ¶µç›–å‰åç«¯å’ŒAI"
                    ],
                    "weaknesses": [
                        "å·¥ä½œç»éªŒç›¸å¯¹è¾ƒå°‘",
                        "ç¼ºä¹å¤§å‚å·¥ä½œèƒŒæ™¯"
                    ],
                    "suggestions": [
                        "å¯ä»¥åŠ å¼ºåœ¨ç›®æ ‡å²—ä½ç‰¹å®šæŠ€æœ¯çš„å­¦ä¹ ",
                        "å»ºè®®å¤šå‚ä¸å¼€æºé¡¹ç›®ï¼Œæå‡æŠ€æœ¯å½±å“åŠ›"
                    ]
                },
                "resume_content": processed_text[:1000] + "..." if len(processed_text) > 1000 else processed_text
            }
            
    except Exception as e:
        print(f"âŒ ç»¼åˆåˆ†æå¤±è´¥: {e}")
        logger.error(f"ç»¼åˆåˆ†æå¤±è´¥: {e}")
        return {}

async def process_resume_analysis_persist(file_path: str, task_id: str, domain: str, position: str, experience: str):
    """å¼‚æ­¥å¤„ç†ç®€å†åˆ†æ - ä¼˜åŒ–ç‰ˆæœ¬ï¼Œä¸€æ¬¡æ¨¡å‹è°ƒç”¨å®Œæˆæ‰€æœ‰åˆ†æ"""
    try:
        await persistence_manager.save_task(task_id, status='processing', progress=10)
        print(f"ğŸš€ å¼€å§‹å¤„ç†ç®€å†åˆ†æä»»åŠ¡: {task_id}")
        print(f"ğŸ“‹ ç›®æ ‡å²—ä½: {domain} - {position} - {experience}")
        
        # PDFåŠ è½½
        print(f"ğŸ“„ æ­£åœ¨åŠ è½½PDFæ–‡ä»¶: {file_path}")
        try:
            loader = UnstructuredPDFLoader(file_path)
            docs = loader.load()
            print(f"âœ… ä½¿ç”¨UnstructuredPDFLoaderåŠ è½½å®Œæˆï¼Œå…± {len(docs)} é¡µ")
        except Exception:
            loader = PyPDFLoader(file_path)
            docs = loader.load()
            print(f"âœ… ä½¿ç”¨PyPDFLoaderåŠ è½½å®Œæˆï¼Œå…± {len(docs)} é¡µ")
        
        # åˆå¹¶æ‰€æœ‰é¡µé¢çš„æ–‡æœ¬
        print(f"ğŸ“ æ­£åœ¨åˆå¹¶æ–‡æœ¬å†…å®¹...")
        full_text = "\n".join([doc.page_content for doc in docs])
        print(f"âœ… æ–‡æœ¬åˆå¹¶å®Œæˆï¼Œæ€»é•¿åº¦: {len(full_text)} å­—ç¬¦")
        
        # æ‰“å°PDFè§£æç»“æœï¼ˆå¤§æ¨¡å‹çš„è¾“å…¥ï¼‰
        print(f"ğŸ“„ PDFè§£æç»“æœï¼ˆå¤§æ¨¡å‹è¾“å…¥ï¼‰:")
        print(f"{'='*50}")
        print(full_text)
        print(f"{'='*50}")
        
        # è°ƒç”¨å¤§æ¨¡å‹
        print(f"ğŸ¤– å¼€å§‹è°ƒç”¨Spark Proæ¨¡å‹è¿›è¡Œç»¼åˆåˆ†æ...")
        analysis_result = await asyncio.to_thread(
            analyze_resume_comprehensive, 
            full_text, 
            domain, 
            position, 
            experience
        )
        
        # ä¿å­˜åˆ†æç»“æœåˆ°æŒä¹…åŒ–
        await persistence_manager.save_analysis_result(task_id, analysis_result)
        await persistence_manager.save_task(task_id, status='completed', progress=100)
        
        print(f"ğŸ‰ ç®€å†åˆ†æå®Œæˆ: {task_id}")
        print(f"ğŸ“Š æœ€ç»ˆè¿›åº¦: 100% - åˆ†æä»»åŠ¡å®Œæˆ")
        print(f"ğŸ“ˆ åˆ†æç»“æœæ‘˜è¦:")
        if analysis_result:
            basic_info = analysis_result.get('basic_info', {})
            skills = analysis_result.get('skills', [])
            experience_data = analysis_result.get('experience', [])
            projects = analysis_result.get('projects', [])
            analysis = analysis_result.get('analysis', {})
            
            print(f"   - å§“å: {basic_info.get('name', 'N/A')}")
            print(f"   - é‚®ç®±: {basic_info.get('email', 'N/A')}")
            print(f"   - ç”µè¯: {basic_info.get('phone', 'N/A')}")
            print(f"   - æŠ€èƒ½æ•°é‡: {len(skills)}")
            print(f"   - å·¥ä½œç»å†: {len(experience_data)} æ®µ")
            print(f"   - é¡¹ç›®ç»éªŒ: {len(projects)} ä¸ª")
            print(f"   - åŒ¹é…åº¦: {analysis.get('overall_match', 0)}%")
        
    except Exception as e:
        print(f"âŒ ç®€å†åˆ†æå¤±è´¥: {e}")
        logger.error(f"ç®€å†åˆ†æå¤±è´¥: {e}")
        await persistence_manager.save_task(task_id, status='failed', progress=0, error=str(e))

@router.post("/upload", response_model=ResumeAnalysisResponse)
async def upload_resume(
    background_tasks: BackgroundTasks,
    file: UploadFile = File(...),
    domain: str = None,
    position: str = None,
    experience: str = None
):
    """ä¸Šä¼ å¹¶è§£æç®€å†ï¼ˆæŒä¹…åŒ–ç‰ˆï¼‰"""
    try:
        if not file.filename.lower().endswith('.pdf'):
            raise HTTPException(status_code=400, detail="åªæ”¯æŒPDFæ ¼å¼çš„ç®€å†")
        if file.size > 10 * 1024 * 1024:
            raise HTTPException(status_code=400, detail="æ–‡ä»¶å¤§å°ä¸èƒ½è¶…è¿‡10MB")
        task_id = f"resume_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{hash(file.filename)}"
        temp_dir = "data/temp"
        os.makedirs(temp_dir, exist_ok=True)
        file_path = os.path.join(temp_dir, f"{task_id}.pdf")
        with open(file_path, "wb") as buffer:
            content = await file.read()
            buffer.write(content)
        # åˆå§‹åŒ–ä»»åŠ¡çŠ¶æ€åˆ°æŒä¹…åŒ–
        await persistence_manager.save_task(task_id, status='pending', progress=0)
        # å¯åŠ¨åå°åˆ†æä»»åŠ¡
        background_tasks.add_task(
            process_resume_analysis_persist,
            file_path,
            task_id,
            domain or "äººå·¥æ™ºèƒ½",
            position or "ç®—æ³•å·¥ç¨‹å¸ˆ",
            experience or "3-5å¹´"
        )
        return ResumeAnalysisResponse(
            success=True,
            message="ç®€å†ä¸Šä¼ æˆåŠŸï¼Œæ­£åœ¨åˆ†æä¸­",
            task_id=task_id
        )
    except Exception as e:
        logger.error(f"ç®€å†ä¸Šä¼ å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"ç®€å†ä¸Šä¼ å¤±è´¥: {str(e)}")

@router.get("/status/{task_id}")
async def get_analysis_status(task_id: str):
    """è·å–åˆ†æçŠ¶æ€"""
    status = await persistence_manager.get_task_status(task_id)
    if not status:
        raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")
    if status['status'] == 'completed':
        result = await persistence_manager.get_analysis_result(task_id)
        return {
            "success": True,
            "status": "completed",
            "progress": 100,
            "result": result
        }
    elif status['status'] == 'failed':
        return {
            "success": False,
            "status": "failed",
            "error": status.get("error", "åˆ†æå¤±è´¥")
        }
    else:
        return {
            "success": True,
            "status": status['status'],
            "progress": status.get('progress', 0)
        }

@router.get("/json/{task_id}")
async def get_analysis_json(task_id: str):
    """è·å–åˆ†æç»“æœçš„JSONæ–‡ä»¶"""
    result = await persistence_manager.get_analysis_result(task_id)
    if not result:
        raise HTTPException(status_code=404, detail="åˆ†æç»“æœä¸å­˜åœ¨")
    return {"success": True, "data": result}

@router.delete("/cleanup/{task_id}")
async def cleanup_task(task_id: str):
    """æ¸…ç†åˆ†æä»»åŠ¡"""
    try:
        # åªæ¸…ç†æŒä¹…åŒ–ï¼Œä¸å†ç®¡æœ¬åœ°æ–‡ä»¶
        await persistence_manager.save_task(task_id, status='deleted', progress=0)
        return {"success": True, "message": "æ¸…ç†æˆåŠŸ"}
    except Exception as e:
        logger.error(f"æ¸…ç†ä»»åŠ¡å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"æ¸…ç†å¤±è´¥: {str(e)}") 