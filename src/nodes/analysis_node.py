"""
ç»¼åˆåˆ†æèŠ‚ç‚¹ - "è¯„ä¼°å¼•æ“"
å¤„ç†å¤šæ¨¡æ€æ•°æ®å¹¶ç”Ÿæˆè¯„ä¼°ç»“æœçš„ç¡®å®šæ€§èŠ‚ç‚¹
æ³¨æ„ï¼šè¿™æ˜¯ç®€åŒ–ç‰ˆæœ¬ï¼Œå®é™…é¡¹ç›®ä¸­éœ€è¦é›†æˆè§†è§‰ã€å¬è§‰åˆ†æç­‰ç»„ä»¶
"""
import json
import time
from typing import Dict, Any, List
from dataclasses import asdict

from ..models.state import InterviewState, MultimodalAnalysis
from ..models.spark_client import create_spark_model


class ComprehensiveAnalysisNode:
    """ç»¼åˆåˆ†æèŠ‚ç‚¹"""
    
    def __init__(self):
        # ä½¿ç”¨Spark Ultraæ¨¡å‹è¿›è¡Œæœ€ç»ˆè¯„åˆ¤
        self.llm = create_spark_model("ultra")
        
        # è¯„ä¼°ç»´åº¦å®šä¹‰
        self.assessment_dimensions = {
            "professional_knowledge": "ä¸“ä¸šçŸ¥è¯†æ°´å¹³",
            "skill_match": "æŠ€èƒ½åŒ¹é…åº¦", 
            "communication_ability": "è¯­è¨€è¡¨è¾¾èƒ½åŠ›",
            "logical_thinking": "é€»è¾‘æ€ç»´èƒ½åŠ›",
            "stress_resilience": "åº”å˜æŠ—å‹èƒ½åŠ›"
        }
    
    def _analyze_text_content(self, conversation_history: List) -> Dict[str, Any]:
        """åˆ†ææ–‡æœ¬å†…å®¹"""
        
        # æå–æ‰€æœ‰å›ç­”æ–‡æœ¬
        all_answers = []
        for turn in conversation_history:
            all_answers.append(turn.answer)
        
        combined_text = "\n\n".join(all_answers)
        
        # åˆ†æå›ç­”è´¨é‡
        analysis = {
            "total_words": len(combined_text.split()),
            "total_answers": len(all_answers),
            "avg_answer_length": len(combined_text) / len(all_answers) if all_answers else 0,
            "detailed_answers": sum(1 for answer in all_answers if len(answer) > 100),
            "technical_terms_mentioned": self._count_technical_terms(combined_text),
            "star_structure_usage": self._detect_star_structure(all_answers)
        }
        
        return analysis
    
    def _count_technical_terms(self, text: str) -> int:
        """ç»Ÿè®¡æŠ€æœ¯æœ¯è¯­æåŠæ¬¡æ•°ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        
        technical_terms = [
            "ç®—æ³•", "æœºå™¨å­¦ä¹ ", "æ·±åº¦å­¦ä¹ ", "ç¥ç»ç½‘ç»œ", "æ¨¡å‹", "æ•°æ®",
            "Python", "Java", "JavaScript", "React", "Vue", "Node.js",
            "æ•°æ®åº“", "MySQL", "MongoDB", "Redis", "å¾®æœåŠ¡", "API",
            "æ¶æ„", "è®¾è®¡æ¨¡å¼", "æ¡†æ¶", "åº“", "å¼€æº", "GitHub"
        ]
        
        text_lower = text.lower()
        count = 0
        
        for term in technical_terms:
            count += text_lower.count(term.lower())
        
        return count
    
    def _detect_star_structure(self, answers: List[str]) -> Dict[str, Any]:
        """æ£€æµ‹STARç»“æ„ä½¿ç”¨æƒ…å†µï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        
        star_indicators = {
            "situation": ["é¡¹ç›®", "åœ¨...ä¸­", "å½“æ—¶", "èƒŒæ™¯", "æƒ…å†µ"],
            "task": ["ä»»åŠ¡", "ç›®æ ‡", "è¦æ±‚", "éœ€è¦", "è´Ÿè´£"],
            "action": ["æˆ‘åšäº†", "å®ç°", "ä½¿ç”¨", "é‡‡ç”¨", "è§£å†³", "å¤„ç†"],
            "result": ["ç»“æœ", "æ•ˆæœ", "æˆåŠŸ", "å®Œæˆ", "æå‡", "æ”¹å–„"]
        }
        
        star_usage = {"situation": 0, "task": 0, "action": 0, "result": 0}
        
        for answer in answers:
            for component, indicators in star_indicators.items():
                for indicator in indicators:
                    if indicator in answer:
                        star_usage[component] += 1
                        break
        
        # è®¡ç®—STARå®Œæ•´æ€§
        star_completeness = sum(1 for count in star_usage.values() if count > 0) / 4
        
        return {
            "usage_by_component": star_usage,
            "completeness_score": star_completeness,
            "total_star_answers": sum(star_usage.values())
        }
    
    def _perform_real_multimodal_analysis(self, state: InterviewState) -> Dict[str, Any]:
        """æ‰§è¡ŒçœŸå®çš„å¤šæ¨¡æ€åˆ†æ"""
        
        conversation_history = state.get("conversation_history", [])
        
        # å°è¯•è¿›è¡ŒçœŸå®çš„éŸ³è§†é¢‘åˆ†æ
        try:
            from ..tools.multimodal_analyzer import create_multimodal_analyzer
            from ..tools.star_classifier import create_star_classifier
            from ..tools.skill_matcher import create_skill_matcher
            
            multimodal_analyzer = create_multimodal_analyzer()
            star_classifier = create_star_classifier()
            skill_matcher = create_skill_matcher()
            
            # æ£€æŸ¥æ˜¯å¦æœ‰éŸ³è§†é¢‘æ–‡ä»¶è·¯å¾„
            video_path = state.get("video_path")
            audio_path = state.get("audio_path")
            
            # æ‰§è¡ŒéŸ³è§†é¢‘åˆ†æ
            if video_path or audio_path:
                media_analysis = multimodal_analyzer.analyze_interview_media(
                    video_path=video_path,
                    audio_path=audio_path
                )
                visual_analysis = media_analysis.get("visual_analysis", {})
                audio_analysis = media_analysis.get("audio_analysis", {})
            else:
                # å¦‚æœæ²¡æœ‰éŸ³è§†é¢‘æ–‡ä»¶ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®
                visual_analysis = self._get_fallback_visual_analysis()
                audio_analysis = self._get_fallback_audio_analysis()
            
            # æ–‡æœ¬åˆ†æï¼ˆå¢å¼ºç‰ˆï¼‰
            text_analysis = self._analyze_text_content_enhanced(
                conversation_history, star_classifier, skill_matcher, state
            )
            
            return {
                "visual_analysis": visual_analysis,
                "audio_analysis": audio_analysis, 
                "text_analysis": text_analysis
            }
            
        except ImportError as e:
            print(f"âš ï¸ å¤šæ¨¡æ€åˆ†ææ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
            return self._simulate_multimodal_analysis_fallback(state)
        except Exception as e:
            print(f"âš ï¸ å¤šæ¨¡æ€åˆ†ææ‰§è¡Œå¤±è´¥: {e}")
            return self._simulate_multimodal_analysis_fallback(state)
    
    def _analyze_text_content_enhanced(
        self, 
        conversation_history: List,
        star_classifier,
        skill_matcher,
        state: InterviewState
    ) -> Dict[str, Any]:
        """å¢å¼ºç‰ˆæ–‡æœ¬å†…å®¹åˆ†æ"""
        
        # åŸºç¡€æ–‡æœ¬åˆ†æ
        basic_analysis = self._analyze_text_content(conversation_history)
        
        # æå–æ‰€æœ‰å›ç­”æ–‡æœ¬
        all_answers = []
        for turn in conversation_history:
            all_answers.append(turn.answer)
        
        combined_text = "\n\n".join(all_answers)
        
        # STARç»“æ„åˆ†æ
        try:
            star_analysis = star_classifier.analyze_star_structure(combined_text)
            basic_analysis["star_structure_analysis"] = star_analysis
        except Exception as e:
            print(f"âš ï¸ STARç»“æ„åˆ†æå¤±è´¥: {e}")
            basic_analysis["star_structure_analysis"] = {
                "completeness_score": 0.5,
                "overall_assessment": "STARç»“æ„åˆ†æä¸å¯ç”¨"
            }
        
        # æŠ€èƒ½åŒ¹é…åˆ†æ
        try:
            user_info = state.get("user_info")
            resume_summary = user_info.resume_summary if user_info else {}
            
            # ä»ç®€å†ä¸­æå–æŠ€èƒ½
            resume_skills = []
            if "skills" in resume_summary:
                skills = resume_summary["skills"]
                for skill_list in skills.values():
                    if isinstance(skill_list, list):
                        resume_skills.extend(skill_list)
            
            # æ ¹æ®ç›®æ ‡é¢†åŸŸæ„å»ºå²—ä½è¦æ±‚
            job_requirements = self._get_job_requirements_by_field(
                user_info.target_field if user_info else "Backend"
            )
            
            skill_analysis = skill_matcher.analyze_skill_match(
                combined_text, resume_skills, job_requirements
            )
            basic_analysis["skill_match_analysis"] = skill_analysis
            
        except Exception as e:
            print(f"âš ï¸ æŠ€èƒ½åŒ¹é…åˆ†æå¤±è´¥: {e}")
            basic_analysis["skill_match_analysis"] = {
                "overall_skill_score": 0.5,
                "detailed_analysis": "æŠ€èƒ½åŒ¹é…åˆ†æä¸å¯ç”¨"
            }
        
        return basic_analysis
    
    def _get_job_requirements_by_field(self, field: str) -> List[str]:
        """æ ¹æ®æŠ€æœ¯é¢†åŸŸè·å–å²—ä½è¦æ±‚"""
        
        requirements_map = {
            "AI": [
                "Python", "æœºå™¨å­¦ä¹ ", "æ·±åº¦å­¦ä¹ ", "TensorFlow", "PyTorch",
                "æ•°æ®åˆ†æ", "ç®—æ³•è®¾è®¡", "ç»Ÿè®¡å­¦", "ç¥ç»ç½‘ç»œ"
            ],
            "Backend": [
                "Java", "Python", "Spring", "æ•°æ®åº“è®¾è®¡", "å¾®æœåŠ¡",
                "APIå¼€å‘", "ç³»ç»Ÿæ¶æ„", "MySQL", "Redis", "Docker"
            ],
            "Frontend": [
                "JavaScript", "React", "Vue", "HTML", "CSS", 
                "å‰ç«¯æ¡†æ¶", "å“åº”å¼è®¾è®¡", "ç”¨æˆ·ä½“éªŒ", "TypeScript"
            ]
        }
        
        return requirements_map.get(field, requirements_map["Backend"])
    
    def _simulate_multimodal_analysis_fallback(self, state: InterviewState) -> Dict[str, Any]:
        """æ¨¡æ‹Ÿå¤šæ¨¡æ€åˆ†æç»“æœï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰"""
        
        conversation_history = state.get("conversation_history", [])
        
        # æ¨¡æ‹Ÿè§†è§‰åˆ†æç»“æœ
        visual_analysis = self._get_fallback_visual_analysis()
        
        # æ¨¡æ‹Ÿå¬è§‰åˆ†æç»“æœ
        audio_analysis = self._get_fallback_audio_analysis()
        
        # åŸºç¡€æ–‡æœ¬åˆ†æ
        text_analysis = self._analyze_text_content(conversation_history)
        
        return {
            "visual_analysis": visual_analysis,
            "audio_analysis": audio_analysis, 
            "text_analysis": text_analysis
        }
    
    def _get_fallback_visual_analysis(self) -> Dict[str, Any]:
        """å¤‡ç”¨è§†è§‰åˆ†æç»“æœ"""
        return {
            "head_pose_stability": 0.75,
            "gaze_stability": 0.70,
            "dominant_emotion": "neutral",
            "emotion_stability": 0.80,
            "eye_contact_ratio": 0.75,
            "confidence_indicators": 0.70
        }
    
    def _get_fallback_audio_analysis(self) -> Dict[str, Any]:
        """å¤‡ç”¨å¬è§‰åˆ†æç»“æœ"""
        return {
            "speech_rate_bpm": 140,
            "pitch_variance": 25.5,
            "volume_stability": 0.85,
            "clarity_score": 0.90
        }
    
    def _generate_comprehensive_assessment(
        self, 
        state: InterviewState,
        multimodal_data: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ç”Ÿæˆç»¼åˆè¯„ä¼°ç»“æœ"""
        
        user_info = state["user_info"]
        conversation_history = state["conversation_history"]
        
        # æ„å»ºè¯„ä¼°æç¤º
        assessment_prompt = f"""
ä½ æ˜¯ä¸€ä½èµ„æ·±çš„HRåˆ†æä¸“å®¶å’ŒæŠ€æœ¯é¢è¯•å®˜ã€‚è¯·æ ¹æ®ä»¥ä¸‹æä¾›çš„å¤šç»´åº¦ä¿¡æ¯ï¼Œå¯¹å€™é€‰äººçš„å„é¡¹æ ¸å¿ƒèƒ½åŠ›è¿›è¡Œç»¼åˆè¯„ä¼°ï¼Œå¹¶æŒ‰ç…§æŒ‡å®šçš„JSONæ ¼å¼è¾“å‡ºä½ çš„è¯„åˆ†ï¼ˆ1-10åˆ†ï¼‰å’Œå…·ä½“è¯„è¯­ã€‚

# è¾“å…¥æ•°æ®
---
## å€™é€‰äººä¿¡æ¯
- **å§“å**: {user_info.name}
- **ç›®æ ‡å²—ä½**: {user_info.target_position}
- **ç›®æ ‡é¢†åŸŸ**: {user_info.target_field}
- **ç®€å†æ‘˜è¦**: {user_info.resume_summary.get('summary', 'æ— ç®€å†ä¿¡æ¯')}

## é¢è¯•è¡¨ç°æ•°æ®
### é—®ç­”å®å½•
"""
        
        # æ·»åŠ å¯¹è¯å†å²
        for i, turn in enumerate(conversation_history):
            assessment_prompt += f"- **Q{i+1}**: {turn.question}\n"
            assessment_prompt += f"- **A{i+1}**: {turn.answer}\n\n"
        
        # æ·»åŠ å¤šæ¨¡æ€åˆ†ææ•°æ®
        assessment_prompt += f"""
### å¤šæ¨¡æ€åˆ†ææŠ¥å‘Š
- **æ–‡æœ¬åˆ†æ**: 
  - æ€»å›ç­”æ•°: {multimodal_data['text_analysis']['total_answers']}
  - å¹³å‡å›ç­”é•¿åº¦: {multimodal_data['text_analysis']['avg_answer_length']:.0f}å­—
  - æŠ€æœ¯æœ¯è¯­ä½¿ç”¨: {multimodal_data['text_analysis']['technical_terms_mentioned']}æ¬¡
  - STARç»“æ„å®Œæ•´æ€§: {multimodal_data['text_analysis']['star_structure_usage']['completeness_score']:.2f}

- **è§†è§‰åˆ†æ**: 
  - å¤´éƒ¨å§¿æ€ç¨³å®šæ€§: {multimodal_data['visual_analysis'].get('head_pose_stability', 0.7):.2f}
  - çœ¼ç¥æ¥è§¦æ¯”ä¾‹: {multimodal_data['visual_analysis'].get('eye_contact_ratio', 0.7):.2f}
  - ä¸»å¯¼æƒ…ç»ª: {multimodal_data['visual_analysis'].get('dominant_emotion', 'neutral')}
  - æƒ…ç»ªç¨³å®šæ€§: {multimodal_data['visual_analysis'].get('emotion_stability', 0.8):.2f}
  - åˆ†æçŠ¶æ€: {'çœŸå®åˆ†æ' if not multimodal_data['visual_analysis'].get('error') else 'é»˜è®¤æ•°æ®'}

- **å¬è§‰åˆ†æ**:
  - è¯­é€Ÿ: {multimodal_data['audio_analysis'].get('speech_rate_bpm', 120):.1f} BPM
  - éŸ³è°ƒå˜åŒ–åº¦: {multimodal_data['audio_analysis'].get('pitch_variance', 20):.1f}
  - è¯­éŸ³æ¸…æ™°åº¦: {multimodal_data['audio_analysis'].get('clarity_score', 0.8):.2f}
  - åˆ†æçŠ¶æ€: {'çœŸå®åˆ†æ' if not multimodal_data['audio_analysis'].get('error') else 'é»˜è®¤æ•°æ®'}
---

# è¾“å‡ºè¦æ±‚
è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼æä¾›ä½ çš„è¯„ä¼°ç»“æœï¼Œä¸è¦æ·»åŠ ä»»ä½•é¢å¤–çš„è§£é‡Šæˆ–æ–‡æœ¬ï¼š

{{
  "professional_knowledge": {{"score": 0, "comment": "åœ¨æ­¤å¤„å¡«å†™å¯¹ä¸“ä¸šçŸ¥è¯†æ°´å¹³çš„å…·ä½“è¯„è¯­..."}},
  "skill_match": {{"score": 0, "comment": "åœ¨æ­¤å¤„å¡«å†™å¯¹æŠ€èƒ½åŒ¹é…åº¦çš„å…·ä½“è¯„è¯­..."}},
  "communication_ability": {{"score": 0, "comment": "åœ¨æ­¤å¤„å¡«å†™å¯¹è¯­è¨€è¡¨è¾¾èƒ½åŠ›çš„å…·ä½“è¯„è¯­..."}},
  "logical_thinking": {{"score": 0, "comment": "åœ¨æ­¤å¤„å¡«å†™å¯¹é€»è¾‘æ€ç»´èƒ½åŠ›ï¼ˆç‰¹åˆ«æ˜¯STARç»“æ„ï¼‰çš„å…·ä½“è¯„è¯­..."}},
  "stress_resilience": {{"score": 0, "comment": "åœ¨æ­¤å¤„å¡«å†™å¯¹åº”å˜æŠ—å‹èƒ½åŠ›ï¼ˆç»“åˆæƒ…ç»ªå’Œéè¨€è¯­çº¿ç´¢ï¼‰çš„å…·ä½“è¯„è¯­..."}}
}}
"""
        
        try:
            # è°ƒç”¨LLMè¿›è¡Œè¯„ä¼°
            response = self.llm(assessment_prompt)
            
            # æ¸…ç†å“åº”å¹¶è§£æJSON
            response = response.strip()
            if response.startswith("```json"):
                response = response[7:]
            if response.endswith("```"):
                response = response[:-3]
            
            assessment_result = json.loads(response)
            
            # éªŒè¯ç»“æœæ ¼å¼
            for dimension in self.assessment_dimensions.keys():
                if dimension not in assessment_result:
                    assessment_result[dimension] = {
                        "score": 5,
                        "comment": "è¯„ä¼°æ•°æ®ä¸å®Œæ•´"
                    }
            
            return assessment_result
            
        except json.JSONDecodeError as e:
            print(f"âš ï¸ è¯„ä¼°ç»“æœJSONè§£æå¤±è´¥: {e}")
            
            # è¿”å›é»˜è®¤è¯„ä¼°ç»“æœ
            return {
                dimension: {
                    "score": 5,
                    "comment": "è¯„ä¼°è¿‡ç¨‹ä¸­å‡ºç°é—®é¢˜ï¼Œæ— æ³•ç”Ÿæˆè¯¦ç»†è¯„è¯­"
                }
                for dimension in self.assessment_dimensions.keys()
            }
        
        except Exception as e:
            print(f"âš ï¸ ç»¼åˆè¯„ä¼°å¤±è´¥: {e}")
            
            # è¿”å›é»˜è®¤è¯„ä¼°ç»“æœ
            return {
                dimension: {
                    "score": 5,
                    "comment": "ç³»ç»Ÿè¯„ä¼°å¼‚å¸¸"
                }
                for dimension in self.assessment_dimensions.keys()
            }
    
    def analyze(self, state: InterviewState) -> InterviewState:
        """æ‰§è¡Œç»¼åˆåˆ†æ"""
        
        print("ğŸ“Š å¼€å§‹å¤šæ¨¡æ€åˆ†æ...")
        
        try:
            # æ£€æŸ¥å¿…è¦æ•°æ®
            conversation_history = state.get("conversation_history", [])
            if not conversation_history:
                raise Exception("æ²¡æœ‰å¯åˆ†æçš„å¯¹è¯æ•°æ®")
            
            # æ£€æŸ¥éŸ³è§†é¢‘æ–‡ä»¶çŠ¶æ€
            video_path = state.get("video_path")
            audio_path = state.get("audio_path")
            
            print(f"  ğŸ“ è§†é¢‘æ–‡ä»¶: {video_path or 'æœªæä¾›'}")
            print(f"  ğŸ“ éŸ³é¢‘æ–‡ä»¶: {audio_path or 'æœªæä¾›'}")
            
            # 1. æ‰§è¡Œå¤šæ¨¡æ€åˆ†æï¼ˆçœŸå®å®ç° + å¤‡ç”¨æ–¹æ¡ˆï¼‰
            print("  ğŸ” æ‰§è¡Œå¤šæ¨¡æ€ç‰¹å¾æå–...")
            multimodal_data = self._perform_real_multimodal_analysis(state)
            
            # 2. ç”Ÿæˆç»¼åˆè¯„ä¼°
            print("  ğŸ§  ç”Ÿæˆç»¼åˆè¯„ä¼°...")
            comprehensive_assessment = self._generate_comprehensive_assessment(
                state, multimodal_data
            )
            
            # 3. æ„å»ºåˆ†æç»“æœ
            analysis_result = MultimodalAnalysis(
                visual_analysis=multimodal_data["visual_analysis"],
                audio_analysis=multimodal_data["audio_analysis"],
                text_analysis=multimodal_data["text_analysis"],
                comprehensive_assessment=comprehensive_assessment
            )
            
            # 4. æ›´æ–°çŠ¶æ€
            state["multimodal_analysis"] = analysis_result
            
            # 5. æ‰“å°ç»“æœæ‘˜è¦
            self._print_analysis_summary(comprehensive_assessment)
            
            # 6. æ‰“å°å¤šæ¨¡æ€åˆ†æçŠ¶æ€
            self._print_multimodal_status(multimodal_data)
            
            print("âœ… å¤šæ¨¡æ€åˆ†æå®Œæˆ")
            return state
            
        except Exception as e:
            error_msg = f"å¤šæ¨¡æ€åˆ†æå¤±è´¥: {str(e)}"
            print(f"âŒ {error_msg}")
            state["errors"].append(error_msg)
            return state
    
    def _print_analysis_summary(self, assessment: Dict[str, Any]):
        """æ‰“å°åˆ†æç»“æœæ‘˜è¦"""
        
        print("\nğŸ“ˆ è¯„ä¼°ç»“æœæ‘˜è¦:")
        print("-" * 40)
        
        total_score = 0
        for dimension_key, dimension_name in self.assessment_dimensions.items():
            if dimension_key in assessment:
                score = assessment[dimension_key].get("score", 0)
                total_score += score
                print(f"  {dimension_name}: {score}/10")
        
        avg_score = total_score / len(self.assessment_dimensions)
        print(f"\n  æ€»ä½“è¯„åˆ†: {avg_score:.1f}/10")
        print("-" * 40)
    
    def _print_multimodal_status(self, multimodal_data: Dict[str, Any]):
        """æ‰“å°å¤šæ¨¡æ€åˆ†æçŠ¶æ€"""
        
        print("\nğŸ¥ å¤šæ¨¡æ€åˆ†æçŠ¶æ€:")
        print("-" * 40)
        
        # è§†è§‰åˆ†æçŠ¶æ€
        visual_analysis = multimodal_data.get("visual_analysis", {})
        if visual_analysis.get("error"):
            print("  ğŸ¥ è§†è§‰åˆ†æ: ä½¿ç”¨é»˜è®¤æ•°æ®")
        else:
            print("  ğŸ¥ è§†è§‰åˆ†æ: çœŸå®åˆ†æå®Œæˆ")
            if "head_pose_stability" in visual_analysis:
                print(f"    å¤´éƒ¨å§¿æ€ç¨³å®šæ€§: {visual_analysis['head_pose_stability']:.2f}")
            if "eye_contact_ratio" in visual_analysis:
                print(f"    çœ¼ç¥æ¥è§¦æ¯”ä¾‹: {visual_analysis['eye_contact_ratio']:.2f}")
            if "dominant_emotion" in visual_analysis:
                print(f"    ä¸»å¯¼æƒ…ç»ª: {visual_analysis['dominant_emotion']}")
        
        # å¬è§‰åˆ†æçŠ¶æ€
        audio_analysis = multimodal_data.get("audio_analysis", {})
        if audio_analysis.get("error"):
            print("  ğŸµ å¬è§‰åˆ†æ: ä½¿ç”¨é»˜è®¤æ•°æ®")
        else:
            print("  ğŸµ å¬è§‰åˆ†æ: çœŸå®åˆ†æå®Œæˆ")
            if "speech_rate_bpm" in audio_analysis:
                print(f"    è¯­é€Ÿ: {audio_analysis['speech_rate_bpm']:.1f} BPM")
            if "clarity_score" in audio_analysis:
                print(f"    æ¸…æ™°åº¦: {audio_analysis['clarity_score']:.2f}")
            if "pitch_variance" in audio_analysis:
                print(f"    éŸ³è°ƒå˜åŒ–: {audio_analysis['pitch_variance']:.1f}")
        
        # æ–‡æœ¬åˆ†æçŠ¶æ€
        text_analysis = multimodal_data.get("text_analysis", {})
        print("  ğŸ“ æ–‡æœ¬åˆ†æ: å®Œæˆ")
        if "total_answers" in text_analysis:
            print(f"    å›ç­”æ•°é‡: {text_analysis['total_answers']}")
        if "technical_terms_mentioned" in text_analysis:
            print(f"    æŠ€æœ¯æœ¯è¯­: {text_analysis['technical_terms_mentioned']} æ¬¡")
        
        print("-" * 40)


def create_analysis_node() -> ComprehensiveAnalysisNode:
    """åˆ›å»ºç»¼åˆåˆ†æèŠ‚ç‚¹å®ä¾‹"""
    return ComprehensiveAnalysisNode() 