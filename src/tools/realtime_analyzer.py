"""
å®æ—¶å¤šæ¨¡æ€åˆ†æå¤„ç†å™¨
ä¸“ä¸ºWebSocketå®æ—¶ä¼ è¾“ä¼˜åŒ–çš„è½»é‡çº§åˆ†ææ¨¡å—
"""
import cv2
import numpy as np
import mediapipe as mp
import librosa
import io
import tempfile
import logging
from typing import Dict, Any, Optional, Tuple
from datetime import datetime
import time
import traceback

try:
    from deepface import DeepFace
    DEEPFACE_AVAILABLE = True
except ImportError:
    DEEPFACE_AVAILABLE = False
    logging.error("âŒ DeepFace not available, emotion analysis will fail")

logger = logging.getLogger(__name__)


class RealtimeVideoAnalyzer:
    """å®æ—¶è§†é¢‘åˆ†æå™¨ - ä¼˜åŒ–ç‰ˆ"""
    
    def __init__(self):
        # åˆå§‹åŒ–MediaPipe (è½»é‡çº§é…ç½®)
        self.mp_face_mesh = mp.solutions.face_mesh
        self.face_mesh = self.mp_face_mesh.FaceMesh(
            static_image_mode=False,
            max_num_faces=1,
            refine_landmarks=False,  # å…³é—­ç²¾ç»†åŒ–ä»¥æé«˜é€Ÿåº¦
            min_detection_confidence=0.5,  # é™ä½é˜ˆå€¼ä»¥æé«˜é€Ÿåº¦
            min_tracking_confidence=0.5
        )
        
        # é¢éƒ¨å…³é”®ç‚¹ç´¢å¼•ï¼ˆç®€åŒ–ç‰ˆï¼‰
        self.key_landmarks = {
            'nose_tip': 1,
            'chin': 175,
            'left_eye': 33,
            'right_eye': 263,
            'mouth_center': 13
        }
        
        # æƒ…ç»ªåˆ†æé…ç½®
        self.emotion_cache = {}
        self.emotion_cache_duration = 2.0  # ç¼“å­˜2ç§’
        
        logger.info("âœ… å®æ—¶è§†é¢‘åˆ†æå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def analyze_frame(self, frame: np.ndarray) -> Dict[str, Any]:
        """åˆ†æå•å¸§è§†é¢‘"""
        start_time = time.time()
        
        if frame is None or frame.size == 0:
            error_msg = "è¾“å…¥è§†é¢‘å¸§ä¸ºç©ºæˆ–æ— æ•ˆ"
            logger.error(f"âŒ {error_msg}")
            raise ValueError(error_msg)
        
        try:
            # è½¬æ¢é¢œè‰²ç©ºé—´
            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            h, w = frame.shape[:2]
            
            # MediaPipeé¢éƒ¨æ£€æµ‹
            results = self.face_mesh.process(rgb_frame)
            
            analysis_result = {
                'timestamp': datetime.now().isoformat(),
                'processing_time': 0,
                'face_detected': False
            }
            
            if not results.multi_face_landmarks:
                error_msg = "è§†é¢‘å¸§ä¸­æœªæ£€æµ‹åˆ°äººè„¸"
                logger.error(f"âŒ {error_msg}")
                raise Exception(error_msg)
            
            face_landmarks = results.multi_face_landmarks[0]
            analysis_result['face_detected'] = True
            
            # å¤´éƒ¨å§¿æ€åˆ†æï¼ˆè½»é‡çº§ï¼‰
            head_pose = self._analyze_head_pose_light(face_landmarks, (w, h))
            analysis_result.update(head_pose)
            
            # è§†çº¿æ–¹å‘åˆ†æï¼ˆç®€åŒ–ç‰ˆï¼‰
            gaze = self._analyze_gaze_light(face_landmarks, (w, h))
            analysis_result['gaze_direction'] = gaze
            
            # æƒ…ç»ªåˆ†æï¼ˆå¸¦ç¼“å­˜ï¼‰
            emotion = self._analyze_emotion_cached(frame)
            analysis_result.update(emotion)
            
            # è®°å½•å¤„ç†æ—¶é—´
            processing_time = time.time() - start_time
            analysis_result['processing_time'] = round(processing_time * 1000, 2)  # æ¯«ç§’
            
            logger.debug(f"ğŸ¥ è§†é¢‘å¸§åˆ†æå®Œæˆï¼Œè€—æ—¶: {processing_time:.3f}s")
            return analysis_result
            
        except Exception as e:
            logger.error(f"âŒ è§†é¢‘å¸§åˆ†æå¤±è´¥: {e}")
            logger.error(f"ğŸ”§ é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            raise
    
    def _analyze_head_pose_light(self, landmarks, frame_shape) -> Dict[str, float]:
        """è½»é‡çº§å¤´éƒ¨å§¿æ€åˆ†æ"""
        try:
            w, h = frame_shape
            
            # è·å–å…³é”®ç‚¹
            nose = landmarks.landmark[self.key_landmarks['nose_tip']]
            chin = landmarks.landmark[self.key_landmarks['chin']]
            left_eye = landmarks.landmark[self.key_landmarks['left_eye']]
            right_eye = landmarks.landmark[self.key_landmarks['right_eye']]
            
            # è½¬æ¢ä¸ºåƒç´ åæ ‡
            nose_px = (nose.x * w, nose.y * h)
            chin_px = (chin.x * w, chin.y * h)
            left_eye_px = (left_eye.x * w, left_eye.y * h)
            right_eye_px = (right_eye.x * w, right_eye.y * h)
            
            # è®¡ç®—å¤´éƒ¨å€¾æ–œè§’åº¦ï¼ˆç®€åŒ–è®¡ç®—ï¼‰
            eye_center_x = (left_eye_px[0] + right_eye_px[0]) / 2
            eye_center_y = (left_eye_px[1] + right_eye_px[1]) / 2
            
            # å¤´éƒ¨å‚ç›´åç§»
            face_center_x = (nose_px[0] + chin_px[0]) / 2
            horizontal_deviation = abs(face_center_x - w/2) / (w/2)
            
            # å¤´éƒ¨ç¨³å®šæ€§è¯„åˆ†
            stability = max(0.0, 1.0 - horizontal_deviation)
            
            return {
                'head_pose_stability': round(stability, 3),
                'horizontal_deviation': round(horizontal_deviation, 3)
            }
            
        except Exception as e:
            logger.error(f"âŒ å¤´éƒ¨å§¿æ€åˆ†æå¤±è´¥: {e}")
            logger.error(f"ğŸ”§ é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            raise
    
    def _analyze_gaze_light(self, landmarks, frame_shape) -> Dict[str, float]:
        """è½»é‡çº§è§†çº¿æ–¹å‘åˆ†æ"""
        try:
            w, h = frame_shape
            
            # è·å–çœ¼éƒ¨å…³é”®ç‚¹
            left_eye = landmarks.landmark[self.key_landmarks['left_eye']]
            right_eye = landmarks.landmark[self.key_landmarks['right_eye']]
            nose = landmarks.landmark[self.key_landmarks['nose_tip']]
            
            # è®¡ç®—çœ¼éƒ¨ä¸­å¿ƒ
            eye_center_x = (left_eye.x + right_eye.x) / 2
            eye_center_y = (left_eye.y + right_eye.y) / 2
            
            # ä¸é¼»å°–çš„ç›¸å¯¹ä½ç½®
            gaze_x = eye_center_x - nose.x
            gaze_y = eye_center_y - nose.y
            
            return {
                'x': round(gaze_x * 100, 2),  # æ ‡å‡†åŒ–åˆ°-100åˆ°100
                'y': round(gaze_y * 100, 2)
            }
            
        except Exception as e:
            logger.error(f"âŒ è§†çº¿åˆ†æå¤±è´¥: {e}")
            logger.error(f"ğŸ”§ é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            raise
    
    def _analyze_emotion_cached(self, frame: np.ndarray) -> Optional[Dict[str, Any]]:
        """å¸¦ç¼“å­˜çš„æƒ…ç»ªåˆ†æ"""
        current_time = time.time()
        
        # æ£€æŸ¥ç¼“å­˜
        if 'last_analysis' in self.emotion_cache:
            last_time = self.emotion_cache['last_analysis']
            if current_time - last_time < self.emotion_cache_duration:
                return self.emotion_cache.get('result')
        
        # æ‰§è¡Œæ–°çš„æƒ…ç»ªåˆ†æ
        emotion_result = self._analyze_emotion_fast(frame)
        
        # æ›´æ–°ç¼“å­˜
        self.emotion_cache = {
            'last_analysis': current_time,
            'result': emotion_result
        }
        
        return emotion_result
    
    def _analyze_emotion_fast(self, frame: np.ndarray) -> Dict[str, Any]:
        """å¿«é€Ÿæƒ…ç»ªåˆ†æ"""
        if not DEEPFACE_AVAILABLE:
            error_msg = "DeepFaceä¸å¯ç”¨ï¼Œæ— æ³•è¿›è¡Œæƒ…ç»ªåˆ†æ"
            logger.error(f"âŒ {error_msg}")
            raise Exception(error_msg)
        
        try:
            # ç¼©å°å›¾ç‰‡ä»¥æé«˜é€Ÿåº¦
            small_frame = cv2.resize(frame, (224, 224))
            
            result = DeepFace.analyze(
                small_frame,
                actions=['emotion'],
                enforce_detection=False,
                detector_backend='opencv'  # ä½¿ç”¨æœ€å¿«çš„æ£€æµ‹å™¨
            )
            
            if isinstance(result, list):
                result = result[0]
            
            emotions = result.get('emotion', {})
            
            if not emotions:
                error_msg = "DeepFaceè¿”å›ç©ºçš„æƒ…ç»ªç»“æœ"
                logger.error(f"âŒ {error_msg}")
                raise Exception(error_msg)
            
            dominant_emotion = max(emotions.items(), key=lambda x: x[1])
            
            return {
                'dominant_emotion': dominant_emotion[0],
                'emotion_confidence': round(dominant_emotion[1] / 100, 3),
                'emotion_distribution': {k: round(v/100, 3) for k, v in emotions.items()}
            }
            
        except Exception as e:
            logger.error(f"âŒ æƒ…ç»ªåˆ†æå¤±è´¥: {e}")
            logger.error(f"ğŸ”§ é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            raise


class RealtimeAudioAnalyzer:
    """å®æ—¶éŸ³é¢‘åˆ†æå™¨ - ä¼˜åŒ–ç‰ˆ"""
    
    def __init__(self):
        self.sample_rate = 16000  # é™ä½é‡‡æ ·ç‡ä»¥æé«˜é€Ÿåº¦
        self.analysis_cache = {}
        self.cache_duration = 1.0  # ç¼“å­˜1ç§’
        
        logger.info("âœ… å®æ—¶éŸ³é¢‘åˆ†æå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def analyze_chunk(self, audio_bytes: bytes) -> Dict[str, Any]:
        """åˆ†æéŸ³é¢‘ç‰‡æ®µ"""
        start_time = time.time()
        
        if not audio_bytes or len(audio_bytes) == 0:
            error_msg = "è¾“å…¥éŸ³é¢‘æ•°æ®ä¸ºç©º"
            logger.error(f"âŒ {error_msg}")
            raise ValueError(error_msg)
        
        try:
            # å°†éŸ³é¢‘å­—èŠ‚è½¬æ¢ä¸ºnumpy array
            audio_data = self._bytes_to_audio(audio_bytes)
            
            if audio_data is None or len(audio_data) == 0:
                error_msg = "éŸ³é¢‘æ•°æ®è½¬æ¢å¤±è´¥æˆ–ä¸ºç©º"
                logger.error(f"âŒ {error_msg}")
                raise Exception(error_msg)
            
            analysis_result = {
                'timestamp': datetime.now().isoformat(),
                'processing_time': 0,
                'audio_detected': True
            }
            
            # åŸºç¡€éŸ³é¢‘ç‰¹å¾åˆ†æ
            audio_features = self._analyze_audio_features(audio_data)
            analysis_result.update(audio_features)
            
            # è¯­éŸ³æƒ…æ„Ÿåˆ†æï¼ˆç®€åŒ–ç‰ˆï¼‰
            emotion_result = self._analyze_audio_emotion(audio_data)
            analysis_result.update(emotion_result)
            
            # è®°å½•å¤„ç†æ—¶é—´
            processing_time = time.time() - start_time
            analysis_result['processing_time'] = round(processing_time * 1000, 2)
            
            logger.debug(f"ğŸµ éŸ³é¢‘ç‰‡æ®µåˆ†æå®Œæˆï¼Œè€—æ—¶: {processing_time:.3f}s")
            return analysis_result
            
        except Exception as e:
            logger.error(f"âŒ éŸ³é¢‘åˆ†æå¤±è´¥: {e}")
            logger.error(f"ğŸ”§ é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            raise
    
    def _bytes_to_audio(self, audio_bytes: bytes) -> Optional[np.ndarray]:
        """å°†éŸ³é¢‘å­—èŠ‚è½¬æ¢ä¸ºnumpyæ•°ç»„"""
        try:
            # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
            with tempfile.NamedTemporaryFile(suffix='.webm', delete=False) as temp_file:
                temp_file.write(audio_bytes)
                temp_file.flush()
                
                # ä½¿ç”¨librosaåŠ è½½éŸ³é¢‘
                y, sr = librosa.load(temp_file.name, sr=self.sample_rate)
                return y
                
        except Exception as e:
            logger.error(f"âŒ éŸ³é¢‘è½¬æ¢å¤±è´¥: {e}")
            logger.error(f"ğŸ”§ é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            raise
    
    def _analyze_audio_features(self, audio_data: np.ndarray) -> Dict[str, Any]:
        """åˆ†æåŸºç¡€éŸ³é¢‘ç‰¹å¾"""
        try:
            # è¯­é€Ÿä¼°ç®—ï¼ˆåŸºäºé›¶äº¤å‰ç‡ï¼‰
            zcr = librosa.feature.zero_crossing_rate(audio_data)[0]
            if len(zcr) == 0:
                error_msg = "é›¶äº¤å‰ç‡è®¡ç®—å¤±è´¥"
                logger.error(f"âŒ {error_msg}")
                raise Exception(error_msg)
            speech_rate = np.mean(zcr) * 1000  # è½¬æ¢ä¸ºBPMä¼°ç®—
            
            # éŸ³é«˜åˆ†æï¼ˆç®€åŒ–ç‰ˆï¼‰
            pitches, magnitudes = librosa.piptrack(y=audio_data, sr=self.sample_rate)
            pitch_values = pitches[magnitudes > np.max(magnitudes) * 0.1]
            
            if len(pitch_values) == 0:
                error_msg = "éŸ³é«˜åˆ†æå¤±è´¥ï¼šæœªæ£€æµ‹åˆ°æœ‰æ•ˆéŸ³é«˜"
                logger.error(f"âŒ {error_msg}")
                raise Exception(error_msg)
                
            valid_pitches = pitch_values[pitch_values > 0]
            if len(valid_pitches) == 0:
                error_msg = "éŸ³é«˜åˆ†æå¤±è´¥ï¼šæ‰€æœ‰éŸ³é«˜å€¼æ— æ•ˆ"
                logger.error(f"âŒ {error_msg}")
                raise Exception(error_msg)
                
            pitch_mean = np.mean(valid_pitches)
            pitch_variance = np.var(valid_pitches)
            
            # éŸ³é‡åˆ†æ
            rms = librosa.feature.rms(y=audio_data)[0]
            if len(rms) == 0:
                error_msg = "éŸ³é‡åˆ†æå¤±è´¥ï¼šRMSè®¡ç®—ç»“æœä¸ºç©º"
                logger.error(f"âŒ {error_msg}")
                raise Exception(error_msg)
            volume_mean = np.mean(rms)
            
            # æ¸…æ™°åº¦è¯„ä¼°ï¼ˆåŸºäºé¢‘è°±é‡å¿ƒï¼‰
            spectral_centroids = librosa.feature.spectral_centroid(y=audio_data, sr=self.sample_rate)[0]
            if len(spectral_centroids) == 0:
                error_msg = "æ¸…æ™°åº¦åˆ†æå¤±è´¥ï¼šé¢‘è°±é‡å¿ƒè®¡ç®—ç»“æœä¸ºç©º"
                logger.error(f"âŒ {error_msg}")
                raise Exception(error_msg)
            clarity_score = min(1.0, np.mean(spectral_centroids) / 2000)
            
            return {
                'speech_rate': round(speech_rate, 1),
                'pitch_mean': round(pitch_mean, 1),
                'pitch_variance': round(pitch_variance, 1),
                'volume_mean': round(volume_mean, 3),
                'clarity_score': round(clarity_score, 3)
            }
            
        except Exception as e:
            logger.error(f"âŒ éŸ³é¢‘ç‰¹å¾åˆ†æå¤±è´¥: {e}")
            logger.error(f"ğŸ”§ é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            raise
    
    def _analyze_audio_emotion(self, audio_data: np.ndarray) -> Dict[str, Any]:
        """éŸ³é¢‘æƒ…æ„Ÿåˆ†æï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        try:
            # åŸºäºéŸ³é¢‘ç‰¹å¾çš„ç®€å•æƒ…æ„Ÿåˆ¤æ–­
            rms = librosa.feature.rms(y=audio_data)[0]
            if len(rms) == 0:
                error_msg = "æƒ…æ„Ÿåˆ†æå¤±è´¥ï¼šæ— æ³•è®¡ç®—éŸ³é‡ç‰¹å¾"
                logger.error(f"âŒ {error_msg}")
                raise Exception(error_msg)
                
            volume_level = np.mean(rms)
            
            # åŸºäºéŸ³é‡å’Œé¢‘ç‡å˜åŒ–æ¨æ–­æƒ…æ„Ÿ
            if volume_level > 0.7:
                emotion = 'excited'
                confidence = 0.8
            elif volume_level > 0.4:
                emotion = 'confident'
                confidence = 0.7
            elif volume_level > 0.2:
                emotion = 'calm'
                confidence = 0.8
            else:
                emotion = 'uncertain'
                confidence = 0.6
            
            return {
                'emotion': emotion,
                'emotion_confidence': confidence
            }
            
        except Exception as e:
            logger.error(f"âŒ éŸ³é¢‘æƒ…æ„Ÿåˆ†æå¤±è´¥: {e}")
            logger.error(f"ğŸ”§ é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            raise


class RealtimeMultimodalProcessor:
    """å®æ—¶å¤šæ¨¡æ€å¤„ç†å™¨ - ä¸»æ§åˆ¶å™¨"""
    
    def __init__(self):
        self.video_analyzer = RealtimeVideoAnalyzer()
        self.audio_analyzer = RealtimeAudioAnalyzer()
        
        # æ€§èƒ½ç›‘æ§
        self.performance_stats = {
            'video_analysis_count': 0,
            'audio_analysis_count': 0,
            'avg_video_time': 0,
            'avg_audio_time': 0,
            'start_time': time.time(),
            'video_errors': 0,
            'audio_errors': 0
        }
        
        logger.info("âœ… å®æ—¶å¤šæ¨¡æ€å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def analyze_video_frame(self, frame: np.ndarray) -> Dict[str, Any]:
        """åˆ†æè§†é¢‘å¸§"""
        start_time = time.time()
        
        # è®°å½•åˆ†æå¼€å§‹
        frame_info = f"å¸§å¤§å°: {frame.shape}" if frame is not None else "ç©ºå¸§"
        logger.debug(f"ğŸ¥ [åˆ†æå™¨] å¼€å§‹è§†é¢‘å¸§åˆ†æ ({frame_info})")
        
        try:
            result = self.video_analyzer.analyze_frame(frame)
            
            # æ›´æ–°æ€§èƒ½ç»Ÿè®¡
            processing_time = time.time() - start_time
            self.performance_stats['video_analysis_count'] += 1
            count = self.performance_stats['video_analysis_count']
            self.performance_stats['avg_video_time'] = (
                (self.performance_stats['avg_video_time'] * (count - 1) + processing_time) / count
            )
            
            # è®°å½•åˆ†æå®Œæˆå’Œè¯¦ç»†ä¿¡æ¯
            logger.debug(f"âœ… [åˆ†æå™¨] è§†é¢‘å¸§åˆ†æå®Œæˆ:")
            logger.debug(f"   - å¤„ç†æ—¶é—´: {processing_time*1000:.1f}ms")
            logger.debug(f"   - ç´¯è®¡åˆ†æ: {count} å¸§")
            logger.debug(f"   - å¹³å‡è€—æ—¶: {self.performance_stats['avg_video_time']*1000:.1f}ms")
            logger.debug(f"   - å®æ—¶FPS: {1/processing_time:.1f}")
            
            return result
            
        except Exception as e:
            self.performance_stats['video_errors'] += 1
            logger.error(f"âŒ [åˆ†æå™¨] è§†é¢‘å¸§åˆ†æå¤±è´¥: {e}")
            logger.error(f"ğŸ”§ é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            raise
    
    def analyze_audio_chunk(self, audio_bytes: bytes) -> Dict[str, Any]:
        """åˆ†æéŸ³é¢‘ç‰‡æ®µ"""
        start_time = time.time()
        
        # è®°å½•åˆ†æå¼€å§‹
        audio_info = f"éŸ³é¢‘å¤§å°: {len(audio_bytes)} bytes" if audio_bytes else "ç©ºéŸ³é¢‘"
        logger.debug(f"ğŸµ [åˆ†æå™¨] å¼€å§‹éŸ³é¢‘ç‰‡æ®µåˆ†æ ({audio_info})")
        
        try:
            result = self.audio_analyzer.analyze_chunk(audio_bytes)
            
            # æ›´æ–°æ€§èƒ½ç»Ÿè®¡
            processing_time = time.time() - start_time
            self.performance_stats['audio_analysis_count'] += 1
            count = self.performance_stats['audio_analysis_count']
            self.performance_stats['avg_audio_time'] = (
                (self.performance_stats['avg_audio_time'] * (count - 1) + processing_time) / count
            )
            
            # è®°å½•åˆ†æå®Œæˆå’Œè¯¦ç»†ä¿¡æ¯
            logger.debug(f"âœ… [åˆ†æå™¨] éŸ³é¢‘ç‰‡æ®µåˆ†æå®Œæˆ:")
            logger.debug(f"   - å¤„ç†æ—¶é—´: {processing_time*1000:.1f}ms")
            logger.debug(f"   - ç´¯è®¡åˆ†æ: {count} ä¸ªç‰‡æ®µ")
            logger.debug(f"   - å¹³å‡è€—æ—¶: {self.performance_stats['avg_audio_time']*1000:.1f}ms")
            # å‡è®¾éŸ³é¢‘ç‰‡æ®µé€šå¸¸ä¸º3ç§’ï¼Œè®¡ç®—å®æ—¶æ¯”ä¾‹
            real_time_ratio = 3000 / (processing_time * 1000) if processing_time > 0 else 0
            logger.debug(f"   - å®æ—¶æ¯”ä¾‹: {real_time_ratio:.1f}x")
            
            return result
            
        except Exception as e:
            self.performance_stats['audio_errors'] += 1
            logger.error(f"âŒ [åˆ†æå™¨] éŸ³é¢‘ç‰‡æ®µåˆ†æå¤±è´¥: {e}")
            logger.error(f"ğŸ”§ é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            raise
    
    def get_performance_stats(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯"""
        runtime = time.time() - self.performance_stats['start_time']
        
        return {
            'runtime_seconds': round(runtime, 2),
            'video_analyses': self.performance_stats['video_analysis_count'],
            'audio_analyses': self.performance_stats['audio_analysis_count'],
            'video_errors': self.performance_stats['video_errors'],
            'audio_errors': self.performance_stats['audio_errors'],
            'avg_video_processing_ms': round(self.performance_stats['avg_video_time'] * 1000, 2),
            'avg_audio_processing_ms': round(self.performance_stats['avg_audio_time'] * 1000, 2),
            'video_fps': round(self.performance_stats['video_analysis_count'] / runtime, 2) if runtime > 0 else 0,
            'audio_chunks_per_second': round(self.performance_stats['audio_analysis_count'] / runtime, 2) if runtime > 0 else 0,
            'video_error_rate': round(self.performance_stats['video_errors'] / max(1, self.performance_stats['video_analysis_count']), 3),
            'audio_error_rate': round(self.performance_stats['audio_errors'] / max(1, self.performance_stats['audio_analysis_count']), 3)
        }
    
    def print_performance_summary(self):
        """æ‰“å°æ€§èƒ½æ‘˜è¦"""
        stats = self.get_performance_stats()
        
        logger.info("ğŸ“Š === å®æ—¶å¤šæ¨¡æ€åˆ†ææ€§èƒ½æ‘˜è¦ ===")
        logger.info(f"   ğŸ• è¿è¡Œæ—¶é—´: {stats['runtime_seconds']} ç§’")
        logger.info(f"   ğŸ¥ è§†é¢‘åˆ†æ: {stats['video_analyses']} å¸§ | å¹³å‡: {stats['avg_video_processing_ms']}ms | FPS: {stats['video_fps']} | é”™è¯¯ç‡: {stats['video_error_rate']*100:.1f}%")
        logger.info(f"   ğŸµ éŸ³é¢‘åˆ†æ: {stats['audio_analyses']} ç‰‡æ®µ | å¹³å‡: {stats['avg_audio_processing_ms']}ms | ç‰‡æ®µ/ç§’: {stats['audio_chunks_per_second']} | é”™è¯¯ç‡: {stats['audio_error_rate']*100:.1f}%")
        
        # æ€§èƒ½è¯„ä¼°
        if stats['avg_video_processing_ms'] < 100 and stats['video_error_rate'] < 0.1:
            video_perf = "ä¼˜ç§€"
        elif stats['avg_video_processing_ms'] < 200 and stats['video_error_rate'] < 0.2:
            video_perf = "è‰¯å¥½"  
        else:
            video_perf = "éœ€è¦ä¼˜åŒ–"
            
        if stats['avg_audio_processing_ms'] < 500 and stats['audio_error_rate'] < 0.1:
            audio_perf = "ä¼˜ç§€"
        elif stats['avg_audio_processing_ms'] < 1000 and stats['audio_error_rate'] < 0.2:
            audio_perf = "è‰¯å¥½"
        else:
            audio_perf = "éœ€è¦ä¼˜åŒ–"
        
        logger.info(f"   ğŸ’¯ æ€§èƒ½è¯„ä¼°: è§†é¢‘-{video_perf} | éŸ³é¢‘-{audio_perf}")
        logger.info("="*50)
    
    def reset_stats(self):
        """é‡ç½®æ€§èƒ½ç»Ÿè®¡"""
        self.performance_stats = {
            'video_analysis_count': 0,
            'audio_analysis_count': 0,
            'avg_video_time': 0,
            'avg_audio_time': 0,
            'start_time': time.time(),
            'video_errors': 0,
            'audio_errors': 0
        }
        logger.info("ğŸ“Š æ€§èƒ½ç»Ÿè®¡å·²é‡ç½®")


# åˆ›å»ºå…¨å±€å®ä¾‹
def create_realtime_processor() -> RealtimeMultimodalProcessor:
    """åˆ›å»ºå®æ—¶å¤šæ¨¡æ€å¤„ç†å™¨å®ä¾‹"""
    return RealtimeMultimodalProcessor()


# å¯¼å‡º
__all__ = ['RealtimeMultimodalProcessor', 'create_realtime_processor'] 