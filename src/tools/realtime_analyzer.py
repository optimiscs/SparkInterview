"""
å®æ—¶å¤šæ¨¡æ€åˆ†æå¤„ç†å™¨
ä¸“ä¸ºWebSocketå®æ—¶ä¼ è¾“ä¼˜åŒ–çš„è½»é‡çº§åˆ†ææ¨¡å—
"""
import cv2
import numpy as np
import mediapipe as mp
import librosa
import io
import tempfile
import logging
from typing import Dict, Any, Optional, Tuple
from datetime import datetime
import time

try:
    from deepface import DeepFace
    DEEPFACE_AVAILABLE = True
except ImportError:
    DEEPFACE_AVAILABLE = False
    logging.warning("DeepFace not available, using fallback emotion analysis")

logger = logging.getLogger(__name__)


class RealtimeVideoAnalyzer:
    """å®æ—¶è§†é¢‘åˆ†æå™¨ - ä¼˜åŒ–ç‰ˆ"""
    
    def __init__(self):
        # åˆå§‹åŒ–MediaPipe (è½»é‡çº§é…ç½®)
        self.mp_face_mesh = mp.solutions.face_mesh
        self.face_mesh = self.mp_face_mesh.FaceMesh(
            static_image_mode=False,
            max_num_faces=1,
            refine_landmarks=False,  # å…³é—­ç²¾ç»†åŒ–ä»¥æé«˜é€Ÿåº¦
            min_detection_confidence=0.5,  # é™ä½é˜ˆå€¼ä»¥æé«˜é€Ÿåº¦
            min_tracking_confidence=0.5
        )
        
        # é¢éƒ¨å…³é”®ç‚¹ç´¢å¼•ï¼ˆç®€åŒ–ç‰ˆï¼‰
        self.key_landmarks = {
            'nose_tip': 1,
            'chin': 175,
            'left_eye': 33,
            'right_eye': 263,
            'mouth_center': 13
        }
        
        # æƒ…ç»ªåˆ†æé…ç½®
        self.emotion_cache = {}
        self.emotion_cache_duration = 2.0  # ç¼“å­˜2ç§’
        
        logger.info("âœ… å®æ—¶è§†é¢‘åˆ†æå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def analyze_frame(self, frame: np.ndarray) -> Dict[str, Any]:
        """åˆ†æå•å¸§è§†é¢‘"""
        start_time = time.time()
        
        try:
            if frame is None or frame.size == 0:
                return self._get_default_result()
            
            # è½¬æ¢é¢œè‰²ç©ºé—´
            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            h, w = frame.shape[:2]
            
            # MediaPipeé¢éƒ¨æ£€æµ‹
            results = self.face_mesh.process(rgb_frame)
            
            analysis_result = {
                'timestamp': datetime.now().isoformat(),
                'processing_time': 0,
                'face_detected': False,
                'head_pose_stability': 0.7,
                'gaze_direction': {'x': 0, 'y': 0},
                'dominant_emotion': 'neutral',
                'emotion_confidence': 0.7,
                'emotion_stability': 0.8,
                'eye_contact_ratio': 0.7
            }
            
            if results.multi_face_landmarks:
                face_landmarks = results.multi_face_landmarks[0]
                analysis_result['face_detected'] = True
                
                # å¤´éƒ¨å§¿æ€åˆ†æï¼ˆè½»é‡çº§ï¼‰
                head_pose = self._analyze_head_pose_light(face_landmarks, (w, h))
                if head_pose:
                    analysis_result.update(head_pose)
                
                # è§†çº¿æ–¹å‘åˆ†æï¼ˆç®€åŒ–ç‰ˆï¼‰
                gaze = self._analyze_gaze_light(face_landmarks, (w, h))
                if gaze:
                    analysis_result['gaze_direction'] = gaze
                
                # æƒ…ç»ªåˆ†æï¼ˆå¸¦ç¼“å­˜ï¼‰
                emotion = self._analyze_emotion_cached(frame)
                if emotion:
                    analysis_result.update(emotion)
            
            # è®°å½•å¤„ç†æ—¶é—´
            processing_time = time.time() - start_time
            analysis_result['processing_time'] = round(processing_time * 1000, 2)  # æ¯«ç§’
            
            logger.debug(f"ğŸ¥ è§†é¢‘å¸§åˆ†æå®Œæˆï¼Œè€—æ—¶: {processing_time:.3f}s")
            return analysis_result
            
        except Exception as e:
            logger.error(f"âŒ è§†é¢‘å¸§åˆ†æå¤±è´¥: {e}")
            return self._get_default_result()
    
    def _analyze_head_pose_light(self, landmarks, frame_shape) -> Dict[str, float]:
        """è½»é‡çº§å¤´éƒ¨å§¿æ€åˆ†æ"""
        try:
            w, h = frame_shape
            
            # è·å–å…³é”®ç‚¹
            nose = landmarks.landmark[self.key_landmarks['nose_tip']]
            chin = landmarks.landmark[self.key_landmarks['chin']]
            left_eye = landmarks.landmark[self.key_landmarks['left_eye']]
            right_eye = landmarks.landmark[self.key_landmarks['right_eye']]
            
            # è½¬æ¢ä¸ºåƒç´ åæ ‡
            nose_px = (nose.x * w, nose.y * h)
            chin_px = (chin.x * w, chin.y * h)
            left_eye_px = (left_eye.x * w, left_eye.y * h)
            right_eye_px = (right_eye.x * w, right_eye.y * h)
            
            # è®¡ç®—å¤´éƒ¨å€¾æ–œè§’åº¦ï¼ˆç®€åŒ–è®¡ç®—ï¼‰
            eye_center_x = (left_eye_px[0] + right_eye_px[0]) / 2
            eye_center_y = (left_eye_px[1] + right_eye_px[1]) / 2
            
            # å¤´éƒ¨å‚ç›´åç§»
            face_center_x = (nose_px[0] + chin_px[0]) / 2
            horizontal_deviation = abs(face_center_x - w/2) / (w/2)
            
            # å¤´éƒ¨ç¨³å®šæ€§è¯„åˆ†
            stability = max(0.0, 1.0 - horizontal_deviation)
            
            return {
                'head_pose_stability': round(stability, 3),
                'horizontal_deviation': round(horizontal_deviation, 3)
            }
            
        except Exception as e:
            logger.debug(f"å¤´éƒ¨å§¿æ€åˆ†æå¤±è´¥: {e}")
            return {'head_pose_stability': 0.7}
    
    def _analyze_gaze_light(self, landmarks, frame_shape) -> Dict[str, float]:
        """è½»é‡çº§è§†çº¿æ–¹å‘åˆ†æ"""
        try:
            w, h = frame_shape
            
            # è·å–çœ¼éƒ¨å…³é”®ç‚¹
            left_eye = landmarks.landmark[self.key_landmarks['left_eye']]
            right_eye = landmarks.landmark[self.key_landmarks['right_eye']]
            nose = landmarks.landmark[self.key_landmarks['nose_tip']]
            
            # è®¡ç®—çœ¼éƒ¨ä¸­å¿ƒ
            eye_center_x = (left_eye.x + right_eye.x) / 2
            eye_center_y = (left_eye.y + right_eye.y) / 2
            
            # ä¸é¼»å°–çš„ç›¸å¯¹ä½ç½®
            gaze_x = eye_center_x - nose.x
            gaze_y = eye_center_y - nose.y
            
            return {
                'x': round(gaze_x * 100, 2),  # æ ‡å‡†åŒ–åˆ°-100åˆ°100
                'y': round(gaze_y * 100, 2)
            }
            
        except Exception as e:
            logger.debug(f"è§†çº¿åˆ†æå¤±è´¥: {e}")
            return {'x': 0, 'y': 0}
    
    def _analyze_emotion_cached(self, frame: np.ndarray) -> Optional[Dict[str, Any]]:
        """å¸¦ç¼“å­˜çš„æƒ…ç»ªåˆ†æ"""
        current_time = time.time()
        
        # æ£€æŸ¥ç¼“å­˜
        if 'last_analysis' in self.emotion_cache:
            last_time = self.emotion_cache['last_analysis']
            if current_time - last_time < self.emotion_cache_duration:
                return self.emotion_cache.get('result')
        
        # æ‰§è¡Œæ–°çš„æƒ…ç»ªåˆ†æ
        emotion_result = self._analyze_emotion_fast(frame)
        
        # æ›´æ–°ç¼“å­˜
        self.emotion_cache = {
            'last_analysis': current_time,
            'result': emotion_result
        }
        
        return emotion_result
    
    def _analyze_emotion_fast(self, frame: np.ndarray) -> Dict[str, Any]:
        """å¿«é€Ÿæƒ…ç»ªåˆ†æ"""
        if not DEEPFACE_AVAILABLE:
            return self._get_fallback_emotion()
        
        try:
            # ç¼©å°å›¾ç‰‡ä»¥æé«˜é€Ÿåº¦
            small_frame = cv2.resize(frame, (224, 224))
            
            result = DeepFace.analyze(
                small_frame,
                actions=['emotion'],
                enforce_detection=False,
                detector_backend='opencv'  # ä½¿ç”¨æœ€å¿«çš„æ£€æµ‹å™¨
            )
            
            if isinstance(result, list):
                result = result[0]
            
            emotions = result.get('emotion', {})
            dominant_emotion = max(emotions.items(), key=lambda x: x[1])
            
            return {
                'dominant_emotion': dominant_emotion[0],
                'emotion_confidence': round(dominant_emotion[1] / 100, 3),
                'emotion_distribution': {k: round(v/100, 3) for k, v in emotions.items()}
            }
            
        except Exception as e:
            logger.debug(f"æƒ…ç»ªåˆ†æå¤±è´¥: {e}")
            return self._get_fallback_emotion()
    
    def _get_fallback_emotion(self) -> Dict[str, Any]:
        """å¤‡ç”¨æƒ…ç»ªç»“æœ"""
        return {
            'dominant_emotion': 'neutral',
            'emotion_confidence': 0.7,
            'emotion_distribution': {
                'neutral': 0.7,
                'happy': 0.1,
                'focused': 0.1,
                'surprised': 0.05,
                'sad': 0.05
            }
        }
    
    def _get_default_result(self) -> Dict[str, Any]:
        """é»˜è®¤åˆ†æç»“æœ"""
        return {
            'timestamp': datetime.now().isoformat(),
            'processing_time': 0,
            'face_detected': False,
            'head_pose_stability': 0.7,
            'gaze_direction': {'x': 0, 'y': 0},
            'dominant_emotion': 'neutral',
            'emotion_confidence': 0.7,
            'emotion_stability': 0.8,
            'eye_contact_ratio': 0.7,
            'error': True
        }


class RealtimeAudioAnalyzer:
    """å®æ—¶éŸ³é¢‘åˆ†æå™¨ - ä¼˜åŒ–ç‰ˆ"""
    
    def __init__(self):
        self.sample_rate = 16000  # é™ä½é‡‡æ ·ç‡ä»¥æé«˜é€Ÿåº¦
        self.analysis_cache = {}
        self.cache_duration = 1.0  # ç¼“å­˜1ç§’
        
        logger.info("âœ… å®æ—¶éŸ³é¢‘åˆ†æå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def analyze_chunk(self, audio_bytes: bytes) -> Dict[str, Any]:
        """åˆ†æéŸ³é¢‘ç‰‡æ®µ"""
        start_time = time.time()
        
        try:
            # å°†éŸ³é¢‘å­—èŠ‚è½¬æ¢ä¸ºnumpy array
            audio_data = self._bytes_to_audio(audio_bytes)
            
            if audio_data is None or len(audio_data) == 0:
                return self._get_default_audio_result()
            
            analysis_result = {
                'timestamp': datetime.now().isoformat(),
                'processing_time': 0,
                'audio_detected': True,
                'speech_rate': 120,
                'pitch_mean': 150,
                'pitch_variance': 20,
                'volume_mean': 0.5,
                'clarity_score': 0.8,
                'emotion': 'calm',
                'emotion_confidence': 0.7
            }
            
            # åŸºç¡€éŸ³é¢‘ç‰¹å¾åˆ†æ
            audio_features = self._analyze_audio_features(audio_data)
            analysis_result.update(audio_features)
            
            # è¯­éŸ³æƒ…æ„Ÿåˆ†æï¼ˆç®€åŒ–ç‰ˆï¼‰
            emotion_result = self._analyze_audio_emotion(audio_data)
            analysis_result.update(emotion_result)
            
            # è®°å½•å¤„ç†æ—¶é—´
            processing_time = time.time() - start_time
            analysis_result['processing_time'] = round(processing_time * 1000, 2)
            
            logger.debug(f"ğŸµ éŸ³é¢‘ç‰‡æ®µåˆ†æå®Œæˆï¼Œè€—æ—¶: {processing_time:.3f}s")
            return analysis_result
            
        except Exception as e:
            logger.error(f"âŒ éŸ³é¢‘åˆ†æå¤±è´¥: {e}")
            return self._get_default_audio_result()
    
    def _bytes_to_audio(self, audio_bytes: bytes) -> Optional[np.ndarray]:
        """å°†éŸ³é¢‘å­—èŠ‚è½¬æ¢ä¸ºnumpyæ•°ç»„"""
        try:
            # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
            with tempfile.NamedTemporaryFile(suffix='.webm', delete=False) as temp_file:
                temp_file.write(audio_bytes)
                temp_file.flush()
                
                # ä½¿ç”¨librosaåŠ è½½éŸ³é¢‘
                y, sr = librosa.load(temp_file.name, sr=self.sample_rate)
                return y
                
        except Exception as e:
            logger.debug(f"éŸ³é¢‘è½¬æ¢å¤±è´¥: {e}")
            return None
    
    def _analyze_audio_features(self, audio_data: np.ndarray) -> Dict[str, Any]:
        """åˆ†æåŸºç¡€éŸ³é¢‘ç‰¹å¾"""
        try:
            # è¯­é€Ÿä¼°ç®—ï¼ˆåŸºäºé›¶äº¤å‰ç‡ï¼‰
            zcr = librosa.feature.zero_crossing_rate(audio_data)[0]
            speech_rate = np.mean(zcr) * 1000  # è½¬æ¢ä¸ºBPMä¼°ç®—
            
            # éŸ³é«˜åˆ†æï¼ˆç®€åŒ–ç‰ˆï¼‰
            pitches, magnitudes = librosa.piptrack(y=audio_data, sr=self.sample_rate)
            pitch_values = pitches[magnitudes > np.max(magnitudes) * 0.1]
            
            if len(pitch_values) > 0:
                pitch_mean = np.mean(pitch_values[pitch_values > 0])
                pitch_variance = np.var(pitch_values[pitch_values > 0])
            else:
                pitch_mean = 150
                pitch_variance = 20
            
            # éŸ³é‡åˆ†æ
            rms = librosa.feature.rms(y=audio_data)[0]
            volume_mean = np.mean(rms)
            
            # æ¸…æ™°åº¦è¯„ä¼°ï¼ˆåŸºäºé¢‘è°±é‡å¿ƒï¼‰
            spectral_centroids = librosa.feature.spectral_centroid(y=audio_data, sr=self.sample_rate)[0]
            clarity_score = min(1.0, np.mean(spectral_centroids) / 2000)
            
            return {
                'speech_rate': round(speech_rate, 1),
                'pitch_mean': round(pitch_mean, 1),
                'pitch_variance': round(pitch_variance, 1),
                'volume_mean': round(volume_mean, 3),
                'clarity_score': round(clarity_score, 3)
            }
            
        except Exception as e:
            logger.debug(f"éŸ³é¢‘ç‰¹å¾åˆ†æå¤±è´¥: {e}")
            return {
                'speech_rate': 120,
                'pitch_mean': 150,
                'pitch_variance': 20,
                'volume_mean': 0.5,
                'clarity_score': 0.8
            }
    
    def _analyze_audio_emotion(self, audio_data: np.ndarray) -> Dict[str, Any]:
        """éŸ³é¢‘æƒ…æ„Ÿåˆ†æï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        try:
            # åŸºäºéŸ³é¢‘ç‰¹å¾çš„ç®€å•æƒ…æ„Ÿåˆ¤æ–­
            rms = librosa.feature.rms(y=audio_data)[0]
            volume_level = np.mean(rms)
            
            # åŸºäºéŸ³é‡å’Œé¢‘ç‡å˜åŒ–æ¨æ–­æƒ…æ„Ÿ
            if volume_level > 0.7:
                emotion = 'excited'
                confidence = 0.8
            elif volume_level > 0.4:
                emotion = 'confident'
                confidence = 0.7
            elif volume_level > 0.2:
                emotion = 'calm'
                confidence = 0.8
            else:
                emotion = 'uncertain'
                confidence = 0.6
            
            return {
                'emotion': emotion,
                'emotion_confidence': confidence
            }
            
        except Exception as e:
            logger.debug(f"éŸ³é¢‘æƒ…æ„Ÿåˆ†æå¤±è´¥: {e}")
            return {
                'emotion': 'calm',
                'emotion_confidence': 0.7
            }
    
    def _get_default_audio_result(self) -> Dict[str, Any]:
        """é»˜è®¤éŸ³é¢‘åˆ†æç»“æœ"""
        return {
            'timestamp': datetime.now().isoformat(),
            'processing_time': 0,
            'audio_detected': False,
            'speech_rate': 120,
            'pitch_mean': 150,
            'pitch_variance': 20,
            'volume_mean': 0.5,
            'clarity_score': 0.8,
            'emotion': 'calm',
            'emotion_confidence': 0.7,
            'error': True
        }


class RealtimeMultimodalProcessor:
    """å®æ—¶å¤šæ¨¡æ€å¤„ç†å™¨ - ä¸»æ§åˆ¶å™¨"""
    
    def __init__(self):
        self.video_analyzer = RealtimeVideoAnalyzer()
        self.audio_analyzer = RealtimeAudioAnalyzer()
        
        # æ€§èƒ½ç›‘æ§
        self.performance_stats = {
            'video_analysis_count': 0,
            'audio_analysis_count': 0,
            'avg_video_time': 0,
            'avg_audio_time': 0,
            'start_time': time.time()
        }
        
        logger.info("âœ… å®æ—¶å¤šæ¨¡æ€å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def analyze_video_frame(self, frame: np.ndarray) -> Dict[str, Any]:
        """åˆ†æè§†é¢‘å¸§"""
        start_time = time.time()
        
        result = self.video_analyzer.analyze_frame(frame)
        
        # æ›´æ–°æ€§èƒ½ç»Ÿè®¡
        processing_time = time.time() - start_time
        self.performance_stats['video_analysis_count'] += 1
        count = self.performance_stats['video_analysis_count']
        self.performance_stats['avg_video_time'] = (
            (self.performance_stats['avg_video_time'] * (count - 1) + processing_time) / count
        )
        
        return result
    
    def analyze_audio_chunk(self, audio_bytes: bytes) -> Dict[str, Any]:
        """åˆ†æéŸ³é¢‘ç‰‡æ®µ"""
        start_time = time.time()
        
        result = self.audio_analyzer.analyze_chunk(audio_bytes)
        
        # æ›´æ–°æ€§èƒ½ç»Ÿè®¡
        processing_time = time.time() - start_time
        self.performance_stats['audio_analysis_count'] += 1
        count = self.performance_stats['audio_analysis_count']
        self.performance_stats['avg_audio_time'] = (
            (self.performance_stats['avg_audio_time'] * (count - 1) + processing_time) / count
        )
        
        return result
    
    def get_performance_stats(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯"""
        runtime = time.time() - self.performance_stats['start_time']
        
        return {
            'runtime_seconds': round(runtime, 2),
            'video_analyses': self.performance_stats['video_analysis_count'],
            'audio_analyses': self.performance_stats['audio_analysis_count'],
            'avg_video_processing_ms': round(self.performance_stats['avg_video_time'] * 1000, 2),
            'avg_audio_processing_ms': round(self.performance_stats['avg_audio_time'] * 1000, 2),
            'video_fps': round(self.performance_stats['video_analysis_count'] / runtime, 2) if runtime > 0 else 0,
            'audio_chunks_per_second': round(self.performance_stats['audio_analysis_count'] / runtime, 2) if runtime > 0 else 0
        }
    
    def reset_stats(self):
        """é‡ç½®æ€§èƒ½ç»Ÿè®¡"""
        self.performance_stats = {
            'video_analysis_count': 0,
            'audio_analysis_count': 0,
            'avg_video_time': 0,
            'avg_audio_time': 0,
            'start_time': time.time()
        }
        logger.info("ğŸ“Š æ€§èƒ½ç»Ÿè®¡å·²é‡ç½®")


# åˆ›å»ºå…¨å±€å®ä¾‹
def create_realtime_processor() -> RealtimeMultimodalProcessor:
    """åˆ›å»ºå®æ—¶å¤šæ¨¡æ€å¤„ç†å™¨å®ä¾‹"""
    return RealtimeMultimodalProcessor()


# å¯¼å‡º
__all__ = ['RealtimeMultimodalProcessor', 'create_realtime_processor'] 