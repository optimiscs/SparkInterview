"""
å¤šæ¨¡æ€åˆ†æå·¥å…·
é›†æˆMediaPipeã€DeepFaceã€Librosaç­‰åº“è¿›è¡ŒçœŸå®çš„éŸ³è§†é¢‘åˆ†æ
"""
import cv2
import numpy as np
import librosa
import mediapipe as mp
from typing import Dict, Any, List, Optional, Tuple
import logging
from pathlib import Path
import sys
import traceback
from datetime import datetime
import os
import json

# é…ç½®è¯¦ç»†æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# DeepFaceå¯ç”¨æ€§æ£€æŸ¥å’Œåˆå§‹åŒ–
DEEPFACE_AVAILABLE = False
DEEPFACE_ERROR = None

# DeepFaceå¯¼å…¥
DeepFace = None

def check_deepface_availability():
    """æ£€æŸ¥DeepFaceæ˜¯å¦å¯ç”¨å¹¶è¿›è¡Œåˆå§‹åŒ–æµ‹è¯•"""
    global DEEPFACE_AVAILABLE, DEEPFACE_ERROR, DeepFace
    
    try:
        logger.info("ğŸ” æ£€æŸ¥DeepFaceå¯ç”¨æ€§...")
        from deepface import DeepFace
        
        # å°è¯•åˆå§‹åŒ–DeepFaceè¿›è¡Œç®€å•æµ‹è¯•
        logger.info("ğŸ§ª æµ‹è¯•DeepFaceåŠŸèƒ½...")
        
        # åˆ›å»ºä¸€ä¸ªæµ‹è¯•å›¾åƒ
        test_image = np.zeros((100, 100, 3), dtype=np.uint8)
        test_image[30:70, 30:70] = [255, 255, 255]  # ç™½è‰²æ–¹å—æ¨¡æ‹Ÿäººè„¸
        
        # å°è¯•åˆ†ææµ‹è¯•å›¾åƒ
        result = DeepFace.analyze(
            test_image, 
            actions=['emotion'], 
            enforce_detection=False,
            silent=True
        )
        
        DEEPFACE_AVAILABLE = True
        logger.info("âœ… DeepFaceåˆå§‹åŒ–æˆåŠŸï¼Œæƒ…æ„Ÿåˆ†æåŠŸèƒ½å¯ç”¨")
        return True
        
    except ImportError as e:
        DEEPFACE_ERROR = f"DeepFaceæœªå®‰è£…: {str(e)}"
        logger.error(f"âŒ {DEEPFACE_ERROR}")
        logger.error("ğŸ’¡ è¯·å®‰è£…DeepFace: pip install deepface")
        return False
    except Exception as e:
        DEEPFACE_ERROR = f"DeepFaceåˆå§‹åŒ–å¤±è´¥: {str(e)}"
        logger.error(f"âŒ {DEEPFACE_ERROR}")
        logger.error(f"ğŸ”§ é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
        return False

# å¯åŠ¨æ—¶æ£€æŸ¥DeepFace
check_deepface_availability()

try:
    from ..config.settings import model_config
except ImportError:
    logger.warning("âš ï¸ æ— æ³•å¯¼å…¥model_configï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
    # é»˜è®¤é…ç½®
    class DefaultConfig:
        MEDIAPIPE_CONFIDENCE = 0.5
        DEEPFACE_BACKEND = 'opencv'
        AUDIO_SAMPLE_RATE = 22050
    model_config = DefaultConfig()


class VideoAnalyzer:
    """è§†é¢‘åˆ†æå™¨ - å¤„ç†è§†è§‰æ¨¡æ€"""
    
    def __init__(self):
        # åˆå§‹åŒ–MediaPipe Face Mesh
        self.mp_face_mesh = mp.solutions.face_mesh
        self.mp_drawing = mp.solutions.drawing_utils
        self.face_mesh = self.mp_face_mesh.FaceMesh(
            static_image_mode=False,
            max_num_faces=1,
            refine_landmarks=True,
            min_detection_confidence=model_config.MEDIAPIPE_CONFIDENCE,
            min_tracking_confidence=model_config.MEDIAPIPE_CONFIDENCE
        )
        
        # åˆå§‹åŒ–MediaPipe Pose
        self.mp_pose = mp.solutions.pose
        self.pose = self.mp_pose.Pose(
            static_image_mode=False,
            model_complexity=1,
            enable_segmentation=False,
            min_detection_confidence=model_config.MEDIAPIPE_CONFIDENCE,
            min_tracking_confidence=model_config.MEDIAPIPE_CONFIDENCE
        )
        
        # åˆå§‹åŒ–MediaPipe Hands
        self.mp_hands = mp.solutions.hands
        self.hands = self.mp_hands.Hands(
            static_image_mode=False,
            max_num_hands=2,
            model_complexity=1,
            min_detection_confidence=model_config.MEDIAPIPE_CONFIDENCE,
            min_tracking_confidence=model_config.MEDIAPIPE_CONFIDENCE
        )
        
        # é¢éƒ¨å…³é”®ç‚¹ç´¢å¼• (468ä¸ªå…³é”®ç‚¹ä¸­çš„é‡è¦ç‚¹)
        self.face_landmarks_indexes = {
            'nose_tip': 1,
            'chin': 175,
            'left_eye_corner': 33,
            'right_eye_corner': 263,
            'left_iris': 468,
            'right_iris': 473
        }
        
        # 3Dé¢éƒ¨æ¨¡å‹ç‚¹ (ç”¨äºPnPç®—æ³•)
        self.model_points = np.array([
            (0.0, 0.0, 0.0),      # é¼»å°–
            (0.0, -330.0, -65.0), # ä¸‹å·´
            (-225.0, 170.0, -135.0), # å·¦çœ¼è§’
            (225.0, 170.0, -135.0),  # å³çœ¼è§’
            (-150.0, -150.0, -125.0), # å·¦å˜´è§’
            (150.0, -150.0, -125.0)   # å³å˜´è§’
        ], dtype=np.float64)
        
        # åˆ›å»ºä¿å­˜ç›®å½•
        self.save_dir = Path("data/analysis_frames")
        self.save_dir.mkdir(parents=True, exist_ok=True)
        logger.info(f"ğŸ“ åˆ†æå¸§ä¿å­˜ç›®å½•: {self.save_dir.absolute()}")
    
    def _save_analysis_frame(self, frame: np.ndarray, frame_count: int, timestamp: float, 
                           analysis_result: Dict[str, Any], frame_type: str = "analysis") -> str:
        """ä¿å­˜åˆ†æå¸§åˆ°æœ¬åœ°"""
        
        try:
            # ç”Ÿæˆæ—¶é—´æˆ³
            timestamp_str = datetime.now().strftime("%Y%m%d_%H%M%S_%f")[:-3]
            
            # æ„å»ºæ–‡ä»¶å
            filename = f"{timestamp_str}_frame_{frame_count:06d}_{frame_type}.jpg"
            filepath = self.save_dir / filename
            
            # åœ¨å›¾åƒä¸Šç»˜åˆ¶åˆ†æç»“æœä¿¡æ¯
            annotated_frame = frame.copy()
            
            # æ·»åŠ æ–‡æœ¬ä¿¡æ¯
            font = cv2.FONT_HERSHEY_SIMPLEX
            font_scale = 0.6
            color = (255, 255, 255)  # ç™½è‰²
            thickness = 2
            
            # åŸºæœ¬ä¿¡æ¯
            y_offset = 30
            cv2.putText(annotated_frame, f"Frame: {frame_count}", (10, y_offset), 
                       font, font_scale, color, thickness)
            y_offset += 25
            cv2.putText(annotated_frame, f"Time: {timestamp:.2f}s", (10, y_offset), 
                       font, font_scale, color, thickness)
            y_offset += 25
            
            # åˆ†æç»“æœä¿¡æ¯
            if 'head_pose' in analysis_result:
                pose = analysis_result['head_pose']
                cv2.putText(annotated_frame, f"Pitch: {pose.get('pitch', 0):.1f}Â°", (10, y_offset), 
                           font, font_scale, color, thickness)
                y_offset += 25
                cv2.putText(annotated_frame, f"Yaw: {pose.get('yaw', 0):.1f}Â°", (10, y_offset), 
                           font, font_scale, color, thickness)
                y_offset += 25
                cv2.putText(annotated_frame, f"Roll: {pose.get('roll', 0):.1f}Â°", (10, y_offset), 
                           font, font_scale, color, thickness)
                y_offset += 25
            
            if 'gaze' in analysis_result:
                gaze = analysis_result['gaze']
                cv2.putText(annotated_frame, f"Gaze: ({gaze.get('gaze_x', 0):.1f}, {gaze.get('gaze_y', 0):.1f})", 
                           (10, y_offset), font, font_scale, color, thickness)
                y_offset += 25
            
            if 'emotion' in analysis_result:
                emotion = analysis_result['emotion']
                cv2.putText(annotated_frame, f"Emotion: {emotion.get('dominant_emotion', 'Unknown')}", 
                           (10, y_offset), font, font_scale, color, thickness)
                y_offset += 25
                cv2.putText(annotated_frame, f"Score: {emotion.get('dominant_score', 0):.1f}%", 
                           (10, y_offset), font, font_scale, color, thickness)
            
            # ä¿å­˜å›¾åƒ
            cv2.imwrite(str(filepath), annotated_frame)
            
            # ä¿å­˜åˆ†æç»“æœJSON
            json_filename = f"{timestamp_str}_frame_{frame_count:06d}_{frame_type}_analysis.json"
            json_filepath = self.save_dir / json_filename
            with open(json_filepath, 'w', encoding='utf-8') as f:
                json.dump({
                    'frame_count': frame_count,
                    'timestamp': timestamp,
                    'frame_type': frame_type,
                    'analysis_result': analysis_result,
                    'saved_at': datetime.now().isoformat()
                }, f, ensure_ascii=False, indent=2)
            
            logger.debug(f"ğŸ’¾ ä¿å­˜åˆ†æå¸§: {filename}")
            return str(filepath)
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜åˆ†æå¸§å¤±è´¥: {str(e)}")
            return ""
    
    def analyze_video(self, video_path: str) -> Dict[str, Any]:
        """åˆ†æè§†é¢‘æ–‡ä»¶ï¼Œæå–è§†è§‰ç‰¹å¾"""
        
        start_time = datetime.now()
        logger.info(f"ğŸ¥ å¼€å§‹è§†é¢‘åˆ†æ: {video_path}")
        
        # æ£€æŸ¥è§†é¢‘æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not Path(video_path).exists():
            error_msg = f"è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_path}"
            logger.error(f"âŒ {error_msg}")
            raise FileNotFoundError(error_msg)
        
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            error_msg = f"æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶: {video_path}"
            logger.error(f"âŒ {error_msg}")
            raise Exception(error_msg)
        
        try:
            # è·å–è§†é¢‘åŸºæœ¬ä¿¡æ¯
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            fps = cap.get(cv2.CAP_PROP_FPS)
            duration = total_frames / fps if fps > 0 else 0
            
            logger.info(f"ğŸ“Š è§†é¢‘ä¿¡æ¯: {total_frames}å¸§, {fps:.1f}FPS, {duration:.1f}ç§’")
            
            # åˆå§‹åŒ–åˆ†æç»“æœå­˜å‚¨
            head_poses = []
            gaze_directions = []
            body_language_results = []
            gesture_results = []
            emotions_timeline = []
            frame_count = 0
            processed_frames = 0
            saved_frames = []
            
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                
                frame_count += 1
                timestamp = frame_count / fps if fps > 0 else 0
                
                try:
                    # è½¬æ¢é¢œè‰²ç©ºé—´
                    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                    
                    # MediaPipeé¢éƒ¨æ£€æµ‹
                    results = self.face_mesh.process(rgb_frame)
                    
                    if results.multi_face_landmarks:
                        processed_frames += 1
                        face_landmarks = results.multi_face_landmarks[0]
                        
                        # åˆ†æå¤´éƒ¨å§¿æ€
                        head_pose = self._analyze_head_pose(face_landmarks, frame.shape)
                        if head_pose:
                            head_poses.append(head_pose)
                        
                        # åˆ†æè§†çº¿æ–¹å‘
                        gaze = self._analyze_gaze_direction(face_landmarks, frame.shape)
                        if gaze:
                            gaze_directions.append(gaze)
                        
                        # æƒ…ç»ªåˆ†æ (æ¯10å¸§åˆ†æä¸€æ¬¡ä»¥æé«˜æ•ˆç‡)
                        emotion = None
                        if frame_count % 10 == 0:
                            emotion = self._analyze_emotion(frame)
                            if emotion:
                                emotions_timeline.append({
                                    'frame': frame_count,
                                    'timestamp': timestamp,
                                    **emotion
                                })
                        
                        # ä½“æ€è¯­è¨€åˆ†æ
                        body_language = self._analyze_body_language(frame)
                        if body_language:
                            body_language_results.append({
                                'frame': frame_count,
                                'timestamp': timestamp,
                                **body_language
                            })
                        
                        # æ‰‹åŠ¿åˆ†æ
                        gestures = self._analyze_gestures(frame)
                        if gestures:
                            gesture_results.append({
                                'frame': frame_count,
                                'timestamp': timestamp,
                                **gestures
                            })
                        
                        # ä¿å­˜å…³é”®åˆ†æå¸§ (æ¯30å¸§ä¿å­˜ä¸€æ¬¡ï¼Œæˆ–è€…æœ‰é‡è¦åˆ†æç»“æœæ—¶)
                        should_save = (frame_count % 30 == 0 or 
                                     (emotion and emotion.get('dominant_score', 0) > 70) or
                                     (head_pose and abs(head_pose.get('yaw', 0)) > 15))
                        
                        if should_save:
                            analysis_result = {
                                'head_pose': head_pose,
                                'gaze': gaze,
                                'emotion': emotion,
                                'body_language': body_language,
                                'gestures': gestures
                            }
                            
                            saved_path = self._save_analysis_frame(
                                frame, frame_count, timestamp, analysis_result, "key"
                            )
                            if saved_path:
                                saved_frames.append({
                                    'frame_count': frame_count,
                                    'timestamp': timestamp,
                                    'filepath': saved_path,
                                    'analysis_result': analysis_result
                                })
                    
                    # è¿›åº¦æ—¥å¿— (æ¯100å¸§æŠ¥å‘Šä¸€æ¬¡)
                    if frame_count % 100 == 0:
                        progress = (frame_count / total_frames) * 100 if total_frames > 0 else 0
                        logger.debug(f"ğŸ“ˆ è§†é¢‘åˆ†æè¿›åº¦: {progress:.1f}% ({frame_count}/{total_frames})")
                        
                except Exception as frame_error:
                    logger.error(f"âŒ ç¬¬{frame_count}å¸§åˆ†æå¤±è´¥: {str(frame_error)}")
                    logger.error(f"ğŸ”§ é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
                    # ä¸å†ä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆï¼Œç»§ç»­å¤„ç†ä¸‹ä¸€å¸§
                    continue
            
            cap.release()
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆçš„åˆ†æç»“æœ
            if processed_frames == 0:
                error_msg = "è§†é¢‘ä¸­æœªæ£€æµ‹åˆ°ä»»ä½•æœ‰æ•ˆçš„é¢éƒ¨æ•°æ®"
                logger.error(f"âŒ {error_msg}")
                raise Exception(error_msg)
            
            # åˆ†æç»“æœç»Ÿè®¡
            processing_time = (datetime.now() - start_time).total_seconds()
            logger.info(f"ğŸ“Š è§†é¢‘åˆ†æå®Œæˆ:")
            logger.info(f"   - æ€»å¸§æ•°: {frame_count}")
            logger.info(f"   - æ£€æµ‹åˆ°äººè„¸çš„å¸§æ•°: {processed_frames}")
            logger.info(f"   - å¤´éƒ¨å§¿æ€åˆ†æ: {len(head_poses)}ä¸ªæœ‰æ•ˆç»“æœ")
            logger.info(f"   - è§†çº¿æ–¹å‘åˆ†æ: {len(gaze_directions)}ä¸ªæœ‰æ•ˆç»“æœ") 
            logger.info(f"   - æƒ…ç»ªåˆ†æ: {len(emotions_timeline)}ä¸ªæœ‰æ•ˆç»“æœ")
            logger.info(f"   - ä½“æ€è¯­è¨€åˆ†æ: {len(body_language_results)}ä¸ªæœ‰æ•ˆç»“æœ")
            logger.info(f"   - æ‰‹åŠ¿åˆ†æ: {len(gesture_results)}ä¸ªæœ‰æ•ˆç»“æœ")
            logger.info(f"   - ä¿å­˜çš„å…³é”®å¸§: {len(saved_frames)}ä¸ª")
            logger.info(f"   - å¤„ç†è€—æ—¶: {processing_time:.2f}ç§’")
            
            # è®¡ç®—ç»Ÿè®¡ç‰¹å¾
            analysis_result = self._compute_visual_statistics(
                head_poses, gaze_directions, emotions_timeline, body_language_results, gesture_results
            )
            
            # æ·»åŠ å¤„ç†ç»Ÿè®¡ä¿¡æ¯
            analysis_result.update({
                'processing_stats': {
                    'total_frames': frame_count,
                    'processed_frames': processed_frames,
                    'processing_time_seconds': processing_time,
                    'head_pose_count': len(head_poses),
                    'gaze_direction_count': len(gaze_directions),
                    'emotion_analysis_count': len(emotions_timeline),
                    'body_language_count': len(body_language_results),
                    'gesture_count': len(gesture_results),
                    'saved_frames_count': len(saved_frames),
                    'save_directory': str(self.save_dir.absolute())
                },
                'saved_frames': saved_frames
            })
            
            logger.info("âœ… è§†é¢‘åˆ†ææˆåŠŸå®Œæˆ")
            return analysis_result
            
        except Exception as e:
            cap.release()
            processing_time = (datetime.now() - start_time).total_seconds()
            logger.error(f"âŒ è§†é¢‘åˆ†æå¤±è´¥: {str(e)}")
            logger.error(f"ğŸ”§ é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            logger.error(f"â±ï¸ å¤±è´¥å‰å¤„ç†æ—¶é—´: {processing_time:.2f}ç§’")
            raise
    
    def _analyze_head_pose(self, face_landmarks, frame_shape) -> Optional[Dict[str, float]]:
        """åˆ†æå¤´éƒ¨å§¿æ€"""
        
        try:
            h, w = frame_shape[:2]
            
            # æå–2Då…³é”®ç‚¹
            landmark_points = []
            for idx in [1, 175, 33, 263, 61, 291]:  # é¼»å°–ã€ä¸‹å·´ã€çœ¼è§’ã€å˜´è§’
                if idx < len(face_landmarks.landmark):
                    lm = face_landmarks.landmark[idx]
                    landmark_points.append([lm.x * w, lm.y * h])
            
            if len(landmark_points) < 6:
                logger.error("âŒ å¤´éƒ¨å§¿æ€åˆ†æ: å…³é”®ç‚¹æ•°é‡ä¸è¶³")
                raise Exception("å¤´éƒ¨å§¿æ€åˆ†æå¤±è´¥ï¼šå…³é”®ç‚¹æ•°é‡ä¸è¶³")
            
            image_points = np.array(landmark_points, dtype=np.float64)
            
            # ç›¸æœºå‚æ•° (ä¼°ç®—)
            focal_length = w
            center = (w/2, h/2)
            camera_matrix = np.array([
                [focal_length, 0, center[0]],
                [0, focal_length, center[1]],
                [0, 0, 1]
            ], dtype=np.float64)
            
            dist_coeffs = np.zeros((4,1))
            
            # ä½¿ç”¨PnPç®—æ³•æ±‚è§£å¤´éƒ¨å§¿æ€
            success, rotation_vector, translation_vector = cv2.solvePnP(
                self.model_points, image_points, camera_matrix, dist_coeffs
            )
            
            if not success:
                logger.error("âŒ å¤´éƒ¨å§¿æ€åˆ†æ: PnPç®—æ³•æ±‚è§£å¤±è´¥")
                raise Exception("å¤´éƒ¨å§¿æ€åˆ†æå¤±è´¥ï¼šPnPç®—æ³•æ±‚è§£å¤±è´¥")
            
            # è½¬æ¢ä¸ºæ¬§æ‹‰è§’
            rotation_matrix, _ = cv2.Rodrigues(rotation_vector)
            angles = self._rotation_matrix_to_euler(rotation_matrix)
            
            return {
                'pitch': float(angles[0]),  # ä¿¯ä»°è§’
                'yaw': float(angles[1]),    # å¶èˆªè§’
                'roll': float(angles[2])    # ç¿»æ»šè§’
            }
            
        except Exception as e:
            logger.error(f"âŒ å¤´éƒ¨å§¿æ€åˆ†æå¤±è´¥: {str(e)}")
            logger.error(f"ğŸ”§ é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            raise
    
    def _analyze_gaze_direction(self, face_landmarks, frame_shape) -> Optional[Dict[str, float]]:
        """åˆ†æè§†çº¿æ–¹å‘"""
        
        try:
            h, w = frame_shape[:2]
            
            # è·å–è™¹è†œå’Œçœ¼è§’å…³é”®ç‚¹
            left_iris_idx = 468
            right_iris_idx = 473
            left_eye_corner_idx = 33
            right_eye_corner_idx = 263
            
            if (left_iris_idx >= len(face_landmarks.landmark) or 
                right_iris_idx >= len(face_landmarks.landmark)):
                logger.error("âŒ è§†çº¿åˆ†æå¤±è´¥ï¼šè™¹è†œå…³é”®ç‚¹ç´¢å¼•è¶…å‡ºèŒƒå›´")
                raise Exception("è§†çº¿åˆ†æå¤±è´¥ï¼šè™¹è†œå…³é”®ç‚¹ç´¢å¼•è¶…å‡ºèŒƒå›´")
            
            # è®¡ç®—è§†çº¿å‘é‡
            left_iris = face_landmarks.landmark[left_iris_idx]
            right_iris = face_landmarks.landmark[right_iris_idx]
            left_corner = face_landmarks.landmark[left_eye_corner_idx]
            right_corner = face_landmarks.landmark[right_eye_corner_idx]
            
            # è®¡ç®—è§†çº¿åç§»
            left_gaze_x = (left_iris.x - left_corner.x) * w
            left_gaze_y = (left_iris.y - left_corner.y) * h
            right_gaze_x = (right_iris.x - right_corner.x) * w
            right_gaze_y = (right_iris.y - right_corner.y) * h
            
            # å¹³å‡è§†çº¿æ–¹å‘
            avg_gaze_x = (left_gaze_x + right_gaze_x) / 2
            avg_gaze_y = (left_gaze_y + right_gaze_y) / 2
            
            return {
                'gaze_x': float(avg_gaze_x),
                'gaze_y': float(avg_gaze_y),
                'gaze_magnitude': float(np.sqrt(avg_gaze_x**2 + avg_gaze_y**2))
            }
            
        except Exception as e:
            logger.error(f"âŒ è§†çº¿åˆ†æå¤±è´¥: {str(e)}")
            logger.error(f"ğŸ”§ é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            raise
    
    def _analyze_emotion(self, frame) -> Optional[Dict[str, float]]:
        """åˆ†ææƒ…ç»ª"""
        
        if not DEEPFACE_AVAILABLE:
            error_msg = f"âŒ DeepFaceä¸å¯ç”¨ï¼Œæ— æ³•è¿›è¡Œæƒ…ç»ªåˆ†æ: {DEEPFACE_ERROR}"
            logger.error(f"âŒ {error_msg}")
            raise Exception(error_msg)
        
        try:
            logger.debug("ğŸ˜Š å¼€å§‹DeepFaceæƒ…ç»ªåˆ†æ...")
            
            # æ£€æŸ¥å¸§çš„æœ‰æ•ˆæ€§
            if frame is None or frame.size == 0:
                logger.error("âŒ æƒ…ç»ªåˆ†æå¤±è´¥: è¾“å…¥å¸§ä¸ºç©º")
                raise Exception("æƒ…ç»ªåˆ†æå¤±è´¥ï¼šè¾“å…¥å¸§ä¸ºç©º")
            
            # ä½¿ç”¨DeepFaceåˆ†ææƒ…ç»ª
            result = DeepFace.analyze(
                frame, 
                actions=['emotion'], 
                enforce_detection=False,
                detector_backend=model_config.DEEPFACE_BACKEND,
                silent=True
            )
            
            if isinstance(result, list):
                result = result[0]
            
            emotions = result.get('emotion', {})
            
            if not emotions:
                logger.error("âŒ DeepFaceè¿”å›ç©ºçš„æƒ…ç»ªç»“æœ")
                raise Exception("DeepFaceè¿”å›ç©ºçš„æƒ…ç»ªç»“æœ")
            
            # æ‰¾å‡ºä¸»å¯¼æƒ…ç»ª
            dominant_emotion = max(emotions.items(), key=lambda x: x[1])
            
            logger.debug(f"âœ… æƒ…ç»ªåˆ†ææˆåŠŸ: {dominant_emotion[0]} ({dominant_emotion[1]:.2f}%)")
            
            return {
                'dominant_emotion': dominant_emotion[0],
                'dominant_score': float(dominant_emotion[1]),
                'emotions': {k: float(v) for k, v in emotions.items()}
            }
            
        except Exception as e:
            logger.error(f"âŒ DeepFaceæƒ…ç»ªåˆ†æå¤±è´¥: {str(e)}")
            logger.error(f"ğŸ”§ é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            raise
    
    def _rotation_matrix_to_euler(self, rotation_matrix) -> Tuple[float, float, float]:
        """æ—‹è½¬çŸ©é˜µè½¬æ¬§æ‹‰è§’"""
        
        sy = np.sqrt(rotation_matrix[0,0] * rotation_matrix[0,0] + 
                    rotation_matrix[1,0] * rotation_matrix[1,0])
        
        singular = sy < 1e-6
        
        if not singular:
            x = np.arctan2(rotation_matrix[2,1], rotation_matrix[2,2])
            y = np.arctan2(-rotation_matrix[2,0], sy)
            z = np.arctan2(rotation_matrix[1,0], rotation_matrix[0,0])
        else:
            x = np.arctan2(-rotation_matrix[1,2], rotation_matrix[1,1])
            y = np.arctan2(-rotation_matrix[2,0], sy)
            z = 0
        
        return np.degrees([x, y, z])
    
    def _analyze_body_language(self, frame: np.ndarray) -> Optional[Dict[str, Any]]:
        """åˆ†æä½“æ€è¯­è¨€ - ä½¿ç”¨MediaPipe Pose"""
        try:
            # è½¬æ¢ä¸ºRGBæ ¼å¼
            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            
            # è¿›è¡Œå§¿æ€æ£€æµ‹
            results = self.pose.process(rgb_frame)
            
            if not results.pose_landmarks:
                logger.debug("ğŸ¤· æœªæ£€æµ‹åˆ°èº«ä½“å§¿æ€")
                return None
            
            landmarks = results.pose_landmarks.landmark
            
            # å…³é”®ç‚¹ç´¢å¼•
            LEFT_SHOULDER = self.mp_pose.PoseLandmark.LEFT_SHOULDER.value
            RIGHT_SHOULDER = self.mp_pose.PoseLandmark.RIGHT_SHOULDER.value
            LEFT_HIP = self.mp_pose.PoseLandmark.LEFT_HIP.value
            RIGHT_HIP = self.mp_pose.PoseLandmark.RIGHT_HIP.value
            NOSE = self.mp_pose.PoseLandmark.NOSE.value
            
            # è®¡ç®—è‚©è†€æ°´å¹³åº¦
            left_shoulder = landmarks[LEFT_SHOULDER]
            right_shoulder = landmarks[RIGHT_SHOULDER]
            shoulder_slope = abs(left_shoulder.y - right_shoulder.y)
            
            # è®¡ç®—èº«ä½“å€¾æ–œåº¦ (åŸºäºè‚©è†€å’Œè‡€éƒ¨çš„åç§»)
            left_hip = landmarks[LEFT_HIP]
            right_hip = landmarks[RIGHT_HIP]
            
            shoulder_center_x = (left_shoulder.x + right_shoulder.x) / 2
            hip_center_x = (left_hip.x + right_hip.x) / 2
            body_angle = abs(shoulder_center_x - hip_center_x) * 180  # è½¬æ¢ä¸ºè§’åº¦
            
            # è®¡ç®—å§¿æ€åˆ†æ•° (0-100)
            posture_score = max(0, 100 - (shoulder_slope * 500) - (body_angle * 2))
            
            # è®¡ç®—èº«ä½“ç¨³å®šæ€§
            nose_position = landmarks[NOSE]
            center_offset = abs(nose_position.x - 0.5)  # ç›¸å¯¹äºç”»é¢ä¸­å¿ƒçš„åç§»
            
            # åˆ†æåå§¿/ç«™å§¿çŠ¶æ€
            shoulder_hip_distance = abs(left_shoulder.y - left_hip.y)
            posture_type = "sitting" if shoulder_hip_distance < 0.3 else "standing"
            
            # æ£€æµ‹ç´§å¼ è¿¹è±¡
            tension_indicators = 0
            if shoulder_slope > 0.05:  # è‚©è†€ä¸å¹³
                tension_indicators += 1
            if body_angle > 10:  # èº«ä½“è¿‡åº¦å€¾æ–œ
                tension_indicators += 1
            if center_offset > 0.15:  # åç¦»ä¸­å¿ƒè¿‡å¤š
                tension_indicators += 1
                
            tension_level = min(100, tension_indicators * 30)
            
            logger.debug(f"ğŸ§ ä½“æ€åˆ†æ: å§¿æ€åˆ†æ•°={posture_score:.1f}, èº«ä½“è§’åº¦={body_angle:.1f}Â°, ç´§å¼ åº¦={tension_level}")
            
            return {
                'posture_score': float(posture_score),
                'body_angle': float(body_angle),
                'shoulder_slope': float(shoulder_slope),
                'tension_level': float(tension_level),
                'posture_type': posture_type,
                'center_offset': float(center_offset),
                'body_stability': float(1.0 - center_offset)
            }
            
        except Exception as e:
            logger.error(f"âŒ ä½“æ€è¯­è¨€åˆ†æå¤±è´¥: {str(e)}")
            return None
    
    def _analyze_gestures(self, frame: np.ndarray) -> Optional[Dict[str, Any]]:
        """åˆ†ææ‰‹åŠ¿ - ä½¿ç”¨MediaPipe Hands"""
        try:
            # è½¬æ¢ä¸ºRGBæ ¼å¼
            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            
            # è¿›è¡Œæ‰‹éƒ¨æ£€æµ‹
            results = self.hands.process(rgb_frame)
            
            if not results.multi_hand_landmarks:
                logger.debug("ğŸ‘‹ æœªæ£€æµ‹åˆ°æ‰‹éƒ¨")
                return {
                    'hands_detected': 0,
                    'gesture_activity': 0.0,
                    'dominant_gesture': 'none',
                    'hand_positions': []
                }
            
            hands_data = []
            total_movement = 0.0
            
            for idx, hand_landmarks in enumerate(results.multi_hand_landmarks):
                landmarks = hand_landmarks.landmark
                
                # å…³é”®ç‚¹ç´¢å¼•
                WRIST = 0
                THUMB_TIP = 4
                INDEX_TIP = 8
                MIDDLE_TIP = 12
                RING_TIP = 16
                PINKY_TIP = 20
                
                # è®¡ç®—æ‰‹çš„ä½ç½® (åŸºäºæ‰‹è…•)
                wrist = landmarks[WRIST]
                hand_position = (wrist.x, wrist.y)
                
                # è®¡ç®—æ‰‹æŒ‡çš„å±•å¼€ç¨‹åº¦
                finger_tips = [
                    landmarks[THUMB_TIP],
                    landmarks[INDEX_TIP], 
                    landmarks[MIDDLE_TIP],
                    landmarks[RING_TIP],
                    landmarks[PINKY_TIP]
                ]
                
                # è®¡ç®—æ‰‹æŒ‡é—´çš„å¹³å‡è·ç¦» (è¡¨ç¤ºæ‰‹çš„å±•å¼€ç¨‹åº¦)
                finger_spread = 0
                for i in range(len(finger_tips)):
                    for j in range(i + 1, len(finger_tips)):
                        tip1, tip2 = finger_tips[i], finger_tips[j]
                        distance = np.sqrt((tip1.x - tip2.x)**2 + (tip1.y - tip2.y)**2)
                        finger_spread += distance
                
                finger_spread /= 10  # æ ‡å‡†åŒ–
                
                # è®¡ç®—æ‰‹çš„ç§»åŠ¨é‡ (åŸºäºç›¸å¯¹ä½ç½®å˜åŒ–)
                movement = abs(wrist.x - 0.5) + abs(wrist.y - 0.5)
                total_movement += movement
                
                # ç®€å•çš„æ‰‹åŠ¿è¯†åˆ«
                gesture_type = self._classify_gesture(landmarks)
                
                hands_data.append({
                    'hand_index': idx,
                    'position': hand_position,
                    'finger_spread': float(finger_spread),
                    'movement': float(movement),
                    'gesture_type': gesture_type
                })
            
            # è®¡ç®—æ•´ä½“æ‰‹åŠ¿æ´»è·ƒåº¦
            gesture_activity = min(100, total_movement * 200)
            
            # ç¡®å®šä¸»å¯¼æ‰‹åŠ¿
            gesture_types = [hand['gesture_type'] for hand in hands_data]
            dominant_gesture = max(set(gesture_types), key=gesture_types.count) if gesture_types else 'none'
            
            logger.debug(f"ğŸ‘‹ æ‰‹åŠ¿åˆ†æ: æ£€æµ‹åˆ°{len(hands_data)}åªæ‰‹, æ´»è·ƒåº¦={gesture_activity:.1f}, ä¸»å¯¼æ‰‹åŠ¿={dominant_gesture}")
            
            return {
                'hands_detected': len(hands_data),
                'gesture_activity': float(gesture_activity),
                'dominant_gesture': dominant_gesture,
                'hand_positions': hands_data,
                'total_movement': float(total_movement)
            }
            
        except Exception as e:
            logger.error(f"âŒ æ‰‹åŠ¿åˆ†æå¤±è´¥: {str(e)}")
            return None
    
    def _classify_gesture(self, landmarks) -> str:
        """ç®€å•çš„æ‰‹åŠ¿åˆ†ç±»"""
        try:
            # å…³é”®ç‚¹
            wrist = landmarks[0]
            thumb_tip = landmarks[4]
            index_tip = landmarks[8]
            middle_tip = landmarks[12]
            ring_tip = landmarks[16]
            pinky_tip = landmarks[20]
            
            # è®¡ç®—æ‰‹æŒ‡ç›¸å¯¹äºæ‰‹è…•çš„ä½ç½®
            fingers_up = []
            
            # æ‹‡æŒ‡ (ç‰¹æ®Šå¤„ç†ï¼Œå› ä¸ºæ–¹å‘ä¸åŒ)
            if thumb_tip.x > wrist.x:  # å‡è®¾å³æ‰‹
                fingers_up.append(thumb_tip.x > landmarks[3].x)
            else:  # å·¦æ‰‹
                fingers_up.append(thumb_tip.x < landmarks[3].x)
            
            # å…¶ä»–å››æŒ‡
            finger_tips = [index_tip, middle_tip, ring_tip, pinky_tip]
            finger_pips = [landmarks[6], landmarks[10], landmarks[14], landmarks[18]]
            
            for tip, pip in zip(finger_tips, finger_pips):
                fingers_up.append(tip.y < pip.y)
            
            # ç®€å•æ‰‹åŠ¿è¯†åˆ«
            fingers_count = sum(fingers_up)
            
            if fingers_count == 0:
                return "fist"
            elif fingers_count == 1:
                if fingers_up[1]:  # åªæœ‰é£ŸæŒ‡
                    return "pointing"
                elif fingers_up[0]:  # åªæœ‰æ‹‡æŒ‡
                    return "thumbs_up"
                else:
                    return "one_finger"
            elif fingers_count == 2:
                if fingers_up[1] and fingers_up[2]:  # é£ŸæŒ‡å’Œä¸­æŒ‡
                    return "peace"
                else:
                    return "two_fingers"
            elif fingers_count == 5:
                return "open_hand"
            else:
                return "partial_open"
                
        except Exception as e:
            logger.debug(f"âš ï¸ æ‰‹åŠ¿åˆ†ç±»å¤±è´¥: {str(e)}")
            return "unknown"
    
    def _compute_visual_statistics(
        self, 
        head_poses: List[Dict], 
        gaze_directions: List[Dict], 
        emotions_timeline: List[Dict],
        body_language_results: List[Dict] = None,
        gesture_results: List[Dict] = None
    ) -> Dict[str, Any]:
        """è®¡ç®—è§†è§‰åˆ†æç»Ÿè®¡ç‰¹å¾"""
        
        if not head_poses and not gaze_directions and not emotions_timeline:
            error_msg = "æ‰€æœ‰è§†è§‰åˆ†æç»“æœå‡ä¸ºç©ºï¼Œæ— æ³•è®¡ç®—ç»Ÿè®¡ç‰¹å¾"
            logger.error(f"âŒ {error_msg}")
            raise Exception(error_msg)
        
        result = {}
        
        if head_poses:
            # è®¡ç®—å¤´éƒ¨å§¿æ€ç¨³å®šæ€§
            yaw_values = [pose['yaw'] for pose in head_poses]
            pitch_values = [pose['pitch'] for pose in head_poses]
            
            yaw_variance = np.var(yaw_values)
            pitch_variance = np.var(pitch_values)
            
            # ç¨³å®šæ€§ä¸æ–¹å·®æˆåæ¯”
            head_stability = 1.0 / (1.0 + (yaw_variance + pitch_variance) / 100)
            result['head_pose_stability'] = float(head_stability)
        else:
            logger.warning("âš ï¸ å¤´éƒ¨å§¿æ€æ•°æ®ä¸ºç©º")
        
        if gaze_directions:
            # è®¡ç®—è§†çº¿ç¨³å®šæ€§
            gaze_magnitudes = [gaze['gaze_magnitude'] for gaze in gaze_directions]
            gaze_variance = np.var(gaze_magnitudes)
            
            gaze_stability = 1.0 / (1.0 + gaze_variance / 10)
            result['gaze_stability'] = float(gaze_stability)
            
            # è®¡ç®—çœ¼ç¥æ¥è§¦æ¯”ä¾‹ (è§†çº¿åç§»å°äºé˜ˆå€¼çš„å¸§æ•°æ¯”ä¾‹)
            eye_contact_frames = sum(1 for mag in gaze_magnitudes if mag < 5.0)
            result['eye_contact_ratio'] = eye_contact_frames / len(gaze_magnitudes)
        else:
            logger.warning("âš ï¸ è§†çº¿æ–¹å‘æ•°æ®ä¸ºç©º")
        
        if emotions_timeline:
            # åˆ†ææƒ…ç»ªåˆ†å¸ƒ
            emotion_counts = {}
            for emotion_data in emotions_timeline:
                emotion = emotion_data['dominant_emotion']
                emotion_counts[emotion] = emotion_counts.get(emotion, 0) + 1
            
            # ä¸»å¯¼æƒ…ç»ª
            if emotion_counts:
                result['dominant_emotion'] = max(emotion_counts.items(), key=lambda x: x[1])[0]
            
            # æƒ…ç»ªç¨³å®šæ€§ (è´Ÿé¢æƒ…ç»ªå æ¯”çš„å€’æ•°)
            negative_emotions = ['angry', 'disgust', 'fear', 'sad']
            negative_count = sum(emotion_counts.get(emotion, 0) for emotion in negative_emotions)
            total_count = sum(emotion_counts.values())
            
            if total_count > 0:
                negative_ratio = negative_count / total_count
                result['emotion_stability'] = 1.0 - negative_ratio
        else:
            logger.warning("âš ï¸ æƒ…ç»ªåˆ†ææ•°æ®ä¸ºç©º")
        
        # ä½“æ€è¯­è¨€åˆ†æç»Ÿè®¡
        if body_language_results:
            posture_scores = [bl.get('posture_score', 0) for bl in body_language_results if 'posture_score' in bl]
            if posture_scores:
                result['avg_posture_score'] = float(np.mean(posture_scores))
                result['posture_stability'] = float(1.0 - (np.std(posture_scores) / 100.0))
            
            # èº«ä½“å€¾æ–œåº¦ç»Ÿè®¡
            body_angles = [bl.get('body_angle', 0) for bl in body_language_results if 'body_angle' in bl]
            if body_angles:
                result['avg_body_angle'] = float(np.mean(body_angles))
                result['body_angle_variance'] = float(np.var(body_angles))
        else:
            logger.warning("âš ï¸ ä½“æ€è¯­è¨€åˆ†ææ•°æ®ä¸ºç©º")
        
        # æ‰‹åŠ¿åˆ†æç»Ÿè®¡
        if gesture_results:
            gesture_activity = [gr.get('gesture_activity', 0) for gr in gesture_results if 'gesture_activity' in gr]
            if gesture_activity:
                result['avg_gesture_activity'] = float(np.mean(gesture_activity))
                result['gesture_expressiveness'] = float(np.max(gesture_activity))
            
            # æ‰‹åŠ¿ç±»å‹åˆ†å¸ƒ
            gesture_types = {}
            for gr in gesture_results:
                gesture_type = gr.get('dominant_gesture', 'unknown')
                gesture_types[gesture_type] = gesture_types.get(gesture_type, 0) + 1
            
            if gesture_types:
                result['dominant_gesture_type'] = max(gesture_types.items(), key=lambda x: x[1])[0]
                result['gesture_variety'] = len(gesture_types)
        else:
            logger.warning("âš ï¸ æ‰‹åŠ¿åˆ†ææ•°æ®ä¸ºç©º")
        
        return result


class AudioAnalyzer:
    """éŸ³é¢‘åˆ†æå™¨ - å¤„ç†å¬è§‰æ¨¡æ€"""
    
    def __init__(self):
        self.sample_rate = model_config.AUDIO_SAMPLE_RATE
    
    def analyze_audio(self, audio_path: str) -> Dict[str, Any]:
        """åˆ†æéŸ³é¢‘æ–‡ä»¶ï¼Œæå–å¬è§‰ç‰¹å¾"""
        
        start_time = datetime.now()
        logger.info(f"ğŸµ å¼€å§‹éŸ³é¢‘åˆ†æ: {audio_path}")
        
        # æ£€æŸ¥éŸ³é¢‘æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not Path(audio_path).exists():
            error_msg = f"éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {audio_path}"
            logger.error(f"âŒ {error_msg}")
            raise FileNotFoundError(error_msg)
        
        # æ£€æŸ¥æ–‡ä»¶å¤§å°
        file_size = Path(audio_path).stat().st_size
        if file_size == 0:
            error_msg = "éŸ³é¢‘æ–‡ä»¶ä¸ºç©º"
            logger.error(f"âŒ {error_msg}")
            raise Exception(error_msg)
        
        logger.info(f"ğŸ“Š éŸ³é¢‘æ–‡ä»¶å¤§å°: {file_size / 1024:.1f} KB")
        
        try:
            # åŠ è½½éŸ³é¢‘æ–‡ä»¶
            logger.debug("ğŸ”„ åŠ è½½éŸ³é¢‘æ–‡ä»¶...")
            y, sr = librosa.load(audio_path, sr=self.sample_rate)
            
            if len(y) == 0:
                error_msg = "éŸ³é¢‘æ•°æ®ä¸ºç©º"
                logger.error(f"âŒ {error_msg}")
                raise Exception(error_msg)
            
            duration = len(y) / sr
            logger.info(f"ğŸ“Š éŸ³é¢‘ä¿¡æ¯: é‡‡æ ·ç‡{sr}Hz, æ—¶é•¿{duration:.2f}ç§’, {len(y)}ä¸ªé‡‡æ ·ç‚¹")
            
            # è¯­é€Ÿåˆ†æ
            logger.debug("ğŸ—£ï¸ åˆ†æè¯­é€Ÿ...")
            speech_rate = self._analyze_speech_rate(y, sr)
            logger.debug(f"âœ… è¯­é€Ÿåˆ†æå®Œæˆ: {speech_rate:.1f} BPM")
            
            # éŸ³é«˜åˆ†æ
            logger.debug("ğŸ¼ åˆ†æéŸ³é«˜...")
            pitch_analysis = self._analyze_pitch(y, sr)
            logger.debug(f"âœ… éŸ³é«˜åˆ†æå®Œæˆ: å¹³å‡{pitch_analysis['mean']:.1f}Hz")
            
            # éŸ³é‡åˆ†æ
            logger.debug("ğŸ“¢ åˆ†æéŸ³é‡...")
            volume_analysis = self._analyze_volume(y)
            logger.debug(f"âœ… éŸ³é‡åˆ†æå®Œæˆ: å¹³å‡{volume_analysis['mean']:.1f}dB")
            
            # è¯­éŸ³æ¸…æ™°åº¦åˆ†æ
            logger.debug("ğŸ¯ åˆ†ææ¸…æ™°åº¦...")
            clarity_score = self._analyze_clarity(y, sr)
            logger.debug(f"âœ… æ¸…æ™°åº¦åˆ†æå®Œæˆ: {clarity_score:.2f}")
            
            processing_time = (datetime.now() - start_time).total_seconds()
            
            result = {
                'speech_rate_bpm': speech_rate,
                'pitch_mean': pitch_analysis['mean'],
                'pitch_variance': pitch_analysis['variance'],
                'pitch_range': pitch_analysis['range'],
                'volume_mean': volume_analysis['mean'],
                'volume_variance': volume_analysis['variance'],
                'clarity_score': clarity_score,
                'audio_duration': duration,
                'processing_stats': {
                    'processing_time_seconds': processing_time,
                    'sample_rate': sr,
                    'samples_count': len(y),
                    'file_size_bytes': file_size
                }
            }
            
            logger.info(f"âœ… éŸ³é¢‘åˆ†ææˆåŠŸå®Œæˆ (è€—æ—¶: {processing_time:.2f}ç§’)")
            return result
            
        except Exception as e:
            processing_time = (datetime.now() - start_time).total_seconds()
            logger.error(f"âŒ éŸ³é¢‘åˆ†æå¤±è´¥: {str(e)}")
            logger.error(f"ğŸ”§ é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            logger.error(f"â±ï¸ å¤±è´¥å‰å¤„ç†æ—¶é—´: {processing_time:.2f}ç§’")
            raise
    
    def _analyze_speech_rate(self, y: np.ndarray, sr: int) -> float:
        """åˆ†æè¯­é€Ÿ"""
        
        try:
            # ä½¿ç”¨èŠ‚æ‹è¿½è¸ªä¼°ç®—è¯­é€Ÿ
            tempo, beats = librosa.beat.beat_track(y=y, sr=sr)
            
            # å°†éŸ³ä¹èŠ‚æ‹è½¬æ¢ä¸ºè¯­éŸ³èŠ‚å¥çš„ä¼°ç®—
            # è¯­éŸ³çš„"èŠ‚æ‹"é€šå¸¸æ¯”éŸ³ä¹æ…¢ï¼Œéœ€è¦è°ƒæ•´
            speech_tempo = tempo * 0.6  # ç»éªŒè°ƒæ•´å› å­
            
            return float(speech_tempo)
            
        except Exception as e:
            logger.error(f"âŒ è¯­é€Ÿåˆ†æå¤±è´¥: {str(e)}")
            logger.error(f"ğŸ”§ é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            raise
    
    def _analyze_pitch(self, y: np.ndarray, sr: int) -> Dict[str, float]:
        """åˆ†æéŸ³é«˜ç‰¹å¾"""
        
        try:
            # ä½¿ç”¨pyinç®—æ³•æå–åŸºé¢‘
            f0, voiced_flag, voiced_probs = librosa.pyin(
                y, fmin=librosa.note_to_hz('C2'), fmax=librosa.note_to_hz('C7')
            )
            
            # åªä¿ç•™æœ‰æ•ˆçš„éŸ³é«˜å€¼
            valid_f0 = f0[voiced_flag]
            
            if len(valid_f0) == 0:
                error_msg = "éŸ³é«˜åˆ†æå¤±è´¥ï¼šæœªæ£€æµ‹åˆ°æœ‰æ•ˆçš„éŸ³é«˜å€¼"
                logger.error(f"âŒ {error_msg}")
                raise Exception(error_msg)
            
            pitch_mean = float(np.mean(valid_f0))
            pitch_variance = float(np.var(valid_f0))
            pitch_range = float(np.max(valid_f0) - np.min(valid_f0))
            
            return {
                'mean': pitch_mean,
                'variance': pitch_variance,
                'range': pitch_range
            }
            
        except Exception as e:
            logger.error(f"âŒ éŸ³é«˜åˆ†æå¤±è´¥: {str(e)}")
            logger.error(f"ğŸ”§ é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            raise
    
    def _analyze_volume(self, y: np.ndarray) -> Dict[str, float]:
        """åˆ†æéŸ³é‡ç‰¹å¾"""
        
        try:
            # è®¡ç®—RMSèƒ½é‡
            rms = librosa.feature.rms(y=y)[0]
            
            if len(rms) == 0:
                error_msg = "éŸ³é‡åˆ†æå¤±è´¥ï¼šRMSè®¡ç®—ç»“æœä¸ºç©º"
                logger.error(f"âŒ {error_msg}")
                raise Exception(error_msg)
            
            # è½¬æ¢ä¸ºåˆ†è´
            db = librosa.amplitude_to_db(rms)
            
            volume_mean = float(np.mean(db))
            volume_variance = float(np.var(db))
            
            return {
                'mean': volume_mean,
                'variance': volume_variance
            }
            
        except Exception as e:
            logger.error(f"âŒ éŸ³é‡åˆ†æå¤±è´¥: {str(e)}")
            logger.error(f"ğŸ”§ é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            raise
    
    def _analyze_clarity(self, y: np.ndarray, sr: int) -> float:
        """åˆ†æè¯­éŸ³æ¸…æ™°åº¦"""
        
        try:
            # ä½¿ç”¨é¢‘è°±è´¨å¿ƒä½œä¸ºæ¸…æ™°åº¦çš„ä»£ç†æŒ‡æ ‡
            spectral_centroids = librosa.feature.spectral_centroid(y=y, sr=sr)[0]
            
            if len(spectral_centroids) == 0:
                error_msg = "æ¸…æ™°åº¦åˆ†æå¤±è´¥ï¼šé¢‘è°±è´¨å¿ƒè®¡ç®—ç»“æœä¸ºç©º"
                logger.error(f"âŒ {error_msg}")
                raise Exception(error_msg)
            
            # è®¡ç®—æ¸…æ™°åº¦åˆ†æ•° (é¢‘è°±è´¨å¿ƒçš„ç¨³å®šæ€§)
            centroid_variance = np.var(spectral_centroids)
            clarity_score = 1.0 / (1.0 + centroid_variance / 1000000)
            
            return float(clarity_score)
            
        except Exception as e:
            logger.error(f"âŒ æ¸…æ™°åº¦åˆ†æå¤±è´¥: {str(e)}")
            logger.error(f"ğŸ”§ é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            raise


class MultimodalAnalyzer:
    """å¤šæ¨¡æ€åˆ†æå™¨ - æ•´åˆè§†è§‰å’Œå¬è§‰åˆ†æ"""
    
    def __init__(self):
        self.video_analyzer = VideoAnalyzer()
        self.audio_analyzer = AudioAnalyzer()
    
    def analyze_interview_media(
        self, 
        video_path: Optional[str] = None, 
        audio_path: Optional[str] = None
    ) -> Dict[str, Any]:
        """åˆ†æé¢è¯•çš„éŸ³è§†é¢‘æ•°æ®"""
        
        start_time = datetime.now()
        logger.info("ğŸš€ å¼€å§‹å¤šæ¨¡æ€é¢è¯•åˆ†æ")
        logger.info(f"ğŸ“ è§†é¢‘æ–‡ä»¶: {video_path if video_path else 'æœªæä¾›'}")
        logger.info(f"ğŸ“ éŸ³é¢‘æ–‡ä»¶: {audio_path if audio_path else 'æœªæä¾›'}")
        
        if not video_path and not audio_path:
            error_msg = "å¿…é¡»è‡³å°‘æä¾›è§†é¢‘æ–‡ä»¶æˆ–éŸ³é¢‘æ–‡ä»¶è·¯å¾„"
            logger.error(f"âŒ {error_msg}")
            raise ValueError(error_msg)
        
        result = {
            'visual_analysis': None,
            'audio_analysis': None,
            'analysis_timestamp': start_time.isoformat(),
            'processing_summary': {
                'video_success': False,
                'audio_success': False,
                'errors': []
            }
        }
        
        # DeepFaceå¯ç”¨æ€§æ£€æŸ¥
        if not DEEPFACE_AVAILABLE:
            error_msg = f"DeepFaceä¸å¯ç”¨ï¼Œæ— æ³•è¿›è¡Œæƒ…ç»ªåˆ†æ: {DEEPFACE_ERROR}"
            logger.error(f"âŒ {error_msg}")
            result['processing_summary']['errors'].append(error_msg)
        
        # è§†è§‰åˆ†æ
        if video_path:
            logger.info("=" * 50)
            logger.info("ğŸ¥ å¼€å§‹è§†è§‰åˆ†æéƒ¨åˆ†")
            
            try:
                result['visual_analysis'] = self.video_analyzer.analyze_video(video_path)
                result['processing_summary']['video_success'] = True
                logger.info("âœ… è§†é¢‘åˆ†æéƒ¨åˆ†å®Œæˆ")
            except Exception as e:
                error_msg = f"è§†é¢‘åˆ†æå¤±è´¥: {str(e)}"
                logger.error(f"âŒ {error_msg}")
                logger.error(f"ğŸ”§ é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
                result['processing_summary']['errors'].append(error_msg)
                # ä¸å†æä¾›å¤‡ç”¨ç»“æœï¼Œç›´æ¥è®°å½•é”™è¯¯
        
        # å¬è§‰åˆ†æ
        if audio_path:
            logger.info("=" * 50)
            logger.info("ğŸµ å¼€å§‹å¬è§‰åˆ†æéƒ¨åˆ†")
            
            try:
                result['audio_analysis'] = self.audio_analyzer.analyze_audio(audio_path)
                result['processing_summary']['audio_success'] = True
                logger.info("âœ… éŸ³é¢‘åˆ†æéƒ¨åˆ†å®Œæˆ")
            except Exception as e:
                error_msg = f"éŸ³é¢‘åˆ†æå¤±è´¥: {str(e)}"
                logger.error(f"âŒ {error_msg}")
                logger.error(f"ğŸ”§ é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
                result['processing_summary']['errors'].append(error_msg)
                # ä¸å†æä¾›å¤‡ç”¨ç»“æœï¼Œç›´æ¥è®°å½•é”™è¯¯
        
        # åˆ†ææ€»ç»“
        total_processing_time = (datetime.now() - start_time).total_seconds()
        result['processing_summary']['total_time_seconds'] = total_processing_time
        
        logger.info("=" * 50)
        logger.info("ğŸ“Š å¤šæ¨¡æ€åˆ†ææ€»ç»“:")
        logger.info(f"   ğŸ¥ è§†é¢‘åˆ†æ: {'âœ… æˆåŠŸ' if result['processing_summary']['video_success'] else 'âŒ å¤±è´¥/è·³è¿‡'}")
        logger.info(f"   ğŸµ éŸ³é¢‘åˆ†æ: {'âœ… æˆåŠŸ' if result['processing_summary']['audio_success'] else 'âŒ å¤±è´¥/è·³è¿‡'}")
        logger.info(f"   â±ï¸ æ€»å¤„ç†æ—¶é—´: {total_processing_time:.2f}ç§’")
        
        if result['processing_summary']['errors']:
            logger.error(f"   âš ï¸ é”™è¯¯æ•°é‡: {len(result['processing_summary']['errors'])}")
            for i, error in enumerate(result['processing_summary']['errors'], 1):
                logger.error(f"     {i}. {error}")
            
            # å¦‚æœæ‰€æœ‰åˆ†æéƒ½å¤±è´¥ï¼ŒæŠ›å‡ºå¼‚å¸¸
            if not result['processing_summary']['video_success'] and not result['processing_summary']['audio_success']:
                error_msg = "æ‰€æœ‰å¤šæ¨¡æ€åˆ†æå‡å¤±è´¥"
                logger.error(f"âŒ {error_msg}")
                raise Exception(error_msg)
        else:
            logger.info("   ğŸ‰ æ‰€æœ‰åˆ†æå‡æˆåŠŸå®Œæˆ!")
        
        logger.info("ğŸ å¤šæ¨¡æ€é¢è¯•åˆ†æå®Œæˆ")
        return result


def create_multimodal_analyzer() -> MultimodalAnalyzer:
    """åˆ›å»ºå¤šæ¨¡æ€åˆ†æå™¨å®ä¾‹"""
    return MultimodalAnalyzer() 