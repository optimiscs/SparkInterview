"""
å¤šæ¨¡æ€åˆ†æå·¥å…·
é›†æˆMediaPipeã€DeepFaceã€Librosaç­‰åº“è¿›è¡ŒçœŸå®çš„éŸ³è§†é¢‘åˆ†æ
"""
import cv2
import numpy as np
import librosa
import mediapipe as mp
from typing import Dict, Any, List, Optional, Tuple
import logging
from pathlib import Path

try:
    from deepface import DeepFace
    DEEPFACE_AVAILABLE = True
except ImportError:
    DEEPFACE_AVAILABLE = False
    logging.warning("DeepFace not available, using fallback emotion analysis")

from ..config.settings import model_config


class VideoAnalyzer:
    """è§†é¢‘åˆ†æå™¨ - å¤„ç†è§†è§‰æ¨¡æ€"""
    
    def __init__(self):
        # åˆå§‹åŒ–MediaPipe
        self.mp_face_mesh = mp.solutions.face_mesh
        self.mp_drawing = mp.solutions.drawing_utils
        self.face_mesh = self.mp_face_mesh.FaceMesh(
            static_image_mode=False,
            max_num_faces=1,
            refine_landmarks=True,
            min_detection_confidence=model_config.MEDIAPIPE_CONFIDENCE,
            min_tracking_confidence=model_config.MEDIAPIPE_CONFIDENCE
        )
        
        # é¢éƒ¨å…³é”®ç‚¹ç´¢å¼• (468ä¸ªå…³é”®ç‚¹ä¸­çš„é‡è¦ç‚¹)
        self.face_landmarks_indexes = {
            'nose_tip': 1,
            'chin': 175,
            'left_eye_corner': 33,
            'right_eye_corner': 263,
            'left_iris': 468,
            'right_iris': 473
        }
        
        # 3Dé¢éƒ¨æ¨¡å‹ç‚¹ (ç”¨äºPnPç®—æ³•)
        self.model_points = np.array([
            (0.0, 0.0, 0.0),      # é¼»å°–
            (0.0, -330.0, -65.0), # ä¸‹å·´
            (-225.0, 170.0, -135.0), # å·¦çœ¼è§’
            (225.0, 170.0, -135.0),  # å³çœ¼è§’
            (-150.0, -150.0, -125.0), # å·¦å˜´è§’
            (150.0, -150.0, -125.0)   # å³å˜´è§’
        ], dtype=np.float64)
    
    def analyze_video(self, video_path: str) -> Dict[str, Any]:
        """åˆ†æè§†é¢‘æ–‡ä»¶ï¼Œæå–è§†è§‰ç‰¹å¾"""
        
        try:
            cap = cv2.VideoCapture(video_path)
            if not cap.isOpened():
                raise Exception(f"æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶: {video_path}")
            
            # åˆå§‹åŒ–åˆ†æç»“æœå­˜å‚¨
            head_poses = []
            gaze_directions = []
            emotions_timeline = []
            frame_count = 0
            
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                
                frame_count += 1
                
                # è½¬æ¢é¢œè‰²ç©ºé—´
                rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                
                # MediaPipeé¢éƒ¨æ£€æµ‹
                results = self.face_mesh.process(rgb_frame)
                
                if results.multi_face_landmarks:
                    face_landmarks = results.multi_face_landmarks[0]
                    
                    # åˆ†æå¤´éƒ¨å§¿æ€
                    head_pose = self._analyze_head_pose(face_landmarks, frame.shape)
                    if head_pose:
                        head_poses.append(head_pose)
                    
                    # åˆ†æè§†çº¿æ–¹å‘
                    gaze = self._analyze_gaze_direction(face_landmarks, frame.shape)
                    if gaze:
                        gaze_directions.append(gaze)
                    
                    # æƒ…ç»ªåˆ†æ (æ¯10å¸§åˆ†æä¸€æ¬¡ä»¥æé«˜æ•ˆç‡)
                    if frame_count % 10 == 0:
                        emotion = self._analyze_emotion(frame)
                        if emotion:
                            emotions_timeline.append({
                                'frame': frame_count,
                                'timestamp': frame_count / cap.get(cv2.CAP_PROP_FPS),
                                **emotion
                            })
            
            cap.release()
            
            # è®¡ç®—ç»Ÿè®¡ç‰¹å¾
            analysis_result = self._compute_visual_statistics(
                head_poses, gaze_directions, emotions_timeline
            )
            
            return analysis_result
            
        except Exception as e:
            logging.error(f"è§†é¢‘åˆ†æå¤±è´¥: {str(e)}")
            return self._get_fallback_visual_analysis()
    
    def _analyze_head_pose(self, face_landmarks, frame_shape) -> Optional[Dict[str, float]]:
        """åˆ†æå¤´éƒ¨å§¿æ€"""
        
        try:
            h, w = frame_shape[:2]
            
            # æå–2Då…³é”®ç‚¹
            landmark_points = []
            for idx in [1, 175, 33, 263, 61, 291]:  # é¼»å°–ã€ä¸‹å·´ã€çœ¼è§’ã€å˜´è§’
                if idx < len(face_landmarks.landmark):
                    lm = face_landmarks.landmark[idx]
                    landmark_points.append([lm.x * w, lm.y * h])
            
            if len(landmark_points) < 6:
                return None
            
            image_points = np.array(landmark_points, dtype=np.float64)
            
            # ç›¸æœºå‚æ•° (ä¼°ç®—)
            focal_length = w
            center = (w/2, h/2)
            camera_matrix = np.array([
                [focal_length, 0, center[0]],
                [0, focal_length, center[1]],
                [0, 0, 1]
            ], dtype=np.float64)
            
            dist_coeffs = np.zeros((4,1))
            
            # ä½¿ç”¨PnPç®—æ³•æ±‚è§£å¤´éƒ¨å§¿æ€
            success, rotation_vector, translation_vector = cv2.solvePnP(
                self.model_points, image_points, camera_matrix, dist_coeffs
            )
            
            if success:
                # è½¬æ¢ä¸ºæ¬§æ‹‰è§’
                rotation_matrix, _ = cv2.Rodrigues(rotation_vector)
                angles = self._rotation_matrix_to_euler(rotation_matrix)
                
                return {
                    'pitch': float(angles[0]),  # ä¿¯ä»°è§’
                    'yaw': float(angles[1]),    # åèˆªè§’
                    'roll': float(angles[2])    # ç¿»æ»šè§’
                }
            
        except Exception as e:
            logging.warning(f"å¤´éƒ¨å§¿æ€åˆ†æå¤±è´¥: {str(e)}")
        
        return None
    
    def _analyze_gaze_direction(self, face_landmarks, frame_shape) -> Optional[Dict[str, float]]:
        """åˆ†æè§†çº¿æ–¹å‘"""
        
        try:
            h, w = frame_shape[:2]
            
            # è·å–è™¹è†œå’Œçœ¼è§’å…³é”®ç‚¹
            left_iris_idx = 468
            right_iris_idx = 473
            left_eye_corner_idx = 33
            right_eye_corner_idx = 263
            
            if (left_iris_idx >= len(face_landmarks.landmark) or 
                right_iris_idx >= len(face_landmarks.landmark)):
                return None
            
            # è®¡ç®—è§†çº¿å‘é‡
            left_iris = face_landmarks.landmark[left_iris_idx]
            right_iris = face_landmarks.landmark[right_iris_idx]
            left_corner = face_landmarks.landmark[left_eye_corner_idx]
            right_corner = face_landmarks.landmark[right_eye_corner_idx]
            
            # è®¡ç®—è§†çº¿åç§»
            left_gaze_x = (left_iris.x - left_corner.x) * w
            left_gaze_y = (left_iris.y - left_corner.y) * h
            right_gaze_x = (right_iris.x - right_corner.x) * w
            right_gaze_y = (right_iris.y - right_corner.y) * h
            
            # å¹³å‡è§†çº¿æ–¹å‘
            avg_gaze_x = (left_gaze_x + right_gaze_x) / 2
            avg_gaze_y = (left_gaze_y + right_gaze_y) / 2
            
            return {
                'gaze_x': float(avg_gaze_x),
                'gaze_y': float(avg_gaze_y),
                'gaze_magnitude': float(np.sqrt(avg_gaze_x**2 + avg_gaze_y**2))
            }
            
        except Exception as e:
            logging.warning(f"è§†çº¿åˆ†æå¤±è´¥: {str(e)}")
        
        return None
    
    def _analyze_emotion(self, frame) -> Optional[Dict[str, float]]:
        """åˆ†ææƒ…ç»ª"""
        
        if not DEEPFACE_AVAILABLE:
            return self._fallback_emotion_analysis()
        
        try:
            # ä½¿ç”¨DeepFaceåˆ†ææƒ…ç»ª
            result = DeepFace.analyze(
                frame, 
                actions=['emotion'], 
                enforce_detection=False,
                detector_backend=model_config.DEEPFACE_BACKEND
            )
            
            if isinstance(result, list):
                result = result[0]
            
            emotions = result.get('emotion', {})
            
            # æ‰¾å‡ºä¸»å¯¼æƒ…ç»ª
            dominant_emotion = max(emotions.items(), key=lambda x: x[1])
            
            return {
                'dominant_emotion': dominant_emotion[0],
                'dominant_score': float(dominant_emotion[1]),
                'emotions': {k: float(v) for k, v in emotions.items()}
            }
            
        except Exception as e:
            logging.warning(f"æƒ…ç»ªåˆ†æå¤±è´¥: {str(e)}")
            return self._fallback_emotion_analysis()
    
    def _rotation_matrix_to_euler(self, rotation_matrix) -> Tuple[float, float, float]:
        """æ—‹è½¬çŸ©é˜µè½¬æ¬§æ‹‰è§’"""
        
        sy = np.sqrt(rotation_matrix[0,0] * rotation_matrix[0,0] + 
                    rotation_matrix[1,0] * rotation_matrix[1,0])
        
        singular = sy < 1e-6
        
        if not singular:
            x = np.arctan2(rotation_matrix[2,1], rotation_matrix[2,2])
            y = np.arctan2(-rotation_matrix[2,0], sy)
            z = np.arctan2(rotation_matrix[1,0], rotation_matrix[0,0])
        else:
            x = np.arctan2(-rotation_matrix[1,2], rotation_matrix[1,1])
            y = np.arctan2(-rotation_matrix[2,0], sy)
            z = 0
        
        return np.degrees([x, y, z])
    
    def _compute_visual_statistics(
        self, 
        head_poses: List[Dict], 
        gaze_directions: List[Dict], 
        emotions_timeline: List[Dict]
    ) -> Dict[str, Any]:
        """è®¡ç®—è§†è§‰åˆ†æç»Ÿè®¡ç‰¹å¾"""
        
        result = {
            'head_pose_stability': 0.8,
            'gaze_stability': 0.7,
            'dominant_emotion': 'neutral',
            'emotion_stability': 0.8,
            'eye_contact_ratio': 0.75
        }
        
        if head_poses:
            # è®¡ç®—å¤´éƒ¨å§¿æ€ç¨³å®šæ€§
            yaw_values = [pose['yaw'] for pose in head_poses]
            pitch_values = [pose['pitch'] for pose in head_poses]
            
            yaw_variance = np.var(yaw_values)
            pitch_variance = np.var(pitch_values)
            
            # ç¨³å®šæ€§ä¸æ–¹å·®æˆåæ¯”
            head_stability = 1.0 / (1.0 + (yaw_variance + pitch_variance) / 100)
            result['head_pose_stability'] = float(head_stability)
        
        if gaze_directions:
            # è®¡ç®—è§†çº¿ç¨³å®šæ€§
            gaze_magnitudes = [gaze['gaze_magnitude'] for gaze in gaze_directions]
            gaze_variance = np.var(gaze_magnitudes)
            
            gaze_stability = 1.0 / (1.0 + gaze_variance / 10)
            result['gaze_stability'] = float(gaze_stability)
            
            # è®¡ç®—çœ¼ç¥æ¥è§¦æ¯”ä¾‹ (è§†çº¿åç§»å°äºé˜ˆå€¼çš„å¸§æ•°æ¯”ä¾‹)
            eye_contact_frames = sum(1 for mag in gaze_magnitudes if mag < 5.0)
            result['eye_contact_ratio'] = eye_contact_frames / len(gaze_magnitudes)
        
        if emotions_timeline:
            # åˆ†ææƒ…ç»ªåˆ†å¸ƒ
            emotion_counts = {}
            for emotion_data in emotions_timeline:
                emotion = emotion_data['dominant_emotion']
                emotion_counts[emotion] = emotion_counts.get(emotion, 0) + 1
            
            # ä¸»å¯¼æƒ…ç»ª
            if emotion_counts:
                result['dominant_emotion'] = max(emotion_counts.items(), key=lambda x: x[1])[0]
            
            # æƒ…ç»ªç¨³å®šæ€§ (è´Ÿé¢æƒ…ç»ªå æ¯”çš„å€’æ•°)
            negative_emotions = ['angry', 'disgust', 'fear', 'sad']
            negative_count = sum(emotion_counts.get(emotion, 0) for emotion in negative_emotions)
            total_count = sum(emotion_counts.values())
            
            if total_count > 0:
                negative_ratio = negative_count / total_count
                result['emotion_stability'] = 1.0 - negative_ratio
        
        return result
    
    def _fallback_emotion_analysis(self) -> Dict[str, Any]:
        """DeepFaceä¸å¯ç”¨æ—¶çš„å¤‡ç”¨æƒ…ç»ªåˆ†æ"""
        return {
            'dominant_emotion': 'neutral',
            'dominant_score': 0.7,
            'emotions': {
                'neutral': 0.7,
                'happy': 0.1,
                'sad': 0.05,
                'angry': 0.05,
                'surprise': 0.05,
                'disgust': 0.025,
                'fear': 0.025
            }
        }
    
    def _get_fallback_visual_analysis(self) -> Dict[str, Any]:
        """è§†é¢‘åˆ†æå¤±è´¥æ—¶çš„å¤‡ç”¨ç»“æœ"""
        return {
            'head_pose_stability': 0.7,
            'gaze_stability': 0.7,
            'dominant_emotion': 'neutral',
            'emotion_stability': 0.8,
            'eye_contact_ratio': 0.6,
            'error': True,
            'message': 'è§†é¢‘åˆ†æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼'
        }


class AudioAnalyzer:
    """éŸ³é¢‘åˆ†æå™¨ - å¤„ç†å¬è§‰æ¨¡æ€"""
    
    def __init__(self):
        self.sample_rate = model_config.AUDIO_SAMPLE_RATE
    
    def analyze_audio(self, audio_path: str) -> Dict[str, Any]:
        """åˆ†æéŸ³é¢‘æ–‡ä»¶ï¼Œæå–å¬è§‰ç‰¹å¾"""
        
        try:
            # åŠ è½½éŸ³é¢‘æ–‡ä»¶
            y, sr = librosa.load(audio_path, sr=self.sample_rate)
            
            if len(y) == 0:
                raise Exception("éŸ³é¢‘æ–‡ä»¶ä¸ºç©º")
            
            # è¯­é€Ÿåˆ†æ
            speech_rate = self._analyze_speech_rate(y, sr)
            
            # éŸ³é«˜åˆ†æ
            pitch_analysis = self._analyze_pitch(y, sr)
            
            # éŸ³é‡åˆ†æ
            volume_analysis = self._analyze_volume(y)
            
            # è¯­éŸ³æ¸…æ™°åº¦åˆ†æ
            clarity_score = self._analyze_clarity(y, sr)
            
            return {
                'speech_rate_bpm': speech_rate,
                'pitch_mean': pitch_analysis['mean'],
                'pitch_variance': pitch_analysis['variance'],
                'pitch_range': pitch_analysis['range'],
                'volume_mean': volume_analysis['mean'],
                'volume_variance': volume_analysis['variance'],
                'clarity_score': clarity_score,
                'audio_duration': len(y) / sr
            }
            
        except Exception as e:
            logging.error(f"éŸ³é¢‘åˆ†æå¤±è´¥: {str(e)}")
            return self._get_fallback_audio_analysis()
    
    def _analyze_speech_rate(self, y: np.ndarray, sr: int) -> float:
        """åˆ†æè¯­é€Ÿ"""
        
        try:
            # ä½¿ç”¨èŠ‚æ‹è¿½è¸ªä¼°ç®—è¯­é€Ÿ
            tempo, beats = librosa.beat.beat_track(y=y, sr=sr)
            
            # å°†éŸ³ä¹èŠ‚æ‹è½¬æ¢ä¸ºè¯­éŸ³èŠ‚å¥çš„ä¼°ç®—
            # è¯­éŸ³çš„"èŠ‚æ‹"é€šå¸¸æ¯”éŸ³ä¹æ…¢ï¼Œéœ€è¦è°ƒæ•´
            speech_tempo = tempo * 0.6  # ç»éªŒè°ƒæ•´å› å­
            
            return float(speech_tempo)
            
        except Exception as e:
            logging.warning(f"è¯­é€Ÿåˆ†æå¤±è´¥: {str(e)}")
            return 120.0  # é»˜è®¤è¯­é€Ÿ
    
    def _analyze_pitch(self, y: np.ndarray, sr: int) -> Dict[str, float]:
        """åˆ†æéŸ³é«˜ç‰¹å¾"""
        
        try:
            # ä½¿ç”¨pyinç®—æ³•æå–åŸºé¢‘
            f0, voiced_flag, voiced_probs = librosa.pyin(
                y, fmin=librosa.note_to_hz('C2'), fmax=librosa.note_to_hz('C7')
            )
            
            # åªä¿ç•™æœ‰æ•ˆçš„éŸ³é«˜å€¼
            valid_f0 = f0[voiced_flag]
            
            if len(valid_f0) > 0:
                pitch_mean = float(np.mean(valid_f0))
                pitch_variance = float(np.var(valid_f0))
                pitch_range = float(np.max(valid_f0) - np.min(valid_f0))
            else:
                pitch_mean = 150.0  # é»˜è®¤éŸ³é«˜
                pitch_variance = 20.0
                pitch_range = 50.0
            
            return {
                'mean': pitch_mean,
                'variance': pitch_variance,
                'range': pitch_range
            }
            
        except Exception as e:
            logging.warning(f"éŸ³é«˜åˆ†æå¤±è´¥: {str(e)}")
            return {
                'mean': 150.0,
                'variance': 20.0,
                'range': 50.0
            }
    
    def _analyze_volume(self, y: np.ndarray) -> Dict[str, float]:
        """åˆ†æéŸ³é‡ç‰¹å¾"""
        
        try:
            # è®¡ç®—RMSèƒ½é‡
            rms = librosa.feature.rms(y=y)[0]
            
            # è½¬æ¢ä¸ºåˆ†è´
            db = librosa.amplitude_to_db(rms)
            
            volume_mean = float(np.mean(db))
            volume_variance = float(np.var(db))
            
            return {
                'mean': volume_mean,
                'variance': volume_variance
            }
            
        except Exception as e:
            logging.warning(f"éŸ³é‡åˆ†æå¤±è´¥: {str(e)}")
            return {
                'mean': -20.0,
                'variance': 5.0
            }
    
    def _analyze_clarity(self, y: np.ndarray, sr: int) -> float:
        """åˆ†æè¯­éŸ³æ¸…æ™°åº¦"""
        
        try:
            # ä½¿ç”¨é¢‘è°±è´¨å¿ƒä½œä¸ºæ¸…æ™°åº¦çš„ä»£ç†æŒ‡æ ‡
            spectral_centroids = librosa.feature.spectral_centroid(y=y, sr=sr)[0]
            
            # è®¡ç®—æ¸…æ™°åº¦åˆ†æ•° (é¢‘è°±è´¨å¿ƒçš„ç¨³å®šæ€§)
            centroid_variance = np.var(spectral_centroids)
            clarity_score = 1.0 / (1.0 + centroid_variance / 1000000)
            
            return float(clarity_score)
            
        except Exception as e:
            logging.warning(f"æ¸…æ™°åº¦åˆ†æå¤±è´¥: {str(e)}")
            return 0.8  # é»˜è®¤æ¸…æ™°åº¦
    
    def _get_fallback_audio_analysis(self) -> Dict[str, Any]:
        """éŸ³é¢‘åˆ†æå¤±è´¥æ—¶çš„å¤‡ç”¨ç»“æœ"""
        return {
            'speech_rate_bpm': 120.0,
            'pitch_mean': 150.0,
            'pitch_variance': 20.0,
            'pitch_range': 50.0,
            'volume_mean': -20.0,
            'volume_variance': 5.0,
            'clarity_score': 0.8,
            'audio_duration': 60.0,
            'error': True,
            'message': 'éŸ³é¢‘åˆ†æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼'
        }


class MultimodalAnalyzer:
    """å¤šæ¨¡æ€åˆ†æå™¨ - æ•´åˆè§†è§‰å’Œå¬è§‰åˆ†æ"""
    
    def __init__(self):
        self.video_analyzer = VideoAnalyzer()
        self.audio_analyzer = AudioAnalyzer()
    
    def analyze_interview_media(
        self, 
        video_path: Optional[str] = None, 
        audio_path: Optional[str] = None
    ) -> Dict[str, Any]:
        """åˆ†æé¢è¯•çš„éŸ³è§†é¢‘æ•°æ®"""
        
        result = {
            'visual_analysis': None,
            'audio_analysis': None,
            'analysis_timestamp': None
        }
        
        # è§†è§‰åˆ†æ
        if video_path:
            video_file = Path(video_path)
            if video_file.exists() and video_file.stat().st_size > 0:
                print(f"ğŸ¥ å¼€å§‹è§†é¢‘åˆ†æ: {video_path}")
                try:
                    result['visual_analysis'] = self.video_analyzer.analyze_video(video_path)
                    print("âœ… è§†é¢‘åˆ†æå®Œæˆ")
                except Exception as e:
                    print(f"âš ï¸ è§†é¢‘åˆ†æå¤±è´¥: {e}")
                    result['visual_analysis'] = self.video_analyzer._get_fallback_visual_analysis()
            else:
                print(f"âš ï¸ è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨æˆ–ä¸ºç©º: {video_path}")
                result['visual_analysis'] = self.video_analyzer._get_fallback_visual_analysis()
        else:
            print("âš ï¸ æœªæä¾›è§†é¢‘æ–‡ä»¶è·¯å¾„ï¼Œä½¿ç”¨é»˜è®¤è§†è§‰åˆ†æ")
            result['visual_analysis'] = self.video_analyzer._get_fallback_visual_analysis()
        
        # å¬è§‰åˆ†æ
        if audio_path:
            audio_file = Path(audio_path)
            if audio_file.exists() and audio_file.stat().st_size > 0:
                print(f"ğŸµ å¼€å§‹éŸ³é¢‘åˆ†æ: {audio_path}")
                try:
                    result['audio_analysis'] = self.audio_analyzer.analyze_audio(audio_path)
                    print("âœ… éŸ³é¢‘åˆ†æå®Œæˆ")
                except Exception as e:
                    print(f"âš ï¸ éŸ³é¢‘åˆ†æå¤±è´¥: {e}")
                    result['audio_analysis'] = self.audio_analyzer._get_fallback_audio_analysis()
            else:
                print(f"âš ï¸ éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨æˆ–ä¸ºç©º: {audio_path}")
                result['audio_analysis'] = self.audio_analyzer._get_fallback_audio_analysis()
        else:
            print("âš ï¸ æœªæä¾›éŸ³é¢‘æ–‡ä»¶è·¯å¾„ï¼Œä½¿ç”¨é»˜è®¤å¬è§‰åˆ†æ")
            result['audio_analysis'] = self.audio_analyzer._get_fallback_audio_analysis()
        
        # æ·»åŠ æ—¶é—´æˆ³
        from datetime import datetime
        result['analysis_timestamp'] = datetime.now().isoformat()
        
        return result


def create_multimodal_analyzer() -> MultimodalAnalyzer:
    """åˆ›å»ºå¤šæ¨¡æ€åˆ†æå™¨å®ä¾‹"""
    return MultimodalAnalyzer() 