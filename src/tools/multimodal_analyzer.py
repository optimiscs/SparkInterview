"""
å¤šæ¨¡æ€åˆ†æå·¥å…·
é›†æˆMediaPipeã€DeepFaceã€Librosaç­‰åº“è¿›è¡ŒçœŸå®çš„éŸ³è§†é¢‘åˆ†æ
"""
import cv2
import numpy as np
import librosa
import mediapipe as mp
from typing import Dict, Any, List, Optional, Tuple
import logging
from pathlib import Path
import sys
import traceback
from datetime import datetime

# é…ç½®è¯¦ç»†æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# DeepFaceå¯ç”¨æ€§æ£€æŸ¥å’Œåˆå§‹åŒ–
DEEPFACE_AVAILABLE = False
DEEPFACE_ERROR = None

def check_deepface_availability():
    """æ£€æŸ¥DeepFaceæ˜¯å¦å¯ç”¨å¹¶è¿›è¡Œåˆå§‹åŒ–æµ‹è¯•"""
    global DEEPFACE_AVAILABLE, DEEPFACE_ERROR
    
    try:
        logger.info("ğŸ” æ£€æŸ¥DeepFaceå¯ç”¨æ€§...")
        from deepface import DeepFace
        
        # å°è¯•åˆå§‹åŒ–DeepFaceè¿›è¡Œç®€å•æµ‹è¯•
        logger.info("ğŸ§ª æµ‹è¯•DeepFaceåŠŸèƒ½...")
        
        # åˆ›å»ºä¸€ä¸ªæµ‹è¯•å›¾åƒ
        test_image = np.zeros((100, 100, 3), dtype=np.uint8)
        test_image[30:70, 30:70] = [255, 255, 255]  # ç™½è‰²æ–¹å—æ¨¡æ‹Ÿäººè„¸
        
        # å°è¯•åˆ†ææµ‹è¯•å›¾åƒ
        result = DeepFace.analyze(
            test_image, 
            actions=['emotion'], 
            enforce_detection=False,
            silent=True
        )
        
        DEEPFACE_AVAILABLE = True
        logger.info("âœ… DeepFaceåˆå§‹åŒ–æˆåŠŸï¼Œæƒ…æ„Ÿåˆ†æåŠŸèƒ½å¯ç”¨")
        return True
        
    except ImportError as e:
        DEEPFACE_ERROR = f"DeepFaceæœªå®‰è£…: {str(e)}"
        logger.error(f"âŒ {DEEPFACE_ERROR}")
        logger.info("ğŸ’¡ è¯·å®‰è£…DeepFace: pip install deepface")
        return False
    except Exception as e:
        DEEPFACE_ERROR = f"DeepFaceåˆå§‹åŒ–å¤±è´¥: {str(e)}"
        logger.error(f"âŒ {DEEPFACE_ERROR}")
        logger.error(f"ğŸ”§ é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
        return False

# å¯åŠ¨æ—¶æ£€æŸ¥DeepFace
check_deepface_availability()

try:
    from ..config.settings import model_config
except ImportError:
    logger.warning("âš ï¸ æ— æ³•å¯¼å…¥model_configï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
    # é»˜è®¤é…ç½®
    class DefaultConfig:
        MEDIAPIPE_CONFIDENCE = 0.5
        DEEPFACE_BACKEND = 'opencv'
        AUDIO_SAMPLE_RATE = 22050
    model_config = DefaultConfig()


class VideoAnalyzer:
    """è§†é¢‘åˆ†æå™¨ - å¤„ç†è§†è§‰æ¨¡æ€"""
    
    def __init__(self):
        # åˆå§‹åŒ–MediaPipe
        self.mp_face_mesh = mp.solutions.face_mesh
        self.mp_drawing = mp.solutions.drawing_utils
        self.face_mesh = self.mp_face_mesh.FaceMesh(
            static_image_mode=False,
            max_num_faces=1,
            refine_landmarks=True,
            min_detection_confidence=model_config.MEDIAPIPE_CONFIDENCE,
            min_tracking_confidence=model_config.MEDIAPIPE_CONFIDENCE
        )
        
        # é¢éƒ¨å…³é”®ç‚¹ç´¢å¼• (468ä¸ªå…³é”®ç‚¹ä¸­çš„é‡è¦ç‚¹)
        self.face_landmarks_indexes = {
            'nose_tip': 1,
            'chin': 175,
            'left_eye_corner': 33,
            'right_eye_corner': 263,
            'left_iris': 468,
            'right_iris': 473
        }
        
        # 3Dé¢éƒ¨æ¨¡å‹ç‚¹ (ç”¨äºPnPç®—æ³•)
        self.model_points = np.array([
            (0.0, 0.0, 0.0),      # é¼»å°–
            (0.0, -330.0, -65.0), # ä¸‹å·´
            (-225.0, 170.0, -135.0), # å·¦çœ¼è§’
            (225.0, 170.0, -135.0),  # å³çœ¼è§’
            (-150.0, -150.0, -125.0), # å·¦å˜´è§’
            (150.0, -150.0, -125.0)   # å³å˜´è§’
        ], dtype=np.float64)
    
    def analyze_video(self, video_path: str) -> Dict[str, Any]:
        """åˆ†æè§†é¢‘æ–‡ä»¶ï¼Œæå–è§†è§‰ç‰¹å¾"""
        
        start_time = datetime.now()
        logger.info(f"ğŸ¥ å¼€å§‹è§†é¢‘åˆ†æ: {video_path}")
        
        try:
            # æ£€æŸ¥è§†é¢‘æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not Path(video_path).exists():
                raise FileNotFoundError(f"è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_path}")
            
            cap = cv2.VideoCapture(video_path)
            if not cap.isOpened():
                raise Exception(f"æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶: {video_path}")
            
            # è·å–è§†é¢‘åŸºæœ¬ä¿¡æ¯
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            fps = cap.get(cv2.CAP_PROP_FPS)
            duration = total_frames / fps if fps > 0 else 0
            
            logger.info(f"ğŸ“Š è§†é¢‘ä¿¡æ¯: {total_frames}å¸§, {fps:.1f}FPS, {duration:.1f}ç§’")
            
            # åˆå§‹åŒ–åˆ†æç»“æœå­˜å‚¨
            head_poses = []
            gaze_directions = []
            emotions_timeline = []
            frame_count = 0
            processed_frames = 0
            
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                
                frame_count += 1
                
                try:
                    # è½¬æ¢é¢œè‰²ç©ºé—´
                    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                    
                    # MediaPipeé¢éƒ¨æ£€æµ‹
                    results = self.face_mesh.process(rgb_frame)
                    
                    if results.multi_face_landmarks:
                        processed_frames += 1
                        face_landmarks = results.multi_face_landmarks[0]
                        
                        # åˆ†æå¤´éƒ¨å§¿æ€
                        head_pose = self._analyze_head_pose(face_landmarks, frame.shape)
                        if head_pose:
                            head_poses.append(head_pose)
                        
                        # åˆ†æè§†çº¿æ–¹å‘
                        gaze = self._analyze_gaze_direction(face_landmarks, frame.shape)
                        if gaze:
                            gaze_directions.append(gaze)
                        
                        # æƒ…ç»ªåˆ†æ (æ¯10å¸§åˆ†æä¸€æ¬¡ä»¥æé«˜æ•ˆç‡)
                        if frame_count % 10 == 0:
                            emotion = self._analyze_emotion(frame)
                            if emotion:
                                emotions_timeline.append({
                                    'frame': frame_count,
                                    'timestamp': frame_count / fps,
                                    **emotion
                                })
                    
                    # è¿›åº¦æ—¥å¿— (æ¯100å¸§æŠ¥å‘Šä¸€æ¬¡)
                    if frame_count % 100 == 0:
                        progress = (frame_count / total_frames) * 100 if total_frames > 0 else 0
                        logger.debug(f"ğŸ“ˆ è§†é¢‘åˆ†æè¿›åº¦: {progress:.1f}% ({frame_count}/{total_frames})")
                        
                except Exception as frame_error:
                    logger.warning(f"âš ï¸ ç¬¬{frame_count}å¸§åˆ†æå¤±è´¥: {str(frame_error)}")
                    continue
            
            cap.release()
            
            # åˆ†æç»“æœç»Ÿè®¡
            processing_time = (datetime.now() - start_time).total_seconds()
            logger.info(f"ğŸ“Š è§†é¢‘åˆ†æå®Œæˆ:")
            logger.info(f"   - æ€»å¸§æ•°: {frame_count}")
            logger.info(f"   - æ£€æµ‹åˆ°äººè„¸çš„å¸§æ•°: {processed_frames}")
            logger.info(f"   - å¤´éƒ¨å§¿æ€åˆ†æ: {len(head_poses)}ä¸ªæœ‰æ•ˆç»“æœ")
            logger.info(f"   - è§†çº¿æ–¹å‘åˆ†æ: {len(gaze_directions)}ä¸ªæœ‰æ•ˆç»“æœ") 
            logger.info(f"   - æƒ…ç»ªåˆ†æ: {len(emotions_timeline)}ä¸ªæœ‰æ•ˆç»“æœ")
            logger.info(f"   - å¤„ç†è€—æ—¶: {processing_time:.2f}ç§’")
            
            # è®¡ç®—ç»Ÿè®¡ç‰¹å¾
            analysis_result = self._compute_visual_statistics(
                head_poses, gaze_directions, emotions_timeline
            )
            
            # æ·»åŠ å¤„ç†ç»Ÿè®¡ä¿¡æ¯
            analysis_result.update({
                'processing_stats': {
                    'total_frames': frame_count,
                    'processed_frames': processed_frames,
                    'processing_time_seconds': processing_time,
                    'head_pose_count': len(head_poses),
                    'gaze_direction_count': len(gaze_directions),
                    'emotion_analysis_count': len(emotions_timeline)
                }
            })
            
            logger.info("âœ… è§†é¢‘åˆ†ææˆåŠŸå®Œæˆ")
            return analysis_result
            
        except Exception as e:
            processing_time = (datetime.now() - start_time).total_seconds()
            logger.error(f"âŒ è§†é¢‘åˆ†æå¤±è´¥: {str(e)}")
            logger.error(f"ğŸ”§ é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            logger.error(f"â±ï¸ å¤±è´¥å‰å¤„ç†æ—¶é—´: {processing_time:.2f}ç§’")
            
            fallback_result = self._get_fallback_visual_analysis()
            fallback_result['error_details'] = {
                'error_message': str(e),
                'processing_time': processing_time
            }
            return fallback_result
    
    def _analyze_head_pose(self, face_landmarks, frame_shape) -> Optional[Dict[str, float]]:
        """åˆ†æå¤´éƒ¨å§¿æ€"""
        
        try:
            h, w = frame_shape[:2]
            
            # æå–2Då…³é”®ç‚¹
            landmark_points = []
            for idx in [1, 175, 33, 263, 61, 291]:  # é¼»å°–ã€ä¸‹å·´ã€çœ¼è§’ã€å˜´è§’
                if idx < len(face_landmarks.landmark):
                    lm = face_landmarks.landmark[idx]
                    landmark_points.append([lm.x * w, lm.y * h])
            
            if len(landmark_points) < 6:
                logger.debug("âš ï¸ å¤´éƒ¨å§¿æ€åˆ†æ: å…³é”®ç‚¹æ•°é‡ä¸è¶³")
                return None
            
            image_points = np.array(landmark_points, dtype=np.float64)
            
            # ç›¸æœºå‚æ•° (ä¼°ç®—)
            focal_length = w
            center = (w/2, h/2)
            camera_matrix = np.array([
                [focal_length, 0, center[0]],
                [0, focal_length, center[1]],
                [0, 0, 1]
            ], dtype=np.float64)
            
            dist_coeffs = np.zeros((4,1))
            
            # ä½¿ç”¨PnPç®—æ³•æ±‚è§£å¤´éƒ¨å§¿æ€
            success, rotation_vector, translation_vector = cv2.solvePnP(
                self.model_points, image_points, camera_matrix, dist_coeffs
            )
            
            if success:
                # è½¬æ¢ä¸ºæ¬§æ‹‰è§’
                rotation_matrix, _ = cv2.Rodrigues(rotation_vector)
                angles = self._rotation_matrix_to_euler(rotation_matrix)
                
                return {
                    'pitch': float(angles[0]),  # ä¿¯ä»°è§’
                    'yaw': float(angles[1]),    # å¶èˆªè§’
                    'roll': float(angles[2])    # ç¿»æ»šè§’
                }
            else:
                logger.debug("âš ï¸ å¤´éƒ¨å§¿æ€åˆ†æ: PnPç®—æ³•æ±‚è§£å¤±è´¥")
            
        except Exception as e:
            logger.warning(f"âŒ å¤´éƒ¨å§¿æ€åˆ†æå¤±è´¥: {str(e)}")
            logger.debug(f"ğŸ”§ é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
        
        return None
    
    def _analyze_gaze_direction(self, face_landmarks, frame_shape) -> Optional[Dict[str, float]]:
        """åˆ†æè§†çº¿æ–¹å‘"""
        
        try:
            h, w = frame_shape[:2]
            
            # è·å–è™¹è†œå’Œçœ¼è§’å…³é”®ç‚¹
            left_iris_idx = 468
            right_iris_idx = 473
            left_eye_corner_idx = 33
            right_eye_corner_idx = 263
            
            if (left_iris_idx >= len(face_landmarks.landmark) or 
                right_iris_idx >= len(face_landmarks.landmark)):
                return None
            
            # è®¡ç®—è§†çº¿å‘é‡
            left_iris = face_landmarks.landmark[left_iris_idx]
            right_iris = face_landmarks.landmark[right_iris_idx]
            left_corner = face_landmarks.landmark[left_eye_corner_idx]
            right_corner = face_landmarks.landmark[right_eye_corner_idx]
            
            # è®¡ç®—è§†çº¿åç§»
            left_gaze_x = (left_iris.x - left_corner.x) * w
            left_gaze_y = (left_iris.y - left_corner.y) * h
            right_gaze_x = (right_iris.x - right_corner.x) * w
            right_gaze_y = (right_iris.y - right_corner.y) * h
            
            # å¹³å‡è§†çº¿æ–¹å‘
            avg_gaze_x = (left_gaze_x + right_gaze_x) / 2
            avg_gaze_y = (left_gaze_y + right_gaze_y) / 2
            
            return {
                'gaze_x': float(avg_gaze_x),
                'gaze_y': float(avg_gaze_y),
                'gaze_magnitude': float(np.sqrt(avg_gaze_x**2 + avg_gaze_y**2))
            }
            
        except Exception as e:
            logging.warning(f"è§†çº¿åˆ†æå¤±è´¥: {str(e)}")
        
        return None
    
    def _analyze_emotion(self, frame) -> Optional[Dict[str, float]]:
        """åˆ†ææƒ…ç»ª"""
        
        if not DEEPFACE_AVAILABLE:
            logger.warning(f"ğŸš« DeepFaceä¸å¯ç”¨ï¼Œä½¿ç”¨å¤‡ç”¨æƒ…ç»ªåˆ†æ: {DEEPFACE_ERROR}")
            return self._fallback_emotion_analysis()
        
        try:
            logger.debug("ğŸ˜Š å¼€å§‹DeepFaceæƒ…ç»ªåˆ†æ...")
            
            # æ£€æŸ¥å¸§çš„æœ‰æ•ˆæ€§
            if frame is None or frame.size == 0:
                logger.error("âŒ æƒ…ç»ªåˆ†æå¤±è´¥: è¾“å…¥å¸§ä¸ºç©º")
                return self._fallback_emotion_analysis()
            
            # ä½¿ç”¨DeepFaceåˆ†ææƒ…ç»ª
            result = DeepFace.analyze(
                frame, 
                actions=['emotion'], 
                enforce_detection=False,
                detector_backend=model_config.DEEPFACE_BACKEND,
                silent=True
            )
            
            if isinstance(result, list):
                result = result[0]
            
            emotions = result.get('emotion', {})
            
            if not emotions:
                logger.warning("âš ï¸ DeepFaceè¿”å›ç©ºçš„æƒ…ç»ªç»“æœ")
                return self._fallback_emotion_analysis()
            
            # æ‰¾å‡ºä¸»å¯¼æƒ…ç»ª
            dominant_emotion = max(emotions.items(), key=lambda x: x[1])
            
            logger.debug(f"âœ… æƒ…ç»ªåˆ†ææˆåŠŸ: {dominant_emotion[0]} ({dominant_emotion[1]:.2f}%)")
            
            return {
                'dominant_emotion': dominant_emotion[0],
                'dominant_score': float(dominant_emotion[1]),
                'emotions': {k: float(v) for k, v in emotions.items()}
            }
            
        except Exception as e:
            logger.error(f"âŒ DeepFaceæƒ…ç»ªåˆ†æå¤±è´¥: {str(e)}")
            logger.error(f"ğŸ”§ é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            logger.info("ğŸ”„ åˆ‡æ¢åˆ°å¤‡ç”¨æƒ…ç»ªåˆ†æ")
            return self._fallback_emotion_analysis()
    
    def _rotation_matrix_to_euler(self, rotation_matrix) -> Tuple[float, float, float]:
        """æ—‹è½¬çŸ©é˜µè½¬æ¬§æ‹‰è§’"""
        
        sy = np.sqrt(rotation_matrix[0,0] * rotation_matrix[0,0] + 
                    rotation_matrix[1,0] * rotation_matrix[1,0])
        
        singular = sy < 1e-6
        
        if not singular:
            x = np.arctan2(rotation_matrix[2,1], rotation_matrix[2,2])
            y = np.arctan2(-rotation_matrix[2,0], sy)
            z = np.arctan2(rotation_matrix[1,0], rotation_matrix[0,0])
        else:
            x = np.arctan2(-rotation_matrix[1,2], rotation_matrix[1,1])
            y = np.arctan2(-rotation_matrix[2,0], sy)
            z = 0
        
        return np.degrees([x, y, z])
    
    def _compute_visual_statistics(
        self, 
        head_poses: List[Dict], 
        gaze_directions: List[Dict], 
        emotions_timeline: List[Dict]
    ) -> Dict[str, Any]:
        """è®¡ç®—è§†è§‰åˆ†æç»Ÿè®¡ç‰¹å¾"""
        
        result = {
            'head_pose_stability': 0.8,
            'gaze_stability': 0.7,
            'dominant_emotion': 'neutral',
            'emotion_stability': 0.8,
            'eye_contact_ratio': 0.75
        }
        
        if head_poses:
            # è®¡ç®—å¤´éƒ¨å§¿æ€ç¨³å®šæ€§
            yaw_values = [pose['yaw'] for pose in head_poses]
            pitch_values = [pose['pitch'] for pose in head_poses]
            
            yaw_variance = np.var(yaw_values)
            pitch_variance = np.var(pitch_values)
            
            # ç¨³å®šæ€§ä¸æ–¹å·®æˆåæ¯”
            head_stability = 1.0 / (1.0 + (yaw_variance + pitch_variance) / 100)
            result['head_pose_stability'] = float(head_stability)
        
        if gaze_directions:
            # è®¡ç®—è§†çº¿ç¨³å®šæ€§
            gaze_magnitudes = [gaze['gaze_magnitude'] for gaze in gaze_directions]
            gaze_variance = np.var(gaze_magnitudes)
            
            gaze_stability = 1.0 / (1.0 + gaze_variance / 10)
            result['gaze_stability'] = float(gaze_stability)
            
            # è®¡ç®—çœ¼ç¥æ¥è§¦æ¯”ä¾‹ (è§†çº¿åç§»å°äºé˜ˆå€¼çš„å¸§æ•°æ¯”ä¾‹)
            eye_contact_frames = sum(1 for mag in gaze_magnitudes if mag < 5.0)
            result['eye_contact_ratio'] = eye_contact_frames / len(gaze_magnitudes)
        
        if emotions_timeline:
            # åˆ†ææƒ…ç»ªåˆ†å¸ƒ
            emotion_counts = {}
            for emotion_data in emotions_timeline:
                emotion = emotion_data['dominant_emotion']
                emotion_counts[emotion] = emotion_counts.get(emotion, 0) + 1
            
            # ä¸»å¯¼æƒ…ç»ª
            if emotion_counts:
                result['dominant_emotion'] = max(emotion_counts.items(), key=lambda x: x[1])[0]
            
            # æƒ…ç»ªç¨³å®šæ€§ (è´Ÿé¢æƒ…ç»ªå æ¯”çš„å€’æ•°)
            negative_emotions = ['angry', 'disgust', 'fear', 'sad']
            negative_count = sum(emotion_counts.get(emotion, 0) for emotion in negative_emotions)
            total_count = sum(emotion_counts.values())
            
            if total_count > 0:
                negative_ratio = negative_count / total_count
                result['emotion_stability'] = 1.0 - negative_ratio
        
        return result
    
    def _fallback_emotion_analysis(self) -> Dict[str, Any]:
        """DeepFaceä¸å¯ç”¨æ—¶çš„å¤‡ç”¨æƒ…ç»ªåˆ†æ"""
        logger.debug("ğŸ”„ ä½¿ç”¨å¤‡ç”¨æƒ…ç»ªåˆ†æ (é»˜è®¤å€¼)")
        return {
            'dominant_emotion': 'neutral',
            'dominant_score': 0.7,
            'emotions': {
                'neutral': 0.7,
                'happy': 0.1,
                'sad': 0.05,
                'angry': 0.05,
                'surprise': 0.05,
                'disgust': 0.025,
                'fear': 0.025
            },
            'fallback': True,
            'reason': 'DeepFaceä¸å¯ç”¨æˆ–åˆ†æå¤±è´¥'
        }
    
    def _get_fallback_visual_analysis(self) -> Dict[str, Any]:
        """è§†é¢‘åˆ†æå¤±è´¥æ—¶çš„å¤‡ç”¨ç»“æœ"""
        return {
            'head_pose_stability': 0.7,
            'gaze_stability': 0.7,
            'dominant_emotion': 'neutral',
            'emotion_stability': 0.8,
            'eye_contact_ratio': 0.6,
            'error': True,
            'message': 'è§†é¢‘åˆ†æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼'
        }


class AudioAnalyzer:
    """éŸ³é¢‘åˆ†æå™¨ - å¤„ç†å¬è§‰æ¨¡æ€"""
    
    def __init__(self):
        self.sample_rate = model_config.AUDIO_SAMPLE_RATE
    
    def analyze_audio(self, audio_path: str) -> Dict[str, Any]:
        """åˆ†æéŸ³é¢‘æ–‡ä»¶ï¼Œæå–å¬è§‰ç‰¹å¾"""
        
        start_time = datetime.now()
        logger.info(f"ğŸµ å¼€å§‹éŸ³é¢‘åˆ†æ: {audio_path}")
        
        try:
            # æ£€æŸ¥éŸ³é¢‘æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not Path(audio_path).exists():
                raise FileNotFoundError(f"éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {audio_path}")
            
            # æ£€æŸ¥æ–‡ä»¶å¤§å°
            file_size = Path(audio_path).stat().st_size
            if file_size == 0:
                raise Exception("éŸ³é¢‘æ–‡ä»¶ä¸ºç©º")
            
            logger.info(f"ğŸ“Š éŸ³é¢‘æ–‡ä»¶å¤§å°: {file_size / 1024:.1f} KB")
            
            # åŠ è½½éŸ³é¢‘æ–‡ä»¶
            logger.debug("ğŸ”„ åŠ è½½éŸ³é¢‘æ–‡ä»¶...")
            y, sr = librosa.load(audio_path, sr=self.sample_rate)
            
            if len(y) == 0:
                raise Exception("éŸ³é¢‘æ•°æ®ä¸ºç©º")
            
            duration = len(y) / sr
            logger.info(f"ğŸ“Š éŸ³é¢‘ä¿¡æ¯: é‡‡æ ·ç‡{sr}Hz, æ—¶é•¿{duration:.2f}ç§’, {len(y)}ä¸ªé‡‡æ ·ç‚¹")
            
            # è¯­é€Ÿåˆ†æ
            logger.debug("ğŸ—£ï¸ åˆ†æè¯­é€Ÿ...")
            speech_rate = self._analyze_speech_rate(y, sr)
            logger.debug(f"âœ… è¯­é€Ÿåˆ†æå®Œæˆ: {speech_rate:.1f} BPM")
            
            # éŸ³é«˜åˆ†æ
            logger.debug("ğŸ¼ åˆ†æéŸ³é«˜...")
            pitch_analysis = self._analyze_pitch(y, sr)
            logger.debug(f"âœ… éŸ³é«˜åˆ†æå®Œæˆ: å¹³å‡{pitch_analysis['mean']:.1f}Hz")
            
            # éŸ³é‡åˆ†æ
            logger.debug("ğŸ“¢ åˆ†æéŸ³é‡...")
            volume_analysis = self._analyze_volume(y)
            logger.debug(f"âœ… éŸ³é‡åˆ†æå®Œæˆ: å¹³å‡{volume_analysis['mean']:.1f}dB")
            
            # è¯­éŸ³æ¸…æ™°åº¦åˆ†æ
            logger.debug("ğŸ¯ åˆ†ææ¸…æ™°åº¦...")
            clarity_score = self._analyze_clarity(y, sr)
            logger.debug(f"âœ… æ¸…æ™°åº¦åˆ†æå®Œæˆ: {clarity_score:.2f}")
            
            processing_time = (datetime.now() - start_time).total_seconds()
            
            result = {
                'speech_rate_bpm': speech_rate,
                'pitch_mean': pitch_analysis['mean'],
                'pitch_variance': pitch_analysis['variance'],
                'pitch_range': pitch_analysis['range'],
                'volume_mean': volume_analysis['mean'],
                'volume_variance': volume_analysis['variance'],
                'clarity_score': clarity_score,
                'audio_duration': duration,
                'processing_stats': {
                    'processing_time_seconds': processing_time,
                    'sample_rate': sr,
                    'samples_count': len(y),
                    'file_size_bytes': file_size
                }
            }
            
            logger.info(f"âœ… éŸ³é¢‘åˆ†ææˆåŠŸå®Œæˆ (è€—æ—¶: {processing_time:.2f}ç§’)")
            return result
            
        except Exception as e:
            processing_time = (datetime.now() - start_time).total_seconds()
            logger.error(f"âŒ éŸ³é¢‘åˆ†æå¤±è´¥: {str(e)}")
            logger.error(f"ğŸ”§ é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            logger.error(f"â±ï¸ å¤±è´¥å‰å¤„ç†æ—¶é—´: {processing_time:.2f}ç§’")
            
            fallback_result = self._get_fallback_audio_analysis()
            fallback_result['error_details'] = {
                'error_message': str(e),
                'processing_time': processing_time
            }
            return fallback_result
    
    def _analyze_speech_rate(self, y: np.ndarray, sr: int) -> float:
        """åˆ†æè¯­é€Ÿ"""
        
        try:
            # ä½¿ç”¨èŠ‚æ‹è¿½è¸ªä¼°ç®—è¯­é€Ÿ
            tempo, beats = librosa.beat.beat_track(y=y, sr=sr)
            
            # å°†éŸ³ä¹èŠ‚æ‹è½¬æ¢ä¸ºè¯­éŸ³èŠ‚å¥çš„ä¼°ç®—
            # è¯­éŸ³çš„"èŠ‚æ‹"é€šå¸¸æ¯”éŸ³ä¹æ…¢ï¼Œéœ€è¦è°ƒæ•´
            speech_tempo = tempo * 0.6  # ç»éªŒè°ƒæ•´å› å­
            
            return float(speech_tempo)
            
        except Exception as e:
            logging.warning(f"è¯­é€Ÿåˆ†æå¤±è´¥: {str(e)}")
            return 120.0  # é»˜è®¤è¯­é€Ÿ
    
    def _analyze_pitch(self, y: np.ndarray, sr: int) -> Dict[str, float]:
        """åˆ†æéŸ³é«˜ç‰¹å¾"""
        
        try:
            # ä½¿ç”¨pyinç®—æ³•æå–åŸºé¢‘
            f0, voiced_flag, voiced_probs = librosa.pyin(
                y, fmin=librosa.note_to_hz('C2'), fmax=librosa.note_to_hz('C7')
            )
            
            # åªä¿ç•™æœ‰æ•ˆçš„éŸ³é«˜å€¼
            valid_f0 = f0[voiced_flag]
            
            if len(valid_f0) > 0:
                pitch_mean = float(np.mean(valid_f0))
                pitch_variance = float(np.var(valid_f0))
                pitch_range = float(np.max(valid_f0) - np.min(valid_f0))
            else:
                pitch_mean = 150.0  # é»˜è®¤éŸ³é«˜
                pitch_variance = 20.0
                pitch_range = 50.0
            
            return {
                'mean': pitch_mean,
                'variance': pitch_variance,
                'range': pitch_range
            }
            
        except Exception as e:
            logging.warning(f"éŸ³é«˜åˆ†æå¤±è´¥: {str(e)}")
            return {
                'mean': 150.0,
                'variance': 20.0,
                'range': 50.0
            }
    
    def _analyze_volume(self, y: np.ndarray) -> Dict[str, float]:
        """åˆ†æéŸ³é‡ç‰¹å¾"""
        
        try:
            # è®¡ç®—RMSèƒ½é‡
            rms = librosa.feature.rms(y=y)[0]
            
            # è½¬æ¢ä¸ºåˆ†è´
            db = librosa.amplitude_to_db(rms)
            
            volume_mean = float(np.mean(db))
            volume_variance = float(np.var(db))
            
            return {
                'mean': volume_mean,
                'variance': volume_variance
            }
            
        except Exception as e:
            logging.warning(f"éŸ³é‡åˆ†æå¤±è´¥: {str(e)}")
            return {
                'mean': -20.0,
                'variance': 5.0
            }
    
    def _analyze_clarity(self, y: np.ndarray, sr: int) -> float:
        """åˆ†æè¯­éŸ³æ¸…æ™°åº¦"""
        
        try:
            # ä½¿ç”¨é¢‘è°±è´¨å¿ƒä½œä¸ºæ¸…æ™°åº¦çš„ä»£ç†æŒ‡æ ‡
            spectral_centroids = librosa.feature.spectral_centroid(y=y, sr=sr)[0]
            
            # è®¡ç®—æ¸…æ™°åº¦åˆ†æ•° (é¢‘è°±è´¨å¿ƒçš„ç¨³å®šæ€§)
            centroid_variance = np.var(spectral_centroids)
            clarity_score = 1.0 / (1.0 + centroid_variance / 1000000)
            
            return float(clarity_score)
            
        except Exception as e:
            logging.warning(f"æ¸…æ™°åº¦åˆ†æå¤±è´¥: {str(e)}")
            return 0.8  # é»˜è®¤æ¸…æ™°åº¦
    
    def _get_fallback_audio_analysis(self) -> Dict[str, Any]:
        """éŸ³é¢‘åˆ†æå¤±è´¥æ—¶çš„å¤‡ç”¨ç»“æœ"""
        return {
            'speech_rate_bpm': 120.0,
            'pitch_mean': 150.0,
            'pitch_variance': 20.0,
            'pitch_range': 50.0,
            'volume_mean': -20.0,
            'volume_variance': 5.0,
            'clarity_score': 0.8,
            'audio_duration': 60.0,
            'error': True,
            'message': 'éŸ³é¢‘åˆ†æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼'
        }


class MultimodalAnalyzer:
    """å¤šæ¨¡æ€åˆ†æå™¨ - æ•´åˆè§†è§‰å’Œå¬è§‰åˆ†æ"""
    
    def __init__(self):
        self.video_analyzer = VideoAnalyzer()
        self.audio_analyzer = AudioAnalyzer()
    
    def analyze_interview_media(
        self, 
        video_path: Optional[str] = None, 
        audio_path: Optional[str] = None
    ) -> Dict[str, Any]:
        """åˆ†æé¢è¯•çš„éŸ³è§†é¢‘æ•°æ®"""
        
        start_time = datetime.now()
        logger.info("ğŸš€ å¼€å§‹å¤šæ¨¡æ€é¢è¯•åˆ†æ")
        logger.info(f"ğŸ“ è§†é¢‘æ–‡ä»¶: {video_path if video_path else 'æœªæä¾›'}")
        logger.info(f"ğŸ“ éŸ³é¢‘æ–‡ä»¶: {audio_path if audio_path else 'æœªæä¾›'}")
        
        result = {
            'visual_analysis': None,
            'audio_analysis': None,
            'analysis_timestamp': start_time.isoformat(),
            'processing_summary': {
                'video_success': False,
                'audio_success': False,
                'errors': []
            }
        }
        
        # DeepFaceå¯ç”¨æ€§æ£€æŸ¥
        if not DEEPFACE_AVAILABLE:
            logger.warning(f"âš ï¸ DeepFaceä¸å¯ç”¨ï¼Œæƒ…ç»ªåˆ†æå°†ä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ: {DEEPFACE_ERROR}")
            result['processing_summary']['errors'].append(f"DeepFaceä¸å¯ç”¨: {DEEPFACE_ERROR}")
        
        # è§†è§‰åˆ†æ
        logger.info("=" * 50)
        logger.info("ğŸ¥ å¼€å§‹è§†è§‰åˆ†æéƒ¨åˆ†")
        
        if video_path:
            video_file = Path(video_path)
            if video_file.exists() and video_file.stat().st_size > 0:
                logger.info(f"ğŸ“‚ è§†é¢‘æ–‡ä»¶æœ‰æ•ˆ: {video_file.stat().st_size / (1024*1024):.1f} MB")
                try:
                    result['visual_analysis'] = self.video_analyzer.analyze_video(video_path)
                    result['processing_summary']['video_success'] = True
                    logger.info("âœ… è§†é¢‘åˆ†æéƒ¨åˆ†å®Œæˆ")
                except Exception as e:
                    error_msg = f"è§†é¢‘åˆ†æå¤±è´¥: {str(e)}"
                    logger.error(f"âŒ {error_msg}")
                    logger.error(f"ğŸ”§ é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
                    result['visual_analysis'] = self.video_analyzer._get_fallback_visual_analysis()
                    result['processing_summary']['errors'].append(error_msg)
            else:
                error_msg = f"è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨æˆ–ä¸ºç©º: {video_path}"
                logger.warning(f"âš ï¸ {error_msg}")
                result['visual_analysis'] = self.video_analyzer._get_fallback_visual_analysis()
                result['processing_summary']['errors'].append(error_msg)
        else:
            logger.info("â„¹ï¸ æœªæä¾›è§†é¢‘æ–‡ä»¶è·¯å¾„ï¼Œä½¿ç”¨é»˜è®¤è§†è§‰åˆ†æ")
            result['visual_analysis'] = self.video_analyzer._get_fallback_visual_analysis()
        
        # å¬è§‰åˆ†æ
        logger.info("=" * 50)
        logger.info("ğŸµ å¼€å§‹å¬è§‰åˆ†æéƒ¨åˆ†")
        
        if audio_path:
            audio_file = Path(audio_path)
            if audio_file.exists() and audio_file.stat().st_size > 0:
                logger.info(f"ğŸ“‚ éŸ³é¢‘æ–‡ä»¶æœ‰æ•ˆ: {audio_file.stat().st_size / 1024:.1f} KB")
                try:
                    result['audio_analysis'] = self.audio_analyzer.analyze_audio(audio_path)
                    result['processing_summary']['audio_success'] = True
                    logger.info("âœ… éŸ³é¢‘åˆ†æéƒ¨åˆ†å®Œæˆ")
                except Exception as e:
                    error_msg = f"éŸ³é¢‘åˆ†æå¤±è´¥: {str(e)}"
                    logger.error(f"âŒ {error_msg}")
                    logger.error(f"ğŸ”§ é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
                    result['audio_analysis'] = self.audio_analyzer._get_fallback_audio_analysis()
                    result['processing_summary']['errors'].append(error_msg)
            else:
                error_msg = f"éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨æˆ–ä¸ºç©º: {audio_path}"
                logger.warning(f"âš ï¸ {error_msg}")
                result['audio_analysis'] = self.audio_analyzer._get_fallback_audio_analysis()
                result['processing_summary']['errors'].append(error_msg)
        else:
            logger.info("â„¹ï¸ æœªæä¾›éŸ³é¢‘æ–‡ä»¶è·¯å¾„ï¼Œä½¿ç”¨é»˜è®¤å¬è§‰åˆ†æ")
            result['audio_analysis'] = self.audio_analyzer._get_fallback_audio_analysis()
        
        # åˆ†ææ€»ç»“
        total_processing_time = (datetime.now() - start_time).total_seconds()
        result['processing_summary']['total_time_seconds'] = total_processing_time
        
        logger.info("=" * 50)
        logger.info("ğŸ“Š å¤šæ¨¡æ€åˆ†ææ€»ç»“:")
        logger.info(f"   ğŸ¥ è§†é¢‘åˆ†æ: {'âœ… æˆåŠŸ' if result['processing_summary']['video_success'] else 'âŒ å¤±è´¥/è·³è¿‡'}")
        logger.info(f"   ğŸµ éŸ³é¢‘åˆ†æ: {'âœ… æˆåŠŸ' if result['processing_summary']['audio_success'] else 'âŒ å¤±è´¥/è·³è¿‡'}")
        logger.info(f"   â±ï¸ æ€»å¤„ç†æ—¶é—´: {total_processing_time:.2f}ç§’")
        
        if result['processing_summary']['errors']:
            logger.warning(f"   âš ï¸ é”™è¯¯æ•°é‡: {len(result['processing_summary']['errors'])}")
            for i, error in enumerate(result['processing_summary']['errors'], 1):
                logger.warning(f"     {i}. {error}")
        else:
            logger.info("   ğŸ‰ æ‰€æœ‰åˆ†æå‡æˆåŠŸå®Œæˆ!")
        
        logger.info("ğŸ å¤šæ¨¡æ€é¢è¯•åˆ†æå®Œæˆ")
        return result


def create_multimodal_analyzer() -> MultimodalAnalyzer:
    """åˆ›å»ºå¤šæ¨¡æ€åˆ†æå™¨å®ä¾‹"""
    return MultimodalAnalyzer() 