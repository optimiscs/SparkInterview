"""
Â§öÊ®°ÊÄÅÂàÜÊûêÂ∑•ÂÖ∑
ÈõÜÊàêMediaPipe„ÄÅDeepFace„ÄÅLibrosaÁ≠âÂ∫ìËøõË°åÁúüÂÆûÁöÑÈü≥ËßÜÈ¢ëÂàÜÊûê
"""
import cv2
import numpy as np
import librosa
import mediapipe as mp
from typing import Dict, Any, List, Optional, Tuple
import logging
from pathlib import Path
import sys
import traceback
from datetime import datetime
import os
import json

# ÈÖçÁΩÆËØ¶ÁªÜÊó•Âøó
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# DeepFaceÂèØÁî®ÊÄßÊ£ÄÊü•ÂíåÂàùÂßãÂåñ
DEEPFACE_AVAILABLE = False
DEEPFACE_ERROR = None

# DeepFaceÂØºÂÖ•
DeepFace = None

def check_deepface_availability():
    """Ê£ÄÊü•DeepFaceÊòØÂê¶ÂèØÁî®Âπ∂ËøõË°åÂàùÂßãÂåñÊµãËØï"""
    global DEEPFACE_AVAILABLE, DEEPFACE_ERROR, DeepFace
    
    try:
        logger.info("üîç Ê£ÄÊü•DeepFaceÂèØÁî®ÊÄß...")
        from deepface import DeepFace
        
        # Â∞ùËØïÂàùÂßãÂåñDeepFaceËøõË°åÁÆÄÂçïÊµãËØï
        logger.info("üß™ ÊµãËØïDeepFaceÂäüËÉΩ...")
        
        # ÂàõÂª∫‰∏Ä‰∏™ÊµãËØïÂõæÂÉè
        test_image = np.zeros((100, 100, 3), dtype=np.uint8)
        test_image[30:70, 30:70] = [255, 255, 255]  # ÁôΩËâ≤ÊñπÂùóÊ®°Êãü‰∫∫ËÑ∏
        
        # Â∞ùËØïÂàÜÊûêÊµãËØïÂõæÂÉè
        result = DeepFace.analyze(
            test_image, 
            actions=['emotion'], 
            enforce_detection=False,
            silent=True
        )
        
        DEEPFACE_AVAILABLE = True
        logger.info("‚úÖ DeepFaceÂàùÂßãÂåñÊàêÂäüÔºåÊÉÖÊÑüÂàÜÊûêÂäüËÉΩÂèØÁî®")
        return True
        
    except ImportError as e:
        DEEPFACE_ERROR = f"DeepFaceÊú™ÂÆâË£Ö: {str(e)}"
        logger.error(f"‚ùå {DEEPFACE_ERROR}")
        logger.error("üí° ËØ∑ÂÆâË£ÖDeepFace: pip install deepface")
        return False
    except Exception as e:
        DEEPFACE_ERROR = f"DeepFaceÂàùÂßãÂåñÂ§±Ë¥•: {str(e)}"
        logger.error(f"‚ùå {DEEPFACE_ERROR}")
        logger.error(f"üîß ÈîôËØØËØ¶ÊÉÖ: {traceback.format_exc()}")
        return False

# ÂêØÂä®Êó∂Ê£ÄÊü•DeepFace
check_deepface_availability()

try:
    from ..config.settings import model_config
except ImportError:
    logger.warning("‚ö†Ô∏è Êó†Ê≥ïÂØºÂÖ•model_configÔºå‰ΩøÁî®ÈªòËÆ§ÈÖçÁΩÆ")
    # ÈªòËÆ§ÈÖçÁΩÆ
    class DefaultConfig:
        MEDIAPIPE_CONFIDENCE = 0.5
        DEEPFACE_BACKEND = 'opencv'
        AUDIO_SAMPLE_RATE = 22050
    model_config = DefaultConfig()


class VideoAnalyzer:
    """ËßÜÈ¢ëÂàÜÊûêÂô® - Â§ÑÁêÜËßÜËßâÊ®°ÊÄÅ"""
    
    def __init__(self):
        # ÂàùÂßãÂåñMediaPipe Face Mesh
        self.mp_face_mesh = mp.solutions.face_mesh
        self.mp_drawing = mp.solutions.drawing_utils
        self.face_mesh = self.mp_face_mesh.FaceMesh(
            static_image_mode=False,
            max_num_faces=1,
            refine_landmarks=True,
            min_detection_confidence=model_config.MEDIAPIPE_CONFIDENCE,
            min_tracking_confidence=model_config.MEDIAPIPE_CONFIDENCE
        )
        
        # ÂàùÂßãÂåñMediaPipe Pose
        self.mp_pose = mp.solutions.pose
        self.pose = self.mp_pose.Pose(
            static_image_mode=False,
            model_complexity=1,
            enable_segmentation=False,
            min_detection_confidence=model_config.MEDIAPIPE_CONFIDENCE,
            min_tracking_confidence=model_config.MEDIAPIPE_CONFIDENCE
        )
        
        # ÂàùÂßãÂåñMediaPipe Hands
        self.mp_hands = mp.solutions.hands
        self.hands = self.mp_hands.Hands(
            static_image_mode=False,
            max_num_hands=2,
            model_complexity=1,
            min_detection_confidence=model_config.MEDIAPIPE_CONFIDENCE,
            min_tracking_confidence=model_config.MEDIAPIPE_CONFIDENCE
        )
        
        # Èù¢ÈÉ®ÂÖ≥ÈîÆÁÇπÁ¥¢Âºï (468‰∏™ÂÖ≥ÈîÆÁÇπ‰∏≠ÁöÑÈáçË¶ÅÁÇπ)
        self.face_landmarks_indexes = {
            'nose_tip': 1,
            'chin': 175,
            'left_eye_corner': 33,
            'right_eye_corner': 263,
            'left_iris': 468,
            'right_iris': 473
        }
        
        # 3DÈù¢ÈÉ®Ê®°ÂûãÁÇπ (Áî®‰∫éPnPÁÆóÊ≥ï)
        self.model_points = np.array([
            (0.0, 0.0, 0.0),      # ÈºªÂ∞ñ
            (0.0, -330.0, -65.0), # ‰∏ãÂ∑¥
            (-225.0, 170.0, -135.0), # Â∑¶ÁúºËßí
            (225.0, 170.0, -135.0),  # Âè≥ÁúºËßí
            (-150.0, -150.0, -125.0), # Â∑¶Âò¥Ëßí
            (150.0, -150.0, -125.0)   # Âè≥Âò¥Ëßí
        ], dtype=np.float64)
        
        # ÂàõÂª∫‰øùÂ≠òÁõÆÂΩï
        self.save_dir = Path("data/analysis_frames")
        self.save_dir.mkdir(parents=True, exist_ok=True)
        logger.info(f"üìÅ ÂàÜÊûêÂ∏ß‰øùÂ≠òÁõÆÂΩï: {self.save_dir.absolute()}")
    
    def _save_analysis_frame(self, frame: np.ndarray, frame_count: int, timestamp: float, 
                           analysis_result: Dict[str, Any], frame_type: str = "analysis") -> str:
        """‰øùÂ≠òÂàÜÊûêÂ∏ßÂà∞Êú¨Âú∞"""
        
        try:
            # ÁîüÊàêÊó∂Èó¥Êà≥
            timestamp_str = datetime.now().strftime("%Y%m%d_%H%M%S_%f")[:-3]
            
            # ÊûÑÂª∫Êñá‰ª∂Âêç
            filename = f"{timestamp_str}_frame_{frame_count:06d}_{frame_type}.jpg"
            filepath = self.save_dir / filename
            
            # Âú®ÂõæÂÉè‰∏äÁªòÂà∂ÂàÜÊûêÁªìÊûú‰ø°ÊÅØ
            annotated_frame = frame.copy()
            
            # Ê∑ªÂä†ÊñáÊú¨‰ø°ÊÅØ
            font = cv2.FONT_HERSHEY_SIMPLEX
            font_scale = 0.6
            color = (255, 255, 255)  # ÁôΩËâ≤
            thickness = 2
            
            # Âü∫Êú¨‰ø°ÊÅØ
            y_offset = 30
            cv2.putText(annotated_frame, f"Frame: {frame_count}", (10, y_offset), 
                       font, font_scale, color, thickness)
            y_offset += 25
            cv2.putText(annotated_frame, f"Time: {timestamp:.2f}s", (10, y_offset), 
                       font, font_scale, color, thickness)
            y_offset += 25
            
            # ÂàÜÊûêÁªìÊûú‰ø°ÊÅØ
            if 'head_pose' in analysis_result:
                pose = analysis_result['head_pose']
                cv2.putText(annotated_frame, f"Pitch: {pose.get('pitch', 0):.1f}¬∞", (10, y_offset), 
                           font, font_scale, color, thickness)
                y_offset += 25
                cv2.putText(annotated_frame, f"Yaw: {pose.get('yaw', 0):.1f}¬∞", (10, y_offset), 
                           font, font_scale, color, thickness)
                y_offset += 25
                cv2.putText(annotated_frame, f"Roll: {pose.get('roll', 0):.1f}¬∞", (10, y_offset), 
                           font, font_scale, color, thickness)
                y_offset += 25
            
            if 'gaze' in analysis_result:
                gaze = analysis_result['gaze']
                cv2.putText(annotated_frame, f"Gaze: ({gaze.get('gaze_x', 0):.1f}, {gaze.get('gaze_y', 0):.1f})", 
                           (10, y_offset), font, font_scale, color, thickness)
                y_offset += 25
            
            if 'emotion' in analysis_result:
                emotion = analysis_result['emotion']
                cv2.putText(annotated_frame, f"Emotion: {emotion.get('dominant_emotion', 'Unknown')}", 
                           (10, y_offset), font, font_scale, color, thickness)
                y_offset += 25
                cv2.putText(annotated_frame, f"Score: {emotion.get('dominant_score', 0):.1f}%", 
                           (10, y_offset), font, font_scale, color, thickness)
            
            # ‰øùÂ≠òÂõæÂÉè
            cv2.imwrite(str(filepath), annotated_frame)
            
            # ‰øùÂ≠òÂàÜÊûêÁªìÊûúJSON
            json_filename = f"{timestamp_str}_frame_{frame_count:06d}_{frame_type}_analysis.json"
            json_filepath = self.save_dir / json_filename
            with open(json_filepath, 'w', encoding='utf-8') as f:
                json.dump({
                    'frame_count': frame_count,
                    'timestamp': timestamp,
                    'frame_type': frame_type,
                    'analysis_result': analysis_result,
                    'saved_at': datetime.now().isoformat()
                }, f, ensure_ascii=False, indent=2)
            
            logger.debug(f"üíæ ‰øùÂ≠òÂàÜÊûêÂ∏ß: {filename}")
            return str(filepath)
            
        except Exception as e:
            logger.error(f"‚ùå ‰øùÂ≠òÂàÜÊûêÂ∏ßÂ§±Ë¥•: {str(e)}")
            return ""
    
    def analyze_video(self, video_path: str) -> Dict[str, Any]:
        """ÂàÜÊûêËßÜÈ¢ëÊñá‰ª∂ÔºåÊèêÂèñËßÜËßâÁâπÂæÅ"""
        
        start_time = datetime.now()
        logger.info(f"üé• ÂºÄÂßãËßÜÈ¢ëÂàÜÊûê: {video_path}")
        
        # Ê£ÄÊü•ËßÜÈ¢ëÊñá‰ª∂ÊòØÂê¶Â≠òÂú®
        if not Path(video_path).exists():
            error_msg = f"ËßÜÈ¢ëÊñá‰ª∂‰∏çÂ≠òÂú®: {video_path}"
            logger.error(f"‚ùå {error_msg}")
            raise FileNotFoundError(error_msg)
        
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            error_msg = f"Êó†Ê≥ïÊâìÂºÄËßÜÈ¢ëÊñá‰ª∂: {video_path}"
            logger.error(f"‚ùå {error_msg}")
            raise Exception(error_msg)
        
        try:
            # Ëé∑ÂèñËßÜÈ¢ëÂü∫Êú¨‰ø°ÊÅØ
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            fps = cap.get(cv2.CAP_PROP_FPS)
            duration = total_frames / fps if fps > 0 else 0
            
            logger.info(f"üìä ËßÜÈ¢ë‰ø°ÊÅØ: {total_frames}Â∏ß, {fps:.1f}FPS, {duration:.1f}Áßí")
            
            # ÂàùÂßãÂåñÂàÜÊûêÁªìÊûúÂ≠òÂÇ®
            head_poses = []
            gaze_directions = []
            body_language_results = []
            gesture_results = []
            emotions_timeline = []
            frame_count = 0
            processed_frames = 0
            saved_frames = []
            
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                
                frame_count += 1
                timestamp = frame_count / fps if fps > 0 else 0
                
                try:
                    # ËΩ¨Êç¢È¢úËâ≤Á©∫Èó¥
                    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                    
                    # MediaPipeÈù¢ÈÉ®Ê£ÄÊµã
                    results = self.face_mesh.process(rgb_frame)
                    
                    if results.multi_face_landmarks:
                        processed_frames += 1
                        face_landmarks = results.multi_face_landmarks[0]
                        
                        # ÂàÜÊûêÂ§¥ÈÉ®ÂßøÊÄÅ
                        head_pose = self._analyze_head_pose(face_landmarks, frame.shape)
                        if head_pose:
                            head_poses.append(head_pose)
                        
                        # ÂàÜÊûêËßÜÁ∫øÊñπÂêë
                        gaze = self._analyze_gaze_direction(face_landmarks, frame.shape)
                        if gaze:
                            gaze_directions.append(gaze)
                        
                        # ÊÉÖÁª™ÂàÜÊûê (ÊØè10Â∏ßÂàÜÊûê‰∏ÄÊ¨°‰ª•ÊèêÈ´òÊïàÁéá)
                        emotion = None
                        if frame_count % 10 == 0:
                            emotion = self._analyze_emotion(frame)
                            if emotion:
                                emotions_timeline.append({
                                    'frame': frame_count,
                                    'timestamp': timestamp,
                                    **emotion
                                })
                        
                        # ‰ΩìÊÄÅËØ≠Ë®ÄÂàÜÊûê
                        body_language = self._analyze_body_language(frame)
                        if body_language:
                            body_language_results.append({
                                'frame': frame_count,
                                'timestamp': timestamp,
                                **body_language
                            })
                        
                        # ÊâãÂäøÂàÜÊûê
                        gestures = self._analyze_gestures(frame)
                        if gestures:
                            gesture_results.append({
                                'frame': frame_count,
                                'timestamp': timestamp,
                                **gestures
                            })
                        
                        # ‰øùÂ≠òÂÖ≥ÈîÆÂàÜÊûêÂ∏ß (ÊØè30Â∏ß‰øùÂ≠ò‰∏ÄÊ¨°ÔºåÊàñËÄÖÊúâÈáçË¶ÅÂàÜÊûêÁªìÊûúÊó∂)
                        should_save = (frame_count % 30 == 0 or 
                                     (emotion and emotion.get('dominant_score', 0) > 70) or
                                     (head_pose and abs(head_pose.get('yaw', 0)) > 15))
                        
                        if should_save:
                            analysis_result = {
                                'head_pose': head_pose,
                                'gaze': gaze,
                                'emotion': emotion,
                                'body_language': body_language,
                                'gestures': gestures
                            }
                            
                            saved_path = self._save_analysis_frame(
                                frame, frame_count, timestamp, analysis_result, "key"
                            )
                            if saved_path:
                                saved_frames.append({
                                    'frame_count': frame_count,
                                    'timestamp': timestamp,
                                    'filepath': saved_path,
                                    'analysis_result': analysis_result
                                })
                    
                    # ËøõÂ∫¶Êó•Âøó (ÊØè100Â∏ßÊä•Âëä‰∏ÄÊ¨°)
                    if frame_count % 100 == 0:
                        progress = (frame_count / total_frames) * 100 if total_frames > 0 else 0
                        logger.debug(f"üìà ËßÜÈ¢ëÂàÜÊûêËøõÂ∫¶: {progress:.1f}% ({frame_count}/{total_frames})")
                        
                except Exception as frame_error:
                    logger.error(f"‚ùå Á¨¨{frame_count}Â∏ßÂàÜÊûêÂ§±Ë¥•: {str(frame_error)}")
                    logger.error(f"üîß ÈîôËØØËØ¶ÊÉÖ: {traceback.format_exc()}")
                    # ‰∏çÂÜç‰ΩøÁî®Â§áÁî®ÊñπÊ°àÔºåÁªßÁª≠Â§ÑÁêÜ‰∏ã‰∏ÄÂ∏ß
                    continue
            
            cap.release()
            
            # Ê£ÄÊü•ÊòØÂê¶ÊúâÊúâÊïàÁöÑÂàÜÊûêÁªìÊûú
            if processed_frames == 0:
                error_msg = "ËßÜÈ¢ë‰∏≠Êú™Ê£ÄÊµãÂà∞‰ªª‰ΩïÊúâÊïàÁöÑÈù¢ÈÉ®Êï∞ÊçÆ"
                logger.error(f"‚ùå {error_msg}")
                raise Exception(error_msg)
            
            # ÂàÜÊûêÁªìÊûúÁªüËÆ°
            processing_time = (datetime.now() - start_time).total_seconds()
            logger.info(f"üìä ËßÜÈ¢ëÂàÜÊûêÂÆåÊàê:")
            logger.info(f"   - ÊÄªÂ∏ßÊï∞: {frame_count}")
            logger.info(f"   - Ê£ÄÊµãÂà∞‰∫∫ËÑ∏ÁöÑÂ∏ßÊï∞: {processed_frames}")
            logger.info(f"   - Â§¥ÈÉ®ÂßøÊÄÅÂàÜÊûê: {len(head_poses)}‰∏™ÊúâÊïàÁªìÊûú")
            logger.info(f"   - ËßÜÁ∫øÊñπÂêëÂàÜÊûê: {len(gaze_directions)}‰∏™ÊúâÊïàÁªìÊûú") 
            logger.info(f"   - ÊÉÖÁª™ÂàÜÊûê: {len(emotions_timeline)}‰∏™ÊúâÊïàÁªìÊûú")
            logger.info(f"   - ‰ΩìÊÄÅËØ≠Ë®ÄÂàÜÊûê: {len(body_language_results)}‰∏™ÊúâÊïàÁªìÊûú")
            logger.info(f"   - ÊâãÂäøÂàÜÊûê: {len(gesture_results)}‰∏™ÊúâÊïàÁªìÊûú")
            logger.info(f"   - ‰øùÂ≠òÁöÑÂÖ≥ÈîÆÂ∏ß: {len(saved_frames)}‰∏™")
            logger.info(f"   - Â§ÑÁêÜËÄóÊó∂: {processing_time:.2f}Áßí")
            
            # ËÆ°ÁÆóÁªüËÆ°ÁâπÂæÅ
            analysis_result = self._compute_visual_statistics(
                head_poses, gaze_directions, emotions_timeline, body_language_results, gesture_results
            )
            
            # Ê∑ªÂä†Â§ÑÁêÜÁªüËÆ°‰ø°ÊÅØ
            analysis_result.update({
                'processing_stats': {
                    'total_frames': frame_count,
                    'processed_frames': processed_frames,
                    'processing_time_seconds': processing_time,
                    'head_pose_count': len(head_poses),
                    'gaze_direction_count': len(gaze_directions),
                    'emotion_analysis_count': len(emotions_timeline),
                    'body_language_count': len(body_language_results),
                    'gesture_count': len(gesture_results),
                    'saved_frames_count': len(saved_frames),
                    'save_directory': str(self.save_dir.absolute())
                },
                'saved_frames': saved_frames
            })
            
            logger.info("‚úÖ ËßÜÈ¢ëÂàÜÊûêÊàêÂäüÂÆåÊàê")
            return analysis_result
            
        except Exception as e:
            cap.release()
            processing_time = (datetime.now() - start_time).total_seconds()
            logger.error(f"‚ùå ËßÜÈ¢ëÂàÜÊûêÂ§±Ë¥•: {str(e)}")
            logger.error(f"üîß ÈîôËØØËØ¶ÊÉÖ: {traceback.format_exc()}")
            logger.error(f"‚è±Ô∏è Â§±Ë¥•ÂâçÂ§ÑÁêÜÊó∂Èó¥: {processing_time:.2f}Áßí")
            raise
    
    def _analyze_head_pose(self, face_landmarks, frame_shape) -> Optional[Dict[str, float]]:
        """ÂàÜÊûêÂ§¥ÈÉ®ÂßøÊÄÅ"""
        
        try:
            h, w = frame_shape[:2]
            
            # ÊèêÂèñ2DÂÖ≥ÈîÆÁÇπ
            landmark_points = []
            for idx in [1, 175, 33, 263, 61, 291]:  # ÈºªÂ∞ñ„ÄÅ‰∏ãÂ∑¥„ÄÅÁúºËßí„ÄÅÂò¥Ëßí
                if idx < len(face_landmarks.landmark):
                    lm = face_landmarks.landmark[idx]
                    landmark_points.append([lm.x * w, lm.y * h])
            
            if len(landmark_points) < 6:
                logger.error("‚ùå Â§¥ÈÉ®ÂßøÊÄÅÂàÜÊûê: ÂÖ≥ÈîÆÁÇπÊï∞Èáè‰∏çË∂≥")
                raise Exception("Â§¥ÈÉ®ÂßøÊÄÅÂàÜÊûêÂ§±Ë¥•ÔºöÂÖ≥ÈîÆÁÇπÊï∞Èáè‰∏çË∂≥")
            
            image_points = np.array(landmark_points, dtype=np.float64)
            
            # Áõ∏Êú∫ÂèÇÊï∞ (‰º∞ÁÆó)
            focal_length = w
            center = (w/2, h/2)
            camera_matrix = np.array([
                [focal_length, 0, center[0]],
                [0, focal_length, center[1]],
                [0, 0, 1]
            ], dtype=np.float64)
            
            dist_coeffs = np.zeros((4,1))
            
            # ‰ΩøÁî®PnPÁÆóÊ≥ïÊ±ÇËß£Â§¥ÈÉ®ÂßøÊÄÅ
            success, rotation_vector, translation_vector = cv2.solvePnP(
                self.model_points, image_points, camera_matrix, dist_coeffs
            )
            
            if not success:
                logger.error("‚ùå Â§¥ÈÉ®ÂßøÊÄÅÂàÜÊûê: PnPÁÆóÊ≥ïÊ±ÇËß£Â§±Ë¥•")
                raise Exception("Â§¥ÈÉ®ÂßøÊÄÅÂàÜÊûêÂ§±Ë¥•ÔºöPnPÁÆóÊ≥ïÊ±ÇËß£Â§±Ë¥•")
            
            # ËΩ¨Êç¢‰∏∫Ê¨ßÊãâËßí
            rotation_matrix, _ = cv2.Rodrigues(rotation_vector)
            angles = self._rotation_matrix_to_euler(rotation_matrix)
            
            return {
                'pitch': float(angles[0]),  # ‰øØ‰ª∞Ëßí
                'yaw': float(angles[1]),    # ÂÅ∂Ëà™Ëßí
                'roll': float(angles[2])    # ÁøªÊªöËßí
            }
            
        except Exception as e:
            logger.error(f"‚ùå Â§¥ÈÉ®ÂßøÊÄÅÂàÜÊûêÂ§±Ë¥•: {str(e)}")
            logger.error(f"üîß ÈîôËØØËØ¶ÊÉÖ: {traceback.format_exc()}")
            raise
    
    def _analyze_gaze_direction(self, face_landmarks, frame_shape) -> Optional[Dict[str, float]]:
        """ÂàÜÊûêËßÜÁ∫øÊñπÂêë"""
        
        try:
            h, w = frame_shape[:2]
            
            # Ëé∑ÂèñËôπËÜúÂíåÁúºËßíÂÖ≥ÈîÆÁÇπ
            left_iris_idx = 468
            right_iris_idx = 473
            left_eye_corner_idx = 33
            right_eye_corner_idx = 263
            
            if (left_iris_idx >= len(face_landmarks.landmark) or 
                right_iris_idx >= len(face_landmarks.landmark)):
                logger.error("‚ùå ËßÜÁ∫øÂàÜÊûêÂ§±Ë¥•ÔºöËôπËÜúÂÖ≥ÈîÆÁÇπÁ¥¢ÂºïË∂ÖÂá∫ËåÉÂõ¥")
                raise Exception("ËßÜÁ∫øÂàÜÊûêÂ§±Ë¥•ÔºöËôπËÜúÂÖ≥ÈîÆÁÇπÁ¥¢ÂºïË∂ÖÂá∫ËåÉÂõ¥")
            
            # ËÆ°ÁÆóËßÜÁ∫øÂêëÈáè
            left_iris = face_landmarks.landmark[left_iris_idx]
            right_iris = face_landmarks.landmark[right_iris_idx]
            left_corner = face_landmarks.landmark[left_eye_corner_idx]
            right_corner = face_landmarks.landmark[right_eye_corner_idx]
            
            # ËÆ°ÁÆóËßÜÁ∫øÂÅèÁßª
            left_gaze_x = (left_iris.x - left_corner.x) * w
            left_gaze_y = (left_iris.y - left_corner.y) * h
            right_gaze_x = (right_iris.x - right_corner.x) * w
            right_gaze_y = (right_iris.y - right_corner.y) * h
            
            # Âπ≥ÂùáËßÜÁ∫øÊñπÂêë
            avg_gaze_x = (left_gaze_x + right_gaze_x) / 2
            avg_gaze_y = (left_gaze_y + right_gaze_y) / 2
            
            return {
                'gaze_x': float(avg_gaze_x),
                'gaze_y': float(avg_gaze_y),
                'gaze_magnitude': float(np.sqrt(avg_gaze_x**2 + avg_gaze_y**2))
            }
            
        except Exception as e:
            logger.error(f"‚ùå ËßÜÁ∫øÂàÜÊûêÂ§±Ë¥•: {str(e)}")
            logger.error(f"üîß ÈîôËØØËØ¶ÊÉÖ: {traceback.format_exc()}")
            raise
    
    def _analyze_emotion(self, frame) -> Optional[Dict[str, float]]:
        """ÂàÜÊûêÊÉÖÁª™"""
        
        if not DEEPFACE_AVAILABLE:
            error_msg = f"‚ùå DeepFace‰∏çÂèØÁî®ÔºåÊó†Ê≥ïËøõË°åÊÉÖÁª™ÂàÜÊûê: {DEEPFACE_ERROR}"
            logger.error(f"‚ùå {error_msg}")
            raise Exception(error_msg)
        
        try:
            logger.debug("üòä ÂºÄÂßãDeepFaceÊÉÖÁª™ÂàÜÊûê...")
            
            # Ê£ÄÊü•Â∏ßÁöÑÊúâÊïàÊÄß
            if frame is None or frame.size == 0:
                logger.error("‚ùå ÊÉÖÁª™ÂàÜÊûêÂ§±Ë¥•: ËæìÂÖ•Â∏ß‰∏∫Á©∫")
                raise Exception("ÊÉÖÁª™ÂàÜÊûêÂ§±Ë¥•ÔºöËæìÂÖ•Â∏ß‰∏∫Á©∫")
            
            # ‰ΩøÁî®DeepFaceÂàÜÊûêÊÉÖÁª™
            result = DeepFace.analyze(
                frame, 
                actions=['emotion'], 
                enforce_detection=False,
                detector_backend=model_config.DEEPFACE_BACKEND,
                silent=True
            )
            
            if isinstance(result, list):
                result = result[0]
            
            emotions = result.get('emotion', {})
            
            if not emotions:
                logger.error("‚ùå DeepFaceËøîÂõûÁ©∫ÁöÑÊÉÖÁª™ÁªìÊûú")
                raise Exception("DeepFaceËøîÂõûÁ©∫ÁöÑÊÉÖÁª™ÁªìÊûú")
            
            # ÊâæÂá∫‰∏ªÂØºÊÉÖÁª™
            dominant_emotion = max(emotions.items(), key=lambda x: x[1])
            
            logger.debug(f"‚úÖ ÊÉÖÁª™ÂàÜÊûêÊàêÂäü: {dominant_emotion[0]} ({dominant_emotion[1]:.2f}%)")
            
            return {
                'dominant_emotion': dominant_emotion[0],
                'dominant_score': float(dominant_emotion[1]),
                'emotions': {k: float(v) for k, v in emotions.items()}
            }
            
        except Exception as e:
            logger.error(f"‚ùå DeepFaceÊÉÖÁª™ÂàÜÊûêÂ§±Ë¥•: {str(e)}")
            logger.error(f"üîß ÈîôËØØËØ¶ÊÉÖ: {traceback.format_exc()}")
            raise
    
    def _rotation_matrix_to_euler(self, rotation_matrix) -> Tuple[float, float, float]:
        """ÊóãËΩ¨Áü©ÈòµËΩ¨Ê¨ßÊãâËßí"""
        
        sy = np.sqrt(rotation_matrix[0,0] * rotation_matrix[0,0] + 
                    rotation_matrix[1,0] * rotation_matrix[1,0])
        
        singular = sy < 1e-6
        
        if not singular:
            x = np.arctan2(rotation_matrix[2,1], rotation_matrix[2,2])
            y = np.arctan2(-rotation_matrix[2,0], sy)
            z = np.arctan2(rotation_matrix[1,0], rotation_matrix[0,0])
        else:
            x = np.arctan2(-rotation_matrix[1,2], rotation_matrix[1,1])
            y = np.arctan2(-rotation_matrix[2,0], sy)
            z = 0
        
        return np.degrees([x, y, z])
    
    def _analyze_body_language(self, frame: np.ndarray) -> Optional[Dict[str, Any]]:
        """ÂàÜÊûê‰ΩìÊÄÅËØ≠Ë®Ä - ‰ΩøÁî®MediaPipe Pose"""
        try:
            # ËΩ¨Êç¢‰∏∫RGBÊ†ºÂºè
            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            
            # ËøõË°åÂßøÊÄÅÊ£ÄÊµã
            results = self.pose.process(rgb_frame)
            
            if not results.pose_landmarks:
                logger.debug("ü§∑ Êú™Ê£ÄÊµãÂà∞Ë∫´‰ΩìÂßøÊÄÅ")
                return None
            
            landmarks = results.pose_landmarks.landmark
            
            # ÂÖ≥ÈîÆÁÇπÁ¥¢Âºï
            LEFT_SHOULDER = self.mp_pose.PoseLandmark.LEFT_SHOULDER.value
            RIGHT_SHOULDER = self.mp_pose.PoseLandmark.RIGHT_SHOULDER.value
            LEFT_HIP = self.mp_pose.PoseLandmark.LEFT_HIP.value
            RIGHT_HIP = self.mp_pose.PoseLandmark.RIGHT_HIP.value
            NOSE = self.mp_pose.PoseLandmark.NOSE.value
            
            # ËÆ°ÁÆóËÇ©ËÜÄÊ∞¥Âπ≥Â∫¶
            left_shoulder = landmarks[LEFT_SHOULDER]
            right_shoulder = landmarks[RIGHT_SHOULDER]
            shoulder_slope = abs(left_shoulder.y - right_shoulder.y)
            
            # ËÆ°ÁÆóË∫´‰ΩìÂÄæÊñúÂ∫¶ (Âü∫‰∫éËÇ©ËÜÄÂíåËáÄÈÉ®ÁöÑÂÅèÁßª)
            left_hip = landmarks[LEFT_HIP]
            right_hip = landmarks[RIGHT_HIP]
            
            shoulder_center_x = (left_shoulder.x + right_shoulder.x) / 2
            hip_center_x = (left_hip.x + right_hip.x) / 2
            body_angle = abs(shoulder_center_x - hip_center_x) * 180  # ËΩ¨Êç¢‰∏∫ËßíÂ∫¶
            
            # ËÆ°ÁÆóÂßøÊÄÅÂàÜÊï∞ (0-100)
            posture_score = max(0, 100 - (shoulder_slope * 500) - (body_angle * 2))
            
            # ËÆ°ÁÆóË∫´‰ΩìÁ®≥ÂÆöÊÄß
            nose_position = landmarks[NOSE]
            center_offset = abs(nose_position.x - 0.5)  # Áõ∏ÂØπ‰∫éÁîªÈù¢‰∏≠ÂøÉÁöÑÂÅèÁßª
            
            # ÂàÜÊûêÂùêÂßø/Á´ôÂßøÁä∂ÊÄÅ
            shoulder_hip_distance = abs(left_shoulder.y - left_hip.y)
            posture_type = "sitting" if shoulder_hip_distance < 0.3 else "standing"
            
            # Ê£ÄÊµãÁ¥ßÂº†ËøπË±°
            tension_indicators = 0
            if shoulder_slope > 0.05:  # ËÇ©ËÜÄ‰∏çÂπ≥
                tension_indicators += 1
            if body_angle > 10:  # Ë∫´‰ΩìËøáÂ∫¶ÂÄæÊñú
                tension_indicators += 1
            if center_offset > 0.15:  # ÂÅèÁ¶ª‰∏≠ÂøÉËøáÂ§ö
                tension_indicators += 1
                
            tension_level = min(100, tension_indicators * 30)
            
            logger.debug(f"üßç ‰ΩìÊÄÅÂàÜÊûê: ÂßøÊÄÅÂàÜÊï∞={posture_score:.1f}, Ë∫´‰ΩìËßíÂ∫¶={body_angle:.1f}¬∞, Á¥ßÂº†Â∫¶={tension_level}")
            
            return {
                'posture_score': float(posture_score),
                'body_angle': float(body_angle),
                'shoulder_slope': float(shoulder_slope),
                'tension_level': float(tension_level),
                'posture_type': posture_type,
                'center_offset': float(center_offset),
                'body_stability': float(1.0 - center_offset)
            }
            
        except Exception as e:
            logger.error(f"‚ùå ‰ΩìÊÄÅËØ≠Ë®ÄÂàÜÊûêÂ§±Ë¥•: {str(e)}")
            return None
    
    def _analyze_gestures(self, frame: np.ndarray) -> Optional[Dict[str, Any]]:
        """ÂàÜÊûêÊâãÂäø - ‰ΩøÁî®MediaPipe Hands"""
        try:
            # ËΩ¨Êç¢‰∏∫RGBÊ†ºÂºè
            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            
            # ËøõË°åÊâãÈÉ®Ê£ÄÊµã
            results = self.hands.process(rgb_frame)
            
            if not results.multi_hand_landmarks:
                logger.debug("üëã Êú™Ê£ÄÊµãÂà∞ÊâãÈÉ®")
                return {
                    'hands_detected': 0,
                    'gesture_activity': 0.0,
                    'dominant_gesture': 'none',
                    'hand_positions': []
                }
            
            hands_data = []
            total_movement = 0.0
            
            for idx, hand_landmarks in enumerate(results.multi_hand_landmarks):
                landmarks = hand_landmarks.landmark
                
                # ÂÖ≥ÈîÆÁÇπÁ¥¢Âºï
                WRIST = 0
                THUMB_TIP = 4
                INDEX_TIP = 8
                MIDDLE_TIP = 12
                RING_TIP = 16
                PINKY_TIP = 20
                
                # ËÆ°ÁÆóÊâãÁöÑ‰ΩçÁΩÆ (Âü∫‰∫éÊâãËÖï)
                wrist = landmarks[WRIST]
                hand_position = (wrist.x, wrist.y)
                
                # ËÆ°ÁÆóÊâãÊåáÁöÑÂ±ïÂºÄÁ®ãÂ∫¶
                finger_tips = [
                    landmarks[THUMB_TIP],
                    landmarks[INDEX_TIP], 
                    landmarks[MIDDLE_TIP],
                    landmarks[RING_TIP],
                    landmarks[PINKY_TIP]
                ]
                
                # ËÆ°ÁÆóÊâãÊåáÈó¥ÁöÑÂπ≥ÂùáË∑ùÁ¶ª (Ë°®Á§∫ÊâãÁöÑÂ±ïÂºÄÁ®ãÂ∫¶)
                finger_spread = 0
                for i in range(len(finger_tips)):
                    for j in range(i + 1, len(finger_tips)):
                        tip1, tip2 = finger_tips[i], finger_tips[j]
                        distance = np.sqrt((tip1.x - tip2.x)**2 + (tip1.y - tip2.y)**2)
                        finger_spread += distance
                
                finger_spread /= 10  # Ê†áÂáÜÂåñ
                
                # ËÆ°ÁÆóÊâãÁöÑÁßªÂä®Èáè (Âü∫‰∫éÁõ∏ÂØπ‰ΩçÁΩÆÂèòÂåñ)
                movement = abs(wrist.x - 0.5) + abs(wrist.y - 0.5)
                total_movement += movement
                
                # ÁÆÄÂçïÁöÑÊâãÂäøËØÜÂà´
                gesture_type = self._classify_gesture(landmarks)
                
                hands_data.append({
                    'hand_index': idx,
                    'position': hand_position,
                    'finger_spread': float(finger_spread),
                    'movement': float(movement),
                    'gesture_type': gesture_type
                })
            
            # ËÆ°ÁÆóÊï¥‰ΩìÊâãÂäøÊ¥ªË∑ÉÂ∫¶
            gesture_activity = min(100, total_movement * 200)
            
            # Á°ÆÂÆö‰∏ªÂØºÊâãÂäø
            gesture_types = [hand['gesture_type'] for hand in hands_data]
            dominant_gesture = max(set(gesture_types), key=gesture_types.count) if gesture_types else 'none'
            
            logger.debug(f"üëã ÊâãÂäøÂàÜÊûê: Ê£ÄÊµãÂà∞{len(hands_data)}Âè™Êâã, Ê¥ªË∑ÉÂ∫¶={gesture_activity:.1f}, ‰∏ªÂØºÊâãÂäø={dominant_gesture}")
            
            return {
                'hands_detected': len(hands_data),
                'gesture_activity': float(gesture_activity),
                'dominant_gesture': dominant_gesture,
                'hand_positions': hands_data,
                'total_movement': float(total_movement)
            }
            
        except Exception as e:
            logger.error(f"‚ùå ÊâãÂäøÂàÜÊûêÂ§±Ë¥•: {str(e)}")
            return None
    
    def _classify_gesture(self, landmarks) -> str:
        """ÁÆÄÂçïÁöÑÊâãÂäøÂàÜÁ±ª"""
        try:
            # ÂÖ≥ÈîÆÁÇπ
            wrist = landmarks[0]
            thumb_tip = landmarks[4]
            index_tip = landmarks[8]
            middle_tip = landmarks[12]
            ring_tip = landmarks[16]
            pinky_tip = landmarks[20]
            
            # ËÆ°ÁÆóÊâãÊåáÁõ∏ÂØπ‰∫éÊâãËÖïÁöÑ‰ΩçÁΩÆ
            fingers_up = []
            
            # ÊãáÊåá (ÁâπÊÆäÂ§ÑÁêÜÔºåÂõ†‰∏∫ÊñπÂêë‰∏çÂêå)
            if thumb_tip.x > wrist.x:  # ÂÅáËÆæÂè≥Êâã
                fingers_up.append(thumb_tip.x > landmarks[3].x)
            else:  # Â∑¶Êâã
                fingers_up.append(thumb_tip.x < landmarks[3].x)
            
            # ÂÖ∂‰ªñÂõõÊåá
            finger_tips = [index_tip, middle_tip, ring_tip, pinky_tip]
            finger_pips = [landmarks[6], landmarks[10], landmarks[14], landmarks[18]]
            
            for tip, pip in zip(finger_tips, finger_pips):
                fingers_up.append(tip.y < pip.y)
            
            # ÁÆÄÂçïÊâãÂäøËØÜÂà´
            fingers_count = sum(fingers_up)
            
            if fingers_count == 0:
                return "fist"
            elif fingers_count == 1:
                if fingers_up[1]:  # Âè™ÊúâÈ£üÊåá
                    return "pointing"
                elif fingers_up[0]:  # Âè™ÊúâÊãáÊåá
                    return "thumbs_up"
                else:
                    return "one_finger"
            elif fingers_count == 2:
                if fingers_up[1] and fingers_up[2]:  # È£üÊåáÂíå‰∏≠Êåá
                    return "peace"
                else:
                    return "two_fingers"
            elif fingers_count == 5:
                return "open_hand"
            else:
                return "partial_open"
                
        except Exception as e:
            logger.debug(f"‚ö†Ô∏è ÊâãÂäøÂàÜÁ±ªÂ§±Ë¥•: {str(e)}")
            return "unknown"
    
    def _compute_visual_statistics(
        self, 
        head_poses: List[Dict], 
        gaze_directions: List[Dict], 
        emotions_timeline: List[Dict],
        body_language_results: List[Dict] = None,
        gesture_results: List[Dict] = None
    ) -> Dict[str, Any]:
        """ËÆ°ÁÆóËßÜËßâÂàÜÊûêÁªüËÆ°ÁâπÂæÅ"""
        
        if not head_poses and not gaze_directions and not emotions_timeline:
            error_msg = "ÊâÄÊúâËßÜËßâÂàÜÊûêÁªìÊûúÂùá‰∏∫Á©∫ÔºåÊó†Ê≥ïËÆ°ÁÆóÁªüËÆ°ÁâπÂæÅ"
            logger.error(f"‚ùå {error_msg}")
            raise Exception(error_msg)
        
        result = {}
        
        if head_poses:
            # ËÆ°ÁÆóÂ§¥ÈÉ®ÂßøÊÄÅÁ®≥ÂÆöÊÄß
            yaw_values = [pose['yaw'] for pose in head_poses]
            pitch_values = [pose['pitch'] for pose in head_poses]
            
            yaw_variance = np.var(yaw_values)
            pitch_variance = np.var(pitch_values)
            
            # Á®≥ÂÆöÊÄß‰∏éÊñπÂ∑ÆÊàêÂèçÊØî
            head_stability = 1.0 / (1.0 + (yaw_variance + pitch_variance) / 100)
            result['head_pose_stability'] = float(head_stability)
        else:
            logger.warning("‚ö†Ô∏è Â§¥ÈÉ®ÂßøÊÄÅÊï∞ÊçÆ‰∏∫Á©∫")
        
        if gaze_directions:
            # ËÆ°ÁÆóËßÜÁ∫øÁ®≥ÂÆöÊÄß
            gaze_magnitudes = [gaze['gaze_magnitude'] for gaze in gaze_directions]
            gaze_variance = np.var(gaze_magnitudes)
            
            gaze_stability = 1.0 / (1.0 + gaze_variance / 10)
            result['gaze_stability'] = float(gaze_stability)
            
            # ËÆ°ÁÆóÁúºÁ•ûÊé•Ëß¶ÊØî‰æã (ËßÜÁ∫øÂÅèÁßªÂ∞è‰∫éÈòàÂÄºÁöÑÂ∏ßÊï∞ÊØî‰æã)
            eye_contact_frames = sum(1 for mag in gaze_magnitudes if mag < 5.0)
            result['eye_contact_ratio'] = eye_contact_frames / len(gaze_magnitudes)
        else:
            logger.warning("‚ö†Ô∏è ËßÜÁ∫øÊñπÂêëÊï∞ÊçÆ‰∏∫Á©∫")
        
        if emotions_timeline:
            # ÂàÜÊûêÊÉÖÁª™ÂàÜÂ∏É
            emotion_counts = {}
            for emotion_data in emotions_timeline:
                emotion = emotion_data['dominant_emotion']
                emotion_counts[emotion] = emotion_counts.get(emotion, 0) + 1
            
            # ‰∏ªÂØºÊÉÖÁª™
            if emotion_counts:
                result['dominant_emotion'] = max(emotion_counts.items(), key=lambda x: x[1])[0]
            
            # ÊÉÖÁª™Á®≥ÂÆöÊÄß (Ë¥üÈù¢ÊÉÖÁª™Âç†ÊØîÁöÑÂÄíÊï∞)
            negative_emotions = ['angry', 'disgust', 'fear', 'sad']
            negative_count = sum(emotion_counts.get(emotion, 0) for emotion in negative_emotions)
            total_count = sum(emotion_counts.values())
            
            if total_count > 0:
                negative_ratio = negative_count / total_count
                result['emotion_stability'] = 1.0 - negative_ratio
        else:
            logger.warning("‚ö†Ô∏è ÊÉÖÁª™ÂàÜÊûêÊï∞ÊçÆ‰∏∫Á©∫")
        
        # ‰ΩìÊÄÅËØ≠Ë®ÄÂàÜÊûêÁªüËÆ°
        if body_language_results:
            posture_scores = [bl.get('posture_score', 0) for bl in body_language_results if 'posture_score' in bl]
            if posture_scores:
                result['avg_posture_score'] = float(np.mean(posture_scores))
                result['posture_stability'] = float(1.0 - (np.std(posture_scores) / 100.0))
            
            # Ë∫´‰ΩìÂÄæÊñúÂ∫¶ÁªüËÆ°
            body_angles = [bl.get('body_angle', 0) for bl in body_language_results if 'body_angle' in bl]
            if body_angles:
                result['avg_body_angle'] = float(np.mean(body_angles))
                result['body_angle_variance'] = float(np.var(body_angles))
        else:
            logger.warning("‚ö†Ô∏è ‰ΩìÊÄÅËØ≠Ë®ÄÂàÜÊûêÊï∞ÊçÆ‰∏∫Á©∫")
        
        # ÊâãÂäøÂàÜÊûêÁªüËÆ°
        if gesture_results:
            gesture_activity = [gr.get('gesture_activity', 0) for gr in gesture_results if 'gesture_activity' in gr]
            if gesture_activity:
                result['avg_gesture_activity'] = float(np.mean(gesture_activity))
                result['gesture_expressiveness'] = float(np.max(gesture_activity))
            
            # ÊâãÂäøÁ±ªÂûãÂàÜÂ∏É
            gesture_types = {}
            for gr in gesture_results:
                gesture_type = gr.get('dominant_gesture', 'unknown')
                gesture_types[gesture_type] = gesture_types.get(gesture_type, 0) + 1
            
            if gesture_types:
                result['dominant_gesture_type'] = max(gesture_types.items(), key=lambda x: x[1])[0]
                result['gesture_variety'] = len(gesture_types)
        else:
            logger.warning("‚ö†Ô∏è ÊâãÂäøÂàÜÊûêÊï∞ÊçÆ‰∏∫Á©∫")
        
        return result


class AudioAnalyzer:
    """Èü≥È¢ëÂàÜÊûêÂô® - Â§ÑÁêÜÂê¨ËßâÊ®°ÊÄÅ"""
    
    def __init__(self):
        self.sample_rate = model_config.AUDIO_SAMPLE_RATE
    
    def analyze_audio(self, audio_path: str) -> Dict[str, Any]:
        """ÂàÜÊûêÈü≥È¢ëÊñá‰ª∂ÔºåÊèêÂèñÂê¨ËßâÁâπÂæÅ"""
        
        start_time = datetime.now()
        logger.info(f"üéµ ÂºÄÂßãÈü≥È¢ëÂàÜÊûê: {audio_path}")
        
        # Ê£ÄÊü•Èü≥È¢ëÊñá‰ª∂ÊòØÂê¶Â≠òÂú®
        if not Path(audio_path).exists():
            error_msg = f"Èü≥È¢ëÊñá‰ª∂‰∏çÂ≠òÂú®: {audio_path}"
            logger.error(f"‚ùå {error_msg}")
            raise FileNotFoundError(error_msg)
        
        # Ê£ÄÊü•Êñá‰ª∂Â§ßÂ∞è
        file_size = Path(audio_path).stat().st_size
        if file_size == 0:
            error_msg = "Èü≥È¢ëÊñá‰ª∂‰∏∫Á©∫"
            logger.error(f"‚ùå {error_msg}")
            raise Exception(error_msg)
        
        logger.info(f"üìä Èü≥È¢ëÊñá‰ª∂Â§ßÂ∞è: {file_size / 1024:.1f} KB")
        
        try:
            # Âä†ËΩΩÈü≥È¢ëÊñá‰ª∂
            logger.debug("üîÑ Âä†ËΩΩÈü≥È¢ëÊñá‰ª∂...")
            y, sr = librosa.load(audio_path, sr=self.sample_rate)
            
            if len(y) == 0:
                error_msg = "Èü≥È¢ëÊï∞ÊçÆ‰∏∫Á©∫"
                logger.error(f"‚ùå {error_msg}")
                raise Exception(error_msg)
            
            duration = len(y) / sr
            logger.info(f"üìä Èü≥È¢ë‰ø°ÊÅØ: ÈááÊ†∑Áéá{sr}Hz, Êó∂Èïø{duration:.2f}Áßí, {len(y)}‰∏™ÈááÊ†∑ÁÇπ")
            
            # ËØ≠ÈÄüÂàÜÊûê
            logger.debug("üó£Ô∏è ÂàÜÊûêËØ≠ÈÄü...")
            speech_rate = self._analyze_speech_rate(y, sr)
            logger.debug(f"‚úÖ ËØ≠ÈÄüÂàÜÊûêÂÆåÊàê: {speech_rate:.1f} BPM")
            
            # Èü≥È´òÂàÜÊûê
            logger.debug("üéº ÂàÜÊûêÈü≥È´ò...")
            pitch_analysis = self._analyze_pitch(y, sr)
            logger.debug(f"‚úÖ Èü≥È´òÂàÜÊûêÂÆåÊàê: Âπ≥Âùá{pitch_analysis['mean']:.1f}Hz")
            
            # Èü≥ÈáèÂàÜÊûê
            logger.debug("üì¢ ÂàÜÊûêÈü≥Èáè...")
            volume_analysis = self._analyze_volume(y)
            logger.debug(f"‚úÖ Èü≥ÈáèÂàÜÊûêÂÆåÊàê: Âπ≥Âùá{volume_analysis['mean']:.1f}dB")
            
            # ËØ≠Èü≥Ê∏ÖÊô∞Â∫¶ÂàÜÊûê
            logger.debug("üéØ ÂàÜÊûêÊ∏ÖÊô∞Â∫¶...")
            clarity_score = self._analyze_clarity(y, sr)
            logger.debug(f"‚úÖ Ê∏ÖÊô∞Â∫¶ÂàÜÊûêÂÆåÊàê: {clarity_score:.2f}")
            
            processing_time = (datetime.now() - start_time).total_seconds()
            
            result = {
                'speech_rate_bpm': speech_rate,
                'pitch_mean': pitch_analysis['mean'],
                'pitch_variance': pitch_analysis['variance'],
                'pitch_range': pitch_analysis['range'],
                'volume_mean': volume_analysis['mean'],
                'volume_variance': volume_analysis['variance'],
                'clarity_score': clarity_score,
                'audio_duration': duration,
                'processing_stats': {
                    'processing_time_seconds': processing_time,
                    'sample_rate': sr,
                    'samples_count': len(y),
                    'file_size_bytes': file_size
                }
            }
            
            logger.info(f"‚úÖ Èü≥È¢ëÂàÜÊûêÊàêÂäüÂÆåÊàê (ËÄóÊó∂: {processing_time:.2f}Áßí)")
            return result
            
        except Exception as e:
            processing_time = (datetime.now() - start_time).total_seconds()
            logger.error(f"‚ùå Èü≥È¢ëÂàÜÊûêÂ§±Ë¥•: {str(e)}")
            logger.error(f"üîß ÈîôËØØËØ¶ÊÉÖ: {traceback.format_exc()}")
            logger.error(f"‚è±Ô∏è Â§±Ë¥•ÂâçÂ§ÑÁêÜÊó∂Èó¥: {processing_time:.2f}Áßí")
            raise
    
    def _analyze_speech_rate(self, y: np.ndarray, sr: int) -> float:
        """ÂàÜÊûêËØ≠ÈÄü"""
        
        try:
            # ‰ΩøÁî®ËäÇÊãçËøΩË∏™‰º∞ÁÆóËØ≠ÈÄü
            tempo, beats = librosa.beat.beat_track(y=y, sr=sr)
            
            # Â∞ÜÈü≥‰πêËäÇÊãçËΩ¨Êç¢‰∏∫ËØ≠Èü≥ËäÇÂ•èÁöÑ‰º∞ÁÆó
            # ËØ≠Èü≥ÁöÑ"ËäÇÊãç"ÈÄöÂ∏∏ÊØîÈü≥‰πêÊÖ¢ÔºåÈúÄË¶ÅË∞ÉÊï¥
            speech_tempo = tempo * 0.6  # ÁªèÈ™åË∞ÉÊï¥Âõ†Â≠ê
            
            return float(speech_tempo)
            
        except Exception as e:
            logger.error(f"‚ùå ËØ≠ÈÄüÂàÜÊûêÂ§±Ë¥•: {str(e)}")
            logger.error(f"üîß ÈîôËØØËØ¶ÊÉÖ: {traceback.format_exc()}")
            raise
    
    def _analyze_pitch(self, y: np.ndarray, sr: int) -> Dict[str, float]:
        """ÂàÜÊûêÈü≥È´òÁâπÂæÅ"""
        
        try:
            # ‰ΩøÁî®pyinÁÆóÊ≥ïÊèêÂèñÂü∫È¢ë
            f0, voiced_flag, voiced_probs = librosa.pyin(
                y, fmin=librosa.note_to_hz('C2'), fmax=librosa.note_to_hz('C7')
            )
            
            # Âè™‰øùÁïôÊúâÊïàÁöÑÈü≥È´òÂÄº
            valid_f0 = f0[voiced_flag]
            
            if len(valid_f0) == 0:
                error_msg = "Èü≥È´òÂàÜÊûêÂ§±Ë¥•ÔºöÊú™Ê£ÄÊµãÂà∞ÊúâÊïàÁöÑÈü≥È´òÂÄº"
                logger.error(f"‚ùå {error_msg}")
                raise Exception(error_msg)
            
            pitch_mean = float(np.mean(valid_f0))
            pitch_variance = float(np.var(valid_f0))
            pitch_range = float(np.max(valid_f0) - np.min(valid_f0))
            
            return {
                'mean': pitch_mean,
                'variance': pitch_variance,
                'range': pitch_range
            }
            
        except Exception as e:
            logger.error(f"‚ùå Èü≥È´òÂàÜÊûêÂ§±Ë¥•: {str(e)}")
            logger.error(f"üîß ÈîôËØØËØ¶ÊÉÖ: {traceback.format_exc()}")
            raise
    
    def _analyze_volume(self, y: np.ndarray) -> Dict[str, float]:
        """ÂàÜÊûêÈü≥ÈáèÁâπÂæÅ"""
        
        try:
            # ËÆ°ÁÆóRMSËÉΩÈáè
            rms = librosa.feature.rms(y=y)[0]
            
            if len(rms) == 0:
                error_msg = "Èü≥ÈáèÂàÜÊûêÂ§±Ë¥•ÔºöRMSËÆ°ÁÆóÁªìÊûú‰∏∫Á©∫"
                logger.error(f"‚ùå {error_msg}")
                raise Exception(error_msg)
            
            # ËΩ¨Êç¢‰∏∫ÂàÜË¥ù
            db = librosa.amplitude_to_db(rms)
            
            volume_mean = float(np.mean(db))
            volume_variance = float(np.var(db))
            
            return {
                'mean': volume_mean,
                'variance': volume_variance
            }
            
        except Exception as e:
            logger.error(f"‚ùå Èü≥ÈáèÂàÜÊûêÂ§±Ë¥•: {str(e)}")
            logger.error(f"üîß ÈîôËØØËØ¶ÊÉÖ: {traceback.format_exc()}")
            raise
    
    def _analyze_clarity(self, y: np.ndarray, sr: int) -> float:
        """ÂàÜÊûêËØ≠Èü≥Ê∏ÖÊô∞Â∫¶"""
        
        try:
            # ‰ΩøÁî®È¢ëË∞±Ë¥®ÂøÉ‰Ωú‰∏∫Ê∏ÖÊô∞Â∫¶ÁöÑ‰ª£ÁêÜÊåáÊ†á
            spectral_centroids = librosa.feature.spectral_centroid(y=y, sr=sr)[0]
            
            if len(spectral_centroids) == 0:
                error_msg = "Ê∏ÖÊô∞Â∫¶ÂàÜÊûêÂ§±Ë¥•ÔºöÈ¢ëË∞±Ë¥®ÂøÉËÆ°ÁÆóÁªìÊûú‰∏∫Á©∫"
                logger.error(f"‚ùå {error_msg}")
                raise Exception(error_msg)
            
            # ËÆ°ÁÆóÊ∏ÖÊô∞Â∫¶ÂàÜÊï∞ (È¢ëË∞±Ë¥®ÂøÉÁöÑÁ®≥ÂÆöÊÄß)
            centroid_variance = np.var(spectral_centroids)
            clarity_score = 1.0 / (1.0 + centroid_variance / 1000000)
            
            return float(clarity_score)
            
        except Exception as e:
            logger.error(f"‚ùå Ê∏ÖÊô∞Â∫¶ÂàÜÊûêÂ§±Ë¥•: {str(e)}")
            logger.error(f"üîß ÈîôËØØËØ¶ÊÉÖ: {traceback.format_exc()}")
            raise


class MultimodalAnalyzer:
    """Â§öÊ®°ÊÄÅÂàÜÊûêÂô® - Êï¥ÂêàËßÜËßâÂíåÂê¨ËßâÂàÜÊûê"""
    
    def __init__(self):
        self.video_analyzer = VideoAnalyzer()
        self.audio_analyzer = AudioAnalyzer()
    
    def analyze_interview_media(
        self, 
        video_path: Optional[str] = None, 
        audio_path: Optional[str] = None
    ) -> Dict[str, Any]:
        """ÂàÜÊûêÈù¢ËØïÁöÑÈü≥ËßÜÈ¢ëÊï∞ÊçÆ"""
        
        start_time = datetime.now()
        logger.info("üöÄ ÂºÄÂßãÂ§öÊ®°ÊÄÅÈù¢ËØïÂàÜÊûê")
        logger.info(f"üìÅ ËßÜÈ¢ëÊñá‰ª∂: {video_path if video_path else 'Êú™Êèê‰æõ'}")
        logger.info(f"üìÅ Èü≥È¢ëÊñá‰ª∂: {audio_path if audio_path else 'Êú™Êèê‰æõ'}")
        
        if not video_path and not audio_path:
            error_msg = "ÂøÖÈ°ªËá≥Â∞ëÊèê‰æõËßÜÈ¢ëÊñá‰ª∂ÊàñÈü≥È¢ëÊñá‰ª∂Ë∑ØÂæÑ"
            logger.error(f"‚ùå {error_msg}")
            raise ValueError(error_msg)
        
        result = {
            'visual_analysis': None,
            'audio_analysis': None,
            'analysis_timestamp': start_time.isoformat(),
            'processing_summary': {
                'video_success': False,
                'audio_success': False,
                'errors': []
            }
        }
        
        # DeepFaceÂèØÁî®ÊÄßÊ£ÄÊü•
        if not DEEPFACE_AVAILABLE:
            error_msg = f"DeepFace‰∏çÂèØÁî®ÔºåÊó†Ê≥ïËøõË°åÊÉÖÁª™ÂàÜÊûê: {DEEPFACE_ERROR}"
            logger.error(f"‚ùå {error_msg}")
            result['processing_summary']['errors'].append(error_msg)
        
        # ËßÜËßâÂàÜÊûê
        if video_path:
            logger.info("=" * 50)
            logger.info("üé• ÂºÄÂßãËßÜËßâÂàÜÊûêÈÉ®ÂàÜ")
            
            try:
                result['visual_analysis'] = self.video_analyzer.analyze_video(video_path)
                result['processing_summary']['video_success'] = True
                logger.info("‚úÖ ËßÜÈ¢ëÂàÜÊûêÈÉ®ÂàÜÂÆåÊàê")
            except Exception as e:
                error_msg = f"ËßÜÈ¢ëÂàÜÊûêÂ§±Ë¥•: {str(e)}"
                logger.error(f"‚ùå {error_msg}")
                logger.error(f"üîß ÈîôËØØËØ¶ÊÉÖ: {traceback.format_exc()}")
                result['processing_summary']['errors'].append(error_msg)
                # ‰∏çÂÜçÊèê‰æõÂ§áÁî®ÁªìÊûúÔºåÁõ¥Êé•ËÆ∞ÂΩïÈîôËØØ
        
        # Âê¨ËßâÂàÜÊûê
        if audio_path:
            logger.info("=" * 50)
            logger.info("üéµ ÂºÄÂßãÂê¨ËßâÂàÜÊûêÈÉ®ÂàÜ")
            
            try:
                result['audio_analysis'] = self.audio_analyzer.analyze_audio(audio_path)
                result['processing_summary']['audio_success'] = True
                logger.info("‚úÖ Èü≥È¢ëÂàÜÊûêÈÉ®ÂàÜÂÆåÊàê")
            except Exception as e:
                error_msg = f"Èü≥È¢ëÂàÜÊûêÂ§±Ë¥•: {str(e)}"
                logger.error(f"‚ùå {error_msg}")
                logger.error(f"üîß ÈîôËØØËØ¶ÊÉÖ: {traceback.format_exc()}")
                result['processing_summary']['errors'].append(error_msg)
                # ‰∏çÂÜçÊèê‰æõÂ§áÁî®ÁªìÊûúÔºåÁõ¥Êé•ËÆ∞ÂΩïÈîôËØØ
        
        # ÂàÜÊûêÊÄªÁªì
        total_processing_time = (datetime.now() - start_time).total_seconds()
        result['processing_summary']['total_time_seconds'] = total_processing_time
        
        logger.info("=" * 50)
        logger.info("üìä Â§öÊ®°ÊÄÅÂàÜÊûêÊÄªÁªì:")
        logger.info(f"   üé• ËßÜÈ¢ëÂàÜÊûê: {'‚úÖ ÊàêÂäü' if result['processing_summary']['video_success'] else '‚ùå Â§±Ë¥•/Ë∑≥Ëøá'}")
        logger.info(f"   üéµ Èü≥È¢ëÂàÜÊûê: {'‚úÖ ÊàêÂäü' if result['processing_summary']['audio_success'] else '‚ùå Â§±Ë¥•/Ë∑≥Ëøá'}")
        logger.info(f"   ‚è±Ô∏è ÊÄªÂ§ÑÁêÜÊó∂Èó¥: {total_processing_time:.2f}Áßí")
        
        if result['processing_summary']['errors']:
            logger.error(f"   ‚ö†Ô∏è ÈîôËØØÊï∞Èáè: {len(result['processing_summary']['errors'])}")
            for i, error in enumerate(result['processing_summary']['errors'], 1):
                logger.error(f"     {i}. {error}")
            
            # Â¶ÇÊûúÊâÄÊúâÂàÜÊûêÈÉΩÂ§±Ë¥•ÔºåÊäõÂá∫ÂºÇÂ∏∏
            if not result['processing_summary']['video_success'] and not result['processing_summary']['audio_success']:
                error_msg = "ÊâÄÊúâÂ§öÊ®°ÊÄÅÂàÜÊûêÂùáÂ§±Ë¥•"
                logger.error(f"‚ùå {error_msg}")
                raise Exception(error_msg)
        else:
            logger.info("   üéâ ÊâÄÊúâÂàÜÊûêÂùáÊàêÂäüÂÆåÊàê!")
        
        logger.info("üèÅ Â§öÊ®°ÊÄÅÈù¢ËØïÂàÜÊûêÂÆåÊàê")
        return result


def create_multimodal_analyzer() -> MultimodalAnalyzer:
    """ÂàõÂª∫Â§öÊ®°ÊÄÅÂàÜÊûêÂô®ÂÆû‰æã"""
    return MultimodalAnalyzer() 