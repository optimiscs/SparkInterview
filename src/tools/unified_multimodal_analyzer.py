"""
ç»Ÿä¸€å¤šæ¨¡æ€åˆ†æå™¨
æ•´åˆè§†é¢‘ã€éŸ³é¢‘åˆ†æåŠŸèƒ½ï¼Œå……åˆ†åˆ©ç”¨DeepFaceæ€§èƒ½ï¼Œä¼˜å…ˆä¿è¯æ£€æµ‹å‡†ç¡®ç‡
"""
import cv2
import numpy as np
import librosa
import mediapipe as mp
from typing import Dict, Any, List, Optional, Tuple
import logging
from pathlib import Path
import sys
import traceback
from datetime import datetime
import time
import json
import io
import tempfile
from concurrent.futures import ThreadPoolExecutor
import asyncio

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# DeepFaceåˆå§‹åŒ–
DEEPFACE_AVAILABLE = False
DEEPFACE_ERROR = None
DeepFace = None

def initialize_deepface():
    """åˆå§‹åŒ–DeepFaceï¼Œä¼˜å…ˆä¿è¯æ€§èƒ½"""
    global DEEPFACE_AVAILABLE, DEEPFACE_ERROR, DeepFace
    
    try:
        logger.info("ğŸ” åˆå§‹åŒ–DeepFace (é«˜æ€§èƒ½æ¨¡å¼)...")
        from deepface import DeepFace
        
        # ä½¿ç”¨é«˜è´¨é‡çš„æ£€æµ‹å™¨å’Œè¯†åˆ«æ¨¡å‹
        test_image = np.zeros((224, 224, 3), dtype=np.uint8)
        test_image[50:174, 50:174] = [128, 128, 128]  # ç°è‰²æ–¹å—
        
        # æµ‹è¯•å¤šç§åˆ†æåŠŸèƒ½ç¡®ä¿å®Œæ•´æ€§
        result = DeepFace.analyze(
            test_image,
            actions=['emotion', 'age', 'gender'],
            detector_backend='retinaface',  # ä½¿ç”¨æœ€å‡†ç¡®çš„æ£€æµ‹å™¨
            enforce_detection=False,
            silent=True
        )
        
        DEEPFACE_AVAILABLE = True
        logger.info("âœ… DeepFaceåˆå§‹åŒ–æˆåŠŸ (é«˜æ€§èƒ½æ¨¡å¼)")
        logger.info(f"   æ”¯æŒçš„åˆ†æåŠŸèƒ½: emotion, age, gender")
        logger.info(f"   æ£€æµ‹å™¨: retinaface (é«˜ç²¾åº¦)")
        return True
        
    except ImportError as e:
        DEEPFACE_ERROR = f"DeepFaceæœªå®‰è£…: {str(e)}"
        logger.error(f"âŒ {DEEPFACE_ERROR}")
        return False
    except Exception as e:
        # é™çº§åˆ°opencvæ£€æµ‹å™¨
        try:
            result = DeepFace.analyze(
                test_image,
                actions=['emotion'],
                detector_backend='opencv',
                enforce_detection=False,
                silent=True
            )
            DEEPFACE_AVAILABLE = True
            logger.warning("âš ï¸ DeepFaceé™çº§åˆ°opencvæ£€æµ‹å™¨")
            return True
        except:
            DEEPFACE_ERROR = f"DeepFaceåˆå§‹åŒ–å¤±è´¥: {str(e)}"
            logger.error(f"âŒ {DEEPFACE_ERROR}")
            return False

# åˆå§‹åŒ–DeepFace
initialize_deepface()

try:
    from ..config.settings import model_config
except ImportError:
    logger.warning("âš ï¸ ä½¿ç”¨é»˜è®¤é…ç½®")
    class DefaultConfig:
        MEDIAPIPE_CONFIDENCE = 0.7  # æé«˜ç½®ä¿¡åº¦é˜ˆå€¼ä¿è¯å‡†ç¡®ç‡
        DEEPFACE_BACKEND = 'retinaface'  # ä½¿ç”¨é«˜ç²¾åº¦æ£€æµ‹å™¨
        AUDIO_SAMPLE_RATE = 22050
        ENABLE_FACE_LANDMARKS_REFINEMENT = True  # å¯ç”¨ç²¾ç»†åŒ–
    model_config = DefaultConfig()


class UnifiedVideoAnalyzer:
    """ç»Ÿä¸€è§†é¢‘åˆ†æå™¨ - é«˜ç²¾åº¦æ¨¡å¼"""
    
    def __init__(self):
        # é«˜ç²¾åº¦MediaPipeé…ç½®
        self.mp_face_mesh = mp.solutions.face_mesh
        self.mp_drawing = mp.solutions.drawing_utils
        self.face_mesh = self.mp_face_mesh.FaceMesh(
            static_image_mode=False,
            max_num_faces=1,
            refine_landmarks=True,  # å¯ç”¨ç²¾ç»†åŒ–æé«˜å‡†ç¡®ç‡
            min_detection_confidence=model_config.MEDIAPIPE_CONFIDENCE,
            min_tracking_confidence=model_config.MEDIAPIPE_CONFIDENCE
        )
        
        # å®Œæ•´çš„é¢éƒ¨å…³é”®ç‚¹ç´¢å¼•
        self.face_landmarks_indexes = {
            'nose_tip': 1,
            'chin': 175,
            'left_eye_corner': 33,
            'right_eye_corner': 263,
            'left_iris': 468,
            'right_iris': 473,
            'mouth_left': 61,
            'mouth_right': 291,
            'forehead': 10,
            'left_cheek': 116,
            'right_cheek': 345
        }
        
        # 3Dé¢éƒ¨æ¨¡å‹ç‚¹ (ç”¨äºç²¾ç¡®çš„PnPç®—æ³•)
        self.model_points = np.array([
            (0.0, 0.0, 0.0),          # é¼»å°–
            (0.0, -330.0, -65.0),     # ä¸‹å·´
            (-225.0, 170.0, -135.0),  # å·¦çœ¼è§’
            (225.0, 170.0, -135.0),   # å³çœ¼è§’
            (-150.0, -150.0, -125.0), # å·¦å˜´è§’
            (150.0, -150.0, -125.0)   # å³å˜´è§’
        ], dtype=np.float64)
        
        # æƒ…ç»ªåˆ†æç¼“å­˜ (é€‚åº¦ç¼“å­˜ä¿è¯å‡†ç¡®ç‡)
        self.emotion_cache = {}
        self.emotion_cache_duration = 1.0  # å‡å°‘ç¼“å­˜æ—¶é—´æé«˜å®æ—¶æ€§
        
        # å¸§ä¿å­˜é…ç½®
        self.save_dir = Path("data/analysis_frames")
        self.save_dir.mkdir(parents=True, exist_ok=True)
        
        # æ€§èƒ½ç»Ÿè®¡
        self.stats = {
            'frames_analyzed': 0,
            'emotions_detected': 0,
            'head_poses_calculated': 0,
            'processing_times': [],
            'error_count': 0
        }
        
        logger.info("âœ… ç»Ÿä¸€è§†é¢‘åˆ†æå™¨åˆå§‹åŒ–å®Œæˆ (é«˜ç²¾åº¦æ¨¡å¼)")
    
    def analyze_frame(self, frame: np.ndarray, save_frame: bool = False, 
                     frame_count: int = 0, timestamp: float = 0.0) -> Dict[str, Any]:
        """åˆ†æå•å¸§è§†é¢‘ - é«˜ç²¾åº¦æ¨¡å¼"""
        start_time = time.time()
        self.stats['frames_analyzed'] += 1
        
        if frame is None or frame.size == 0:
            raise ValueError("è¾“å…¥è§†é¢‘å¸§ä¸ºç©ºæˆ–æ— æ•ˆ")
        
        try:
            # é«˜è´¨é‡é¢œè‰²ç©ºé—´è½¬æ¢
            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            h, w = frame.shape[:2]
            
            # MediaPipeé¢éƒ¨æ£€æµ‹
            results = self.face_mesh.process(rgb_frame)
            
            analysis_result = {
                'timestamp': datetime.now().isoformat(),
                'processing_time': 0,
                'face_detected': False,
                'frame_size': {'width': w, 'height': h},
                'analysis_mode': 'high_precision'
            }
            
            if not results.multi_face_landmarks:
                logger.debug("æœªæ£€æµ‹åˆ°äººè„¸")
                analysis_result['error'] = "no_face_detected"
                return analysis_result
            
            face_landmarks = results.multi_face_landmarks[0]
            analysis_result['face_detected'] = True
            
            # é«˜ç²¾åº¦å¤´éƒ¨å§¿æ€åˆ†æ
            head_pose = self._analyze_head_pose_precise(face_landmarks, (w, h))
            if head_pose:
                analysis_result.update(head_pose)
                self.stats['head_poses_calculated'] += 1
            
            # ç²¾ç¡®è§†çº¿æ–¹å‘åˆ†æ
            gaze = self._analyze_gaze_precise(face_landmarks, (w, h))
            analysis_result['gaze_direction'] = gaze
            
            # é«˜è´¨é‡æƒ…ç»ªåˆ†æ
            emotion = self._analyze_emotion_enhanced(frame)
            if emotion:
                analysis_result.update(emotion)
                self.stats['emotions_detected'] += 1
            
            # é¢éƒ¨è¡¨æƒ…ç‰¹å¾åˆ†æ
            facial_features = self._analyze_facial_features(face_landmarks, (w, h))
            analysis_result['facial_features'] = facial_features
            
            # ä¿å­˜å…³é”®å¸§
            if save_frame:
                saved_path = self._save_analysis_frame(
                    frame, frame_count, timestamp, analysis_result
                )
                analysis_result['saved_frame_path'] = saved_path
            
            # è®°å½•å¤„ç†æ—¶é—´
            processing_time = time.time() - start_time
            analysis_result['processing_time'] = round(processing_time * 1000, 2)
            self.stats['processing_times'].append(processing_time)
            
            logger.debug(f"ğŸ¥ é«˜ç²¾åº¦å¸§åˆ†æå®Œæˆ: {processing_time:.3f}s")
            return analysis_result
            
        except Exception as e:
            self.stats['error_count'] += 1
            logger.error(f"âŒ å¸§åˆ†æå¤±è´¥: {e}")
            logger.error(f"ğŸ”§ é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            raise
    
    def _analyze_head_pose_precise(self, landmarks, frame_shape) -> Dict[str, float]:
        """ç²¾ç¡®å¤´éƒ¨å§¿æ€åˆ†æ"""
        try:
            w, h = frame_shape
            
            # æå–é«˜ç²¾åº¦å…³é”®ç‚¹
            landmark_points = []
            key_indices = [1, 175, 33, 263, 61, 291]  # é¼»å°–ã€ä¸‹å·´ã€çœ¼è§’ã€å˜´è§’
            
            for idx in key_indices:
                if idx < len(landmarks.landmark):
                    lm = landmarks.landmark[idx]
                    landmark_points.append([lm.x * w, lm.y * h])
            
            if len(landmark_points) < 6:
                raise Exception("å…³é”®ç‚¹æ•°é‡ä¸è¶³")
            
            image_points = np.array(landmark_points, dtype=np.float64)
            
            # ç²¾ç¡®ç›¸æœºå‚æ•°
            focal_length = max(w, h)  # æ›´å‡†ç¡®çš„ç„¦è·ä¼°ç®—
            center = (w/2, h/2)
            camera_matrix = np.array([
                [focal_length, 0, center[0]],
                [0, focal_length, center[1]],
                [0, 0, 1]
            ], dtype=np.float64)
            
            # è€ƒè™‘é•œå¤´ç•¸å˜
            dist_coeffs = np.array([0.1, -0.2, 0, 0, 0], dtype=np.float64)
            
            # é«˜ç²¾åº¦PnPç®—æ³•
            success, rotation_vector, translation_vector = cv2.solvePnP(
                self.model_points, image_points, camera_matrix, dist_coeffs,
                flags=cv2.SOLVEPNP_ITERATIVE  # ä½¿ç”¨è¿­ä»£ç®—æ³•æé«˜ç²¾åº¦
            )
            
            if not success:
                raise Exception("PnPç®—æ³•æ±‚è§£å¤±è´¥")
            
            # è½¬æ¢ä¸ºæ¬§æ‹‰è§’
            rotation_matrix, _ = cv2.Rodrigues(rotation_vector)
            angles = self._rotation_matrix_to_euler(rotation_matrix)
            
            # è®¡ç®—å¤´éƒ¨ç¨³å®šæ€§
            stability = self._calculate_head_stability(angles)
            
            return {
                'pitch': float(angles[0]),
                'yaw': float(angles[1]),
                'roll': float(angles[2]),
                'head_pose_stability': stability,
                'rotation_magnitude': float(np.linalg.norm(angles))
            }
            
        except Exception as e:
            logger.error(f"âŒ ç²¾ç¡®å¤´éƒ¨å§¿æ€åˆ†æå¤±è´¥: {e}")
            raise
    
    def _analyze_gaze_precise(self, landmarks, frame_shape) -> Dict[str, float]:
        """ç²¾ç¡®è§†çº¿æ–¹å‘åˆ†æ"""
        try:
            w, h = frame_shape
            
            # è·å–è™¹è†œå’Œçœ¼éƒ¨å…³é”®ç‚¹
            if len(landmarks.landmark) > 473:  # ç¡®ä¿æœ‰è™¹è†œæ•°æ®
                left_iris = landmarks.landmark[468]
                right_iris = landmarks.landmark[473]
                left_eye_corner = landmarks.landmark[33]
                right_eye_corner = landmarks.landmark[263]
                
                # è®¡ç®—ç²¾ç¡®è§†çº¿å‘é‡
                left_gaze_x = (left_iris.x - left_eye_corner.x) * w
                left_gaze_y = (left_iris.y - left_eye_corner.y) * h
                right_gaze_x = (right_iris.x - right_eye_corner.x) * w
                right_gaze_y = (right_iris.y - right_eye_corner.y) * h
                
                # åŒçœ¼å¹³å‡
                avg_gaze_x = (left_gaze_x + right_gaze_x) / 2
                avg_gaze_y = (left_gaze_y + right_gaze_y) / 2
                
                # è®¡ç®—è§†çº¿ç¨³å®šæ€§
                gaze_magnitude = np.sqrt(avg_gaze_x**2 + avg_gaze_y**2)
                eye_contact_score = max(0.0, 1.0 - gaze_magnitude / 50.0)
                
                return {
                    'x': float(avg_gaze_x),
                    'y': float(avg_gaze_y),
                    'magnitude': float(gaze_magnitude),
                    'eye_contact_score': float(eye_contact_score)
                }
            else:
                # é™çº§åˆ°åŸºç¡€çœ¼éƒ¨åˆ†æ
                return self._analyze_gaze_basic(landmarks, frame_shape)
                
        except Exception as e:
            logger.error(f"âŒ ç²¾ç¡®è§†çº¿åˆ†æå¤±è´¥: {e}")
            return self._analyze_gaze_basic(landmarks, frame_shape)
    
    def _analyze_gaze_basic(self, landmarks, frame_shape) -> Dict[str, float]:
        """åŸºç¡€è§†çº¿åˆ†æï¼ˆé™çº§æ–¹æ¡ˆï¼‰"""
        w, h = frame_shape
        left_eye = landmarks.landmark[33]
        right_eye = landmarks.landmark[263]
        nose = landmarks.landmark[1]
        
        eye_center_x = (left_eye.x + right_eye.x) / 2
        eye_center_y = (left_eye.y + right_eye.y) / 2
        
        gaze_x = (eye_center_x - nose.x) * 100
        gaze_y = (eye_center_y - nose.y) * 100
        
        return {
            'x': float(gaze_x),
            'y': float(gaze_y),
            'magnitude': float(np.sqrt(gaze_x**2 + gaze_y**2)),
            'eye_contact_score': 0.5  # é»˜è®¤å€¼
        }
    
    def _analyze_emotion_enhanced(self, frame: np.ndarray) -> Optional[Dict[str, Any]]:
        """å¢å¼ºæƒ…ç»ªåˆ†æ"""
        if not DEEPFACE_AVAILABLE:
            logger.error("DeepFaceä¸å¯ç”¨")
            return None
        
        current_time = time.time()
        
        # æ£€æŸ¥ç¼“å­˜
        if 'last_analysis' in self.emotion_cache:
            if current_time - self.emotion_cache['last_analysis'] < self.emotion_cache_duration:
                return self.emotion_cache.get('result')
        
        try:
            # ä½¿ç”¨åŸå§‹åˆ†è¾¨ç‡ä¿è¯å‡†ç¡®ç‡
            # å¦‚æœå›¾åƒå¤ªå¤§åˆ™é€‚åº¦ç¼©æ”¾
            h, w = frame.shape[:2]
            if w > 640:
                scale = 640 / w
                new_w = int(w * scale)
                new_h = int(h * scale)
                resized_frame = cv2.resize(frame, (new_w, new_h))
            else:
                resized_frame = frame
            
            # ä½¿ç”¨é«˜ç²¾åº¦æ£€æµ‹å™¨
            result = DeepFace.analyze(
                resized_frame,
                actions=['emotion', 'age', 'gender'],  # å¤šç»´åº¦åˆ†æ
                detector_backend=model_config.DEEPFACE_BACKEND,
                enforce_detection=False,
                silent=True
            )
            
            if isinstance(result, list):
                result = result[0]
            
            emotions = result.get('emotion', {})
            age = result.get('age', 0)
            gender = result.get('gender', {})
            
            if not emotions:
                raise Exception("DeepFaceè¿”å›ç©ºç»“æœ")
            
            # æ‰¾å‡ºä¸»å¯¼æƒ…ç»ª
            dominant_emotion = max(emotions.items(), key=lambda x: x[1])
            
            # è®¡ç®—æƒ…ç»ªç¨³å®šæ€§
            emotion_variance = np.var(list(emotions.values()))
            emotion_stability = 1.0 / (1.0 + emotion_variance / 1000)
            
            emotion_result = {
                'dominant_emotion': dominant_emotion[0],
                'emotion_confidence': float(dominant_emotion[1] / 100),
                'emotion_distribution': {k: float(v/100) for k, v in emotions.items()},
                'emotion_stability': float(emotion_stability),
                'estimated_age': float(age),
                'gender_prediction': gender
            }
            
            # æ›´æ–°ç¼“å­˜
            self.emotion_cache = {
                'last_analysis': current_time,
                'result': emotion_result
            }
            
            logger.debug(f"âœ… å¢å¼ºæƒ…ç»ªåˆ†æ: {dominant_emotion[0]} ({dominant_emotion[1]:.1f}%)")
            return emotion_result
            
        except Exception as e:
            logger.error(f"âŒ å¢å¼ºæƒ…ç»ªåˆ†æå¤±è´¥: {e}")
            return None
    
    def _analyze_facial_features(self, landmarks, frame_shape) -> Dict[str, float]:
        """åˆ†æé¢éƒ¨ç‰¹å¾"""
        try:
            w, h = frame_shape
            
            # è®¡ç®—é¢éƒ¨å‡ ä½•ç‰¹å¾
            nose = landmarks.landmark[1]
            left_eye = landmarks.landmark[33]
            right_eye = landmarks.landmark[263]
            mouth_left = landmarks.landmark[61]
            mouth_right = landmarks.landmark[291]
            
            # çœ¼éƒ¨å¼€åˆåº¦
            eye_distance = abs(left_eye.y - right_eye.y) * h
            eye_width = abs(left_eye.x - right_eye.x) * w
            eye_aspect_ratio = eye_distance / max(eye_width, 1)
            
            # å˜´éƒ¨å¼€åˆåº¦
            mouth_width = abs(mouth_left.x - mouth_right.x) * w
            mouth_height = abs(mouth_left.y - mouth_right.y) * h
            mouth_aspect_ratio = mouth_height / max(mouth_width, 1)
            
            # é¢éƒ¨å¯¹ç§°æ€§
            face_center_x = nose.x * w
            left_distance = abs(face_center_x - left_eye.x * w)
            right_distance = abs(right_eye.x * w - face_center_x)
            symmetry_score = 1.0 - abs(left_distance - right_distance) / max(left_distance + right_distance, 1)
            
            return {
                'eye_aspect_ratio': float(eye_aspect_ratio),
                'mouth_aspect_ratio': float(mouth_aspect_ratio),
                'facial_symmetry': float(symmetry_score),
                'alertness_score': float(min(1.0, eye_aspect_ratio * 2))
            }
            
        except Exception as e:
            logger.error(f"âŒ é¢éƒ¨ç‰¹å¾åˆ†æå¤±è´¥: {e}")
            return {
                'eye_aspect_ratio': 0.0,
                'mouth_aspect_ratio': 0.0,
                'facial_symmetry': 0.5,
                'alertness_score': 0.5
            }
    
    def _rotation_matrix_to_euler(self, rotation_matrix) -> np.ndarray:
        """æ—‹è½¬çŸ©é˜µè½¬æ¬§æ‹‰è§’"""
        sy = np.sqrt(rotation_matrix[0,0]**2 + rotation_matrix[1,0]**2)
        singular = sy < 1e-6
        
        if not singular:
            x = np.arctan2(rotation_matrix[2,1], rotation_matrix[2,2])
            y = np.arctan2(-rotation_matrix[2,0], sy)
            z = np.arctan2(rotation_matrix[1,0], rotation_matrix[0,0])
        else:
            x = np.arctan2(-rotation_matrix[1,2], rotation_matrix[1,1])
            y = np.arctan2(-rotation_matrix[2,0], sy)
            z = 0
        
        return np.degrees([x, y, z])
    
    def _calculate_head_stability(self, angles: np.ndarray) -> float:
        """è®¡ç®—å¤´éƒ¨ç¨³å®šæ€§"""
        angle_magnitude = np.linalg.norm(angles)
        return float(max(0.0, 1.0 - angle_magnitude / 45.0))  # 45åº¦ä¸ºä¸ç¨³å®šé˜ˆå€¼
    
    def _save_analysis_frame(self, frame: np.ndarray, frame_count: int, 
                           timestamp: float, analysis_result: Dict[str, Any]) -> str:
        """ä¿å­˜åˆ†æå¸§"""
        try:
            timestamp_str = datetime.now().strftime("%Y%m%d_%H%M%S_%f")[:-3]
            filename = f"{timestamp_str}_frame_{frame_count:06d}_enhanced.jpg"
            filepath = self.save_dir / filename
            
            # åœ¨å›¾åƒä¸Šç»˜åˆ¶è¯¦ç»†åˆ†æä¿¡æ¯
            annotated_frame = frame.copy()
            font = cv2.FONT_HERSHEY_SIMPLEX
            font_scale = 0.5
            color = (0, 255, 0)  # ç»¿è‰²
            thickness = 1
            
            y_offset = 20
            # åŸºæœ¬ä¿¡æ¯
            cv2.putText(annotated_frame, f"Frame: {frame_count}", (10, y_offset), 
                       font, font_scale, color, thickness)
            y_offset += 20
            
            # æƒ…ç»ªä¿¡æ¯
            if 'dominant_emotion' in analysis_result:
                emotion = analysis_result['dominant_emotion']
                confidence = analysis_result.get('emotion_confidence', 0)
                cv2.putText(annotated_frame, f"Emotion: {emotion} ({confidence:.2f})", 
                           (10, y_offset), font, font_scale, color, thickness)
                y_offset += 20
            
            # å¤´éƒ¨å§¿æ€
            if 'pitch' in analysis_result:
                pitch = analysis_result['pitch']
                yaw = analysis_result['yaw']
                roll = analysis_result['roll']
                cv2.putText(annotated_frame, f"Pose: P{pitch:.1f} Y{yaw:.1f} R{roll:.1f}", 
                           (10, y_offset), font, font_scale, color, thickness)
            
            cv2.imwrite(str(filepath), annotated_frame)
            
            # ä¿å­˜JSONæ•°æ®
            json_filepath = filepath.with_suffix('.json')
            with open(json_filepath, 'w', encoding='utf-8') as f:
                json.dump({
                    'frame_count': frame_count,
                    'timestamp': timestamp,
                    'analysis_result': analysis_result,
                    'saved_at': datetime.now().isoformat()
                }, f, ensure_ascii=False, indent=2)
            
            return str(filepath)
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜åˆ†æå¸§å¤±è´¥: {e}")
            return ""
    
    def get_performance_stats(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½ç»Ÿè®¡"""
        if self.stats['processing_times']:
            avg_time = np.mean(self.stats['processing_times'])
            fps = 1.0 / avg_time if avg_time > 0 else 0
        else:
            avg_time = 0
            fps = 0
        
        return {
            'frames_analyzed': self.stats['frames_analyzed'],
            'emotions_detected': self.stats['emotions_detected'],
            'head_poses_calculated': self.stats['head_poses_calculated'],
            'error_count': self.stats['error_count'],
            'average_processing_time_ms': round(avg_time * 1000, 2),
            'estimated_fps': round(fps, 2),
            'emotion_detection_rate': (self.stats['emotions_detected'] / 
                                     max(self.stats['frames_analyzed'], 1)),
            'error_rate': (self.stats['error_count'] / 
                          max(self.stats['frames_analyzed'], 1))
        }


class UnifiedAudioAnalyzer:
    """ç»Ÿä¸€éŸ³é¢‘åˆ†æå™¨ - é«˜ç²¾åº¦æ¨¡å¼"""
    
    def __init__(self):
        self.sample_rate = model_config.AUDIO_SAMPLE_RATE  # ä½¿ç”¨é«˜é‡‡æ ·ç‡
        self.analysis_cache = {}
        self.cache_duration = 0.5  # å‡å°‘ç¼“å­˜æ—¶é—´
        
        # æ€§èƒ½ç»Ÿè®¡
        self.stats = {
            'chunks_analyzed': 0,
            'processing_times': [],
            'error_count': 0
        }
        
        logger.info("âœ… ç»Ÿä¸€éŸ³é¢‘åˆ†æå™¨åˆå§‹åŒ–å®Œæˆ (é«˜ç²¾åº¦æ¨¡å¼)")
    
    def analyze_chunk(self, audio_bytes: bytes) -> Dict[str, Any]:
        """åˆ†æéŸ³é¢‘ç‰‡æ®µ - é«˜ç²¾åº¦æ¨¡å¼"""
        start_time = time.time()
        self.stats['chunks_analyzed'] += 1
        
        if not audio_bytes:
            raise ValueError("éŸ³é¢‘æ•°æ®ä¸ºç©º")
        
        try:
            # é«˜è´¨é‡éŸ³é¢‘è½¬æ¢
            audio_data = self._bytes_to_audio_hq(audio_bytes)
            
            analysis_result = {
                'timestamp': datetime.now().isoformat(),
                'processing_time': 0,
                'audio_detected': True,
                'analysis_mode': 'high_precision'
            }
            
            # ç»¼åˆéŸ³é¢‘ç‰¹å¾åˆ†æ
            audio_features = self._analyze_audio_features_comprehensive(audio_data)
            analysis_result.update(audio_features)
            
            # é«˜çº§è¯­éŸ³æƒ…æ„Ÿåˆ†æ
            emotion_result = self._analyze_speech_emotion_advanced(audio_data)
            analysis_result.update(emotion_result)
            
            # è¯­éŸ³è´¨é‡è¯„ä¼°
            quality_metrics = self._assess_speech_quality(audio_data)
            analysis_result['speech_quality'] = quality_metrics
            
            processing_time = time.time() - start_time
            analysis_result['processing_time'] = round(processing_time * 1000, 2)
            self.stats['processing_times'].append(processing_time)
            
            logger.debug(f"ğŸµ é«˜ç²¾åº¦éŸ³é¢‘åˆ†æå®Œæˆ: {processing_time:.3f}s")
            return analysis_result
            
        except Exception as e:
            self.stats['error_count'] += 1
            logger.error(f"âŒ éŸ³é¢‘åˆ†æå¤±è´¥: {e}")
            raise
    
    def _bytes_to_audio_hq(self, audio_bytes: bytes) -> np.ndarray:
        """é«˜è´¨é‡éŸ³é¢‘è½¬æ¢"""
        with tempfile.NamedTemporaryFile(suffix='.webm', delete=False) as temp_file:
            temp_file.write(audio_bytes)
            temp_file.flush()
            
            # ä½¿ç”¨é«˜é‡‡æ ·ç‡åŠ è½½
            y, sr = librosa.load(temp_file.name, sr=self.sample_rate, mono=True)
            
            # éŸ³é¢‘é¢„å¤„ç†
            y = librosa.util.normalize(y)  # æ ‡å‡†åŒ–
            y = librosa.effects.trim(y, top_db=20)[0]  # å»é™¤é™éŸ³
            
            return y
    
    def _analyze_audio_features_comprehensive(self, audio_data: np.ndarray) -> Dict[str, Any]:
        """ç»¼åˆéŸ³é¢‘ç‰¹å¾åˆ†æ"""
        try:
            # åŸºç¡€ç‰¹å¾
            # è¯­é€Ÿåˆ†æ
            tempo, beats = librosa.beat.beat_track(y=audio_data, sr=self.sample_rate)
            speech_rate = tempo * 0.6  # è½¬æ¢ä¸ºè¯­éŸ³èŠ‚å¥
            
            # é«˜ç²¾åº¦éŸ³é«˜åˆ†æ
            f0, voiced_flag, voiced_probs = librosa.pyin(
                audio_data, 
                fmin=librosa.note_to_hz('C2'), 
                fmax=librosa.note_to_hz('C7'),
                frame_length=2048
            )
            
            valid_f0 = f0[voiced_flag & (voiced_probs > 0.8)]  # é«˜ç½®ä¿¡åº¦éŸ³é«˜
            
            if len(valid_f0) > 0:
                pitch_mean = float(np.mean(valid_f0))
                pitch_std = float(np.std(valid_f0))
                pitch_range = float(np.max(valid_f0) - np.min(valid_f0))
            else:
                pitch_mean = pitch_std = pitch_range = 0.0
            
            # éŸ³é‡åˆ†æ
            rms = librosa.feature.rms(y=audio_data, frame_length=2048)[0]
            volume_mean = float(np.mean(librosa.amplitude_to_db(rms)))
            volume_variance = float(np.var(librosa.amplitude_to_db(rms)))
            
            # é¢‘è°±ç‰¹å¾
            spectral_centroids = librosa.feature.spectral_centroid(
                y=audio_data, sr=self.sample_rate)[0]
            spectral_rolloff = librosa.feature.spectral_rolloff(
                y=audio_data, sr=self.sample_rate)[0]
            
            clarity_score = float(np.mean(spectral_centroids) / 4000)  # æ ‡å‡†åŒ–
            brightness = float(np.mean(spectral_rolloff) / 8000)  # äº®åº¦æŒ‡æ ‡
            
            # MFCCç‰¹å¾ (æ¢…å°”å€’è°±ç³»æ•°)
            mfccs = librosa.feature.mfcc(y=audio_data, sr=self.sample_rate, n_mfcc=13)
            mfcc_mean = np.mean(mfccs, axis=1)
            
            return {
                'speech_rate_bpm': float(speech_rate),
                'pitch_mean_hz': pitch_mean,
                'pitch_std_hz': pitch_std,
                'pitch_range_hz': pitch_range,
                'volume_mean_db': volume_mean,
                'volume_variance': volume_variance,
                'clarity_score': min(1.0, clarity_score),
                'brightness_score': min(1.0, brightness),
                'voiced_ratio': float(np.sum(voiced_flag) / len(voiced_flag)),
                'mfcc_features': mfcc_mean.tolist()[:5]  # å‰5ä¸ªMFCCç³»æ•°
            }
            
        except Exception as e:
            logger.error(f"âŒ ç»¼åˆéŸ³é¢‘ç‰¹å¾åˆ†æå¤±è´¥: {e}")
            raise
    
    def _analyze_speech_emotion_advanced(self, audio_data: np.ndarray) -> Dict[str, Any]:
        """é«˜çº§è¯­éŸ³æƒ…æ„Ÿåˆ†æ"""
        try:
            # åŸºäºå¤šç»´ç‰¹å¾çš„æƒ…æ„Ÿåˆ†æ
            rms = librosa.feature.rms(y=audio_data)[0]
            zcr = librosa.feature.zero_crossing_rate(audio_data)[0]
            spectral_centroid = librosa.feature.spectral_centroid(y=audio_data, sr=self.sample_rate)[0]
            
            # ç‰¹å¾ç»Ÿè®¡
            energy = np.mean(rms)
            zcr_mean = np.mean(zcr)
            spectral_mean = np.mean(spectral_centroid)
            
            # æƒ…æ„Ÿæ˜ å°„ (åŸºäºå£°å­¦ç‰¹å¾)
            if energy > 0.6 and spectral_mean > 2000:
                emotion = 'excited'
                confidence = 0.85
            elif energy > 0.4 and zcr_mean > 0.1:
                emotion = 'confident'
                confidence = 0.8
            elif energy < 0.2:
                emotion = 'nervous'
                confidence = 0.75
            elif spectral_mean < 1000:
                emotion = 'sad'
                confidence = 0.7
            else:
                emotion = 'neutral'
                confidence = 0.65
            
            # æƒ…æ„Ÿç¨³å®šæ€§è¯„ä¼°
            energy_variance = np.var(rms)
            stability = 1.0 / (1.0 + energy_variance * 10)
            
            return {
                'speech_emotion': emotion,
                'emotion_confidence': confidence,
                'emotion_stability': float(stability),
                'vocal_energy': float(energy),
                'speech_variability': float(zcr_mean)
            }
            
        except Exception as e:
            logger.error(f"âŒ é«˜çº§è¯­éŸ³æƒ…æ„Ÿåˆ†æå¤±è´¥: {e}")
            return {
                'speech_emotion': 'unknown',
                'emotion_confidence': 0.0,
                'emotion_stability': 0.5,
                'vocal_energy': 0.0,
                'speech_variability': 0.0
            }
    
    def _assess_speech_quality(self, audio_data: np.ndarray) -> Dict[str, float]:
        """è¯„ä¼°è¯­éŸ³è´¨é‡"""
        try:
            # ä¿¡å™ªæ¯”ä¼°ç®—
            signal_power = np.mean(audio_data ** 2)
            noise_power = np.mean((audio_data - np.mean(audio_data)) ** 2) * 0.1
            snr = 10 * np.log10(signal_power / max(noise_power, 1e-10))
            
            # æ¸…æ™°åº¦è¯„ä¼°
            high_freq_energy = np.mean(librosa.feature.spectral_centroid(
                y=audio_data, sr=self.sample_rate)[0])
            clarity = min(1.0, high_freq_energy / 3000)
            
            # ç¨³å®šæ€§è¯„ä¼°
            rms = librosa.feature.rms(y=audio_data)[0]
            stability = 1.0 - (np.std(rms) / max(np.mean(rms), 1e-10))
            
            return {
                'estimated_snr_db': float(max(0, min(40, snr))),
                'clarity_score': float(clarity),
                'volume_stability': float(max(0, stability))
            }
            
        except Exception as e:
            logger.error(f"âŒ è¯­éŸ³è´¨é‡è¯„ä¼°å¤±è´¥: {e}")
            return {
                'estimated_snr_db': 0.0,
                'clarity_score': 0.5,
                'volume_stability': 0.5
            }


class UnifiedMultimodalProcessor:
    """ç»Ÿä¸€å¤šæ¨¡æ€å¤„ç†å™¨ - ä¸»æ§åˆ¶å™¨"""
    
    def __init__(self):
        self.video_analyzer = UnifiedVideoAnalyzer()
        self.audio_analyzer = UnifiedAudioAnalyzer()
        
        # ç»¼åˆæ€§èƒ½ç»Ÿè®¡
        self.performance_stats = {
            'start_time': time.time(),
            'total_video_frames': 0,
            'total_audio_chunks': 0,
            'total_errors': 0,
            'avg_video_time': 0,
            'avg_audio_time': 0
        }
        
        logger.info("âœ… ç»Ÿä¸€å¤šæ¨¡æ€å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ (é«˜ç²¾åº¦æ¨¡å¼)")
    
    def analyze_video_frame(self, frame: np.ndarray, **kwargs) -> Dict[str, Any]:
        """åˆ†æè§†é¢‘å¸§"""
        start_time = time.time()
        
        try:
            result = self.video_analyzer.analyze_frame(frame, **kwargs)
            
            # æ›´æ–°æ€§èƒ½ç»Ÿè®¡
            processing_time = time.time() - start_time
            self.performance_stats['total_video_frames'] += 1
            count = self.performance_stats['total_video_frames']
            self.performance_stats['avg_video_time'] = (
                (self.performance_stats['avg_video_time'] * (count - 1) + processing_time) / count
            )
            
            return result
            
        except Exception as e:
            self.performance_stats['total_errors'] += 1
            logger.error(f"âŒ è§†é¢‘å¸§åˆ†æå¤±è´¥: {e}")
            raise
    
    def analyze_audio_chunk(self, audio_bytes: bytes) -> Dict[str, Any]:
        """åˆ†æéŸ³é¢‘ç‰‡æ®µ"""
        start_time = time.time()
        
        try:
            result = self.audio_analyzer.analyze_chunk(audio_bytes)
            
            # æ›´æ–°æ€§èƒ½ç»Ÿè®¡
            processing_time = time.time() - start_time
            self.performance_stats['total_audio_chunks'] += 1
            count = self.performance_stats['total_audio_chunks']
            self.performance_stats['avg_audio_time'] = (
                (self.performance_stats['avg_audio_time'] * (count - 1) + processing_time) / count
            )
            
            return result
            
        except Exception as e:
            self.performance_stats['total_errors'] += 1
            logger.error(f"âŒ éŸ³é¢‘ç‰‡æ®µåˆ†æå¤±è´¥: {e}")
            raise
    
    def get_comprehensive_stats(self) -> Dict[str, Any]:
        """è·å–ç»¼åˆæ€§èƒ½ç»Ÿè®¡"""
        runtime = time.time() - self.performance_stats['start_time']
        
        video_stats = self.video_analyzer.get_performance_stats()
        
        return {
            'runtime_seconds': round(runtime, 2),
            'video_analysis': video_stats,
            'audio_analysis': {
                'chunks_analyzed': self.performance_stats['total_audio_chunks'],
                'average_processing_time_ms': round(self.performance_stats['avg_audio_time'] * 1000, 2)
            },
            'overall_error_rate': (self.performance_stats['total_errors'] / 
                                 max(self.performance_stats['total_video_frames'] + 
                                     self.performance_stats['total_audio_chunks'], 1)),
            'deepface_status': 'available' if DEEPFACE_AVAILABLE else 'unavailable'
        }


# å…¨å±€å®ä¾‹
def create_unified_processor() -> UnifiedMultimodalProcessor:
    """åˆ›å»ºç»Ÿä¸€å¤šæ¨¡æ€å¤„ç†å™¨å®ä¾‹"""
    return UnifiedMultimodalProcessor()


# å¯¼å‡º
__all__ = ['UnifiedMultimodalProcessor', 'create_unified_processor'] 