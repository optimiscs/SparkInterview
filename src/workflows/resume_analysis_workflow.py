"""
åŸºäºLangGraphçš„æ™ºèƒ½ç®€å†åˆ†æå·¥ä½œæµ
é‡æ„åŸæœ‰çš„å•ä½“åˆ†æå‡½æ•°ä¸ºæ¨¡å—åŒ–ã€å¹¶è¡ŒåŒ–çš„çŠ¶æ€å›¾
"""
import json
import re
import logging
from datetime import datetime
from typing import Dict, Any, List, Optional, TypedDict
from pathlib import Path

# LangGraph imports
try:
    from langgraph.graph import StateGraph, END, START
    from langgraph.graph.message import add_messages
    LANGGRAPH_AVAILABLE = True
except ImportError:
    LANGGRAPH_AVAILABLE = False
    # é™çº§å®ç°
    class StateGraph:
        def __init__(self, state_type): pass
        def add_node(self, name, func): pass
        def add_edge(self, start, end): pass
        def add_conditional_edges(self, start, condition, mapping): pass
        def compile(self, **kwargs): return None
    START = "START"
    END = "END"

# LangChain imports
from langchain_core.messages import SystemMessage

# æœ¬åœ°imports
from src.models.spark_client import create_spark_model

logger = logging.getLogger(__name__)


class ResumeAnalysisState(TypedDict):
    """ç®€å†AIåˆ†æå›¾çš„çŠ¶æ€"""
    # è¾“å…¥æ•°æ®
    resume_id: str
    resume_data: Dict[str, Any]
    jd_content: Optional[str]
    analysis_id: str
    
    # æ¯ä¸ªåˆ†æèŠ‚ç‚¹çš„ç»“æœ
    jd_match_result: Optional[Dict[str, Any]]
    star_principle_result: Optional[Dict[str, Any]]
    health_scan_result: Optional[Dict[str, Any]]
    
    # å®ŒæˆçŠ¶æ€è¿½è¸ª
    jd_completed: bool
    star_completed: bool 
    health_completed: bool
    
    # æœ€ç»ˆæ±‡æ€»ç»“æœ
    final_analysis_report: Optional[Dict[str, Any]]
    
    # é”™è¯¯å¤„ç†
    error_message: Optional[str]
    success: bool


class ResumeAnalysisWorkflow:
    """åŸºäºLangGraphçš„ç®€å†åˆ†æå·¥ä½œæµ"""
    
    def __init__(self):
        """åˆå§‹åŒ–å·¥ä½œæµ"""
        self.spark_model = create_spark_model(model_type="chat", temperature=0.7)
        self.workflow = self._build_workflow()
        
        if LANGGRAPH_AVAILABLE:
            self.app = self.workflow.compile()
            logger.info("âœ… LangGraphç®€å†åˆ†æå·¥ä½œæµåˆå§‹åŒ–æˆåŠŸ")
        else:
            self.app = None
            logger.warning("âš ï¸ LangGraphä¸å¯ç”¨ï¼Œä½¿ç”¨é™çº§æ¨¡å¼")
    
    def _build_workflow(self) -> StateGraph:
        """æ„å»ºLangGraphå·¥ä½œæµ"""
        workflow = StateGraph(ResumeAnalysisState)
        
        # æ·»åŠ å¹¶è¡Œåˆ†æèŠ‚ç‚¹
        workflow.add_node("jd_matching", self._jd_matching_node)
        workflow.add_node("star_principle", self._star_principle_node)
        workflow.add_node("health_scan", self._health_scan_node)
        
        # æ·»åŠ æ±‡æ€»èŠ‚ç‚¹
        workflow.add_node("compile_report", self._compile_report_node)
        workflow.add_node("save_results", self._save_results_node)
        
        # è®¾ç½®å¹¶è¡Œå…¥å£
        workflow.add_edge(START, "jd_matching")
        workflow.add_edge(START, "star_principle") 
        workflow.add_edge(START, "health_scan")
        
        # ä¿®å¤ï¼šç›´æ¥è¿æ¥åˆ°æ±‡æ€»èŠ‚ç‚¹ï¼Œè®©æ‰€æœ‰å¹¶è¡Œä»»åŠ¡éƒ½æŒ‡å‘æ±‡æ€»
        workflow.add_edge("jd_matching", "compile_report")
        workflow.add_edge("star_principle", "compile_report") 
        workflow.add_edge("health_scan", "compile_report")
        
        # æ±‡æ€»åä¿å­˜ç»“æœ
        workflow.add_edge("compile_report", "save_results")
        workflow.add_edge("save_results", END)
        
        return workflow
    
    # _check_all_completed æ–¹æ³•å·²ç§»é™¤ - ä½¿ç”¨ç›´æ¥è¾¹è¿æ¥å®ç°å¹¶è¡Œç­‰å¾…
    
    async def _run_analysis_node(self, prompt: str, fallback_data: Dict[str, Any], 
                                node_name: str) -> Dict[str, Any]:
        """é€šç”¨çš„AIåˆ†æèŠ‚ç‚¹æ‰§è¡Œå™¨ - æ¶ˆé™¤ä»£ç å†—ä½™"""
        try:
            logger.info(f"ğŸ¤– æ‰§è¡Œ{node_name}åˆ†æ...")
            
            # 1. è°ƒç”¨å¤§æ¨¡å‹
            messages = [SystemMessage(content=prompt)]
            result = await self.spark_model._agenerate(messages)
            response = result.generations[0].message.content
            
            # 2. è§£æJSON
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                parsed_result = json.loads(json_match.group())
                logger.info(f"âœ… {node_name}åˆ†ææˆåŠŸ")
                return parsed_result
            else:
                logger.warning(f"âš ï¸ {node_name}æ— æ³•è§£æJSONï¼Œä½¿ç”¨é™çº§æ•°æ®")
                return fallback_data
                
        except json.JSONDecodeError as e:
            logger.error(f"âŒ {node_name}JSONè§£æå¤±è´¥: {e}")
            return fallback_data
        except Exception as e:
            logger.error(f"âŒ {node_name}æ‰§è¡Œå¤±è´¥: {e}")
            return fallback_data
    
    async def _jd_matching_node(self, state: ResumeAnalysisState) -> Dict[str, Any]:
        """JDåŒ¹é…åˆ†æèŠ‚ç‚¹"""
        resume_data = state["resume_data"]
        jd_content = state.get("jd_content", "")
        
        # æ„å»ºJDåŒ¹é…æç¤º
        prompt = self._build_jd_matching_prompt(resume_data, jd_content)
        
        # é™çº§æ•°æ®
        fallback_data = {
            "overall_match": 82,
            "skill_match": 85,
            "experience_match": 75,
            "project_relevance": 88,
            "education_match": 90,
            "strengths": ["æŠ€æœ¯åŸºç¡€æ‰å®", "é¡¹ç›®ç»éªŒä¸°å¯Œ"],
            "gaps": ["éœ€è¦åŠ å¼ºç³»ç»Ÿæ¶æ„èƒ½åŠ›"],
            "suggestions": ["å¤šå‚ä¸å¼€æºé¡¹ç›®", "åŠ å¼ºç³»ç»Ÿè®¾è®¡å­¦ä¹ "],
            "analysis_timestamp": datetime.now().isoformat()
        }
        
        # æ‰§è¡Œåˆ†æ
        result = await self._run_analysis_node(prompt, fallback_data, "JDåŒ¹é…")
        
        # æ›´æ–°çŠ¶æ€ - ç§»é™¤å®Œæˆæ ‡å¿—ï¼ŒLangGraphè‡ªåŠ¨ç®¡ç†
        return {
            "jd_match_result": result
        }
    
    async def _star_principle_node(self, state: ResumeAnalysisState) -> Dict[str, Any]:
        """STARåŸåˆ™æ£€æµ‹èŠ‚ç‚¹"""
        resume_data = state["resume_data"]
        
        # æ„å»ºSTARæ£€æµ‹æç¤º
        prompt = self._build_star_principle_prompt(resume_data)
        
        # é™çº§æ•°æ®
        fallback_data = {
            "overall_score": 76,
            "star_items": [
                {
                    "name": "é¡¹ç›®ç»å†ç¤ºä¾‹",
                    "situation_score": 85,
                    "task_score": 90,
                    "action_score": 88,
                    "result_score": 65,
                    "overall_score": 82,
                    "strengths": ["æŠ€æœ¯èƒŒæ™¯æ¸…æ™°", "ä»»åŠ¡ç›®æ ‡æ˜ç¡®"],
                    "suggestions": ["éœ€è¦è¡¥å……é‡åŒ–çš„é¡¹ç›®æˆæœ"]
                }
            ],
            "analysis_timestamp": datetime.now().isoformat()
        }
        
        # æ‰§è¡Œåˆ†æ
        result = await self._run_analysis_node(prompt, fallback_data, "STARåŸåˆ™")
        
        # æ›´æ–°çŠ¶æ€ - ç§»é™¤å®Œæˆæ ‡å¿—ï¼ŒLangGraphè‡ªåŠ¨ç®¡ç†
        return {
            "star_principle_result": result
        }
    
    async def _health_scan_node(self, state: ResumeAnalysisState) -> Dict[str, Any]:
        """ç®€å†å¥åº·åº¦æ‰«æèŠ‚ç‚¹"""
        resume_data = state["resume_data"]
        
        # æ„å»ºå¥åº·åº¦æ‰«ææç¤º
        prompt = self._build_health_scan_prompt(resume_data)
        
        # é™çº§æ•°æ®
        fallback_data = {
            "overall_health": 88,
            "health_checks": [
                {
                    "category": "æ ¼å¼è§„èŒƒ",
                    "score": 95,
                    "status": "é€šè¿‡",
                    "details": "ç®€å†æ ¼å¼è§„èŒƒï¼Œç»“æ„æ¸…æ™°"
                },
                {
                    "category": "å†…å®¹å®Œæ•´æ€§", 
                    "score": 85,
                    "status": "è‰¯å¥½",
                    "details": "åŸºç¡€ä¿¡æ¯å®Œæ•´ï¼Œé¡¹ç›®æè¿°è¯¦ç»†"
                }
            ],
            "analysis_timestamp": datetime.now().isoformat()
        }
        
        # æ‰§è¡Œåˆ†æ
        result = await self._run_analysis_node(prompt, fallback_data, "å¥åº·åº¦æ‰«æ")
        
        # æ›´æ–°çŠ¶æ€ - ç§»é™¤å®Œæˆæ ‡å¿—ï¼ŒLangGraphè‡ªåŠ¨ç®¡ç†
        return {
            "health_scan_result": result
        }
    
    async def _compile_report_node(self, state: ResumeAnalysisState) -> Dict[str, Any]:
        """æ±‡æ€»æŠ¥å‘ŠèŠ‚ç‚¹ - LangGraphè‡ªåŠ¨ç­‰å¾…æ‰€æœ‰å¹¶è¡ŒèŠ‚ç‚¹å®Œæˆ"""
        logger.info("ğŸ“Š å¼€å§‹æ±‡æ€»åˆ†ææŠ¥å‘Š...")
        
        try:
            # éªŒè¯æ‰€æœ‰åˆ†æç»“æœéƒ½å·²å®Œæˆ
            jd_result = state.get("jd_match_result")
            star_result = state.get("star_principle_result") 
            health_result = state.get("health_scan_result")
            
            if not all([jd_result, star_result, health_result]):
                missing = []
                if not jd_result: missing.append("JDåŒ¹é…")
                if not star_result: missing.append("STARåŸåˆ™")
                if not health_result: missing.append("å¥åº·åº¦æ‰«æ")
                raise Exception(f"åˆ†æç»“æœä¸å®Œæ•´ï¼Œç¼ºå°‘: {', '.join(missing)}")
            
            # æ±‡æ€»æ‰€æœ‰åˆ†æç»“æœ
            final_report = {
                "resume_id": state["resume_id"],
                "analysis_id": state["analysis_id"],
                "analysis_timestamp": datetime.now().isoformat(),
                "analysis_version": "v2.0_langgraph_fixed",
                "status": "completed",
                
                # å„é¡¹åˆ†æç»“æœ
                "jd_matching": jd_result,
                "star_principle": star_result,
                "health_scan": health_result,
                
                # ç»¼åˆè¯„åˆ†
                "overall_score": self._calculate_overall_score(state),
                
                # å…ƒæ•°æ®
                "metadata": {
                    "analysis_engine": "LangGraph + æ˜Ÿç«å¤§æ¨¡å‹",
                    "parallel_processing": True,
                    "jd_provided": bool(state.get("jd_content")),
                    "processing_time": "å¹¶è¡Œå¤„ç†",
                    "workflow_type": "direct_edge_parallel"
                }
            }
            
            logger.info(f"âœ… åˆ†ææŠ¥å‘Šæ±‡æ€»å®Œæˆ - ç»¼åˆè¯„åˆ†: {final_report['overall_score']}")
            
            return {
                "final_analysis_report": final_report,
                "success": True
            }
            
        except Exception as e:
            logger.error(f"âŒ æ±‡æ€»æŠ¥å‘Šå¤±è´¥: {e}")
            return {
                "error_message": f"æ±‡æ€»æŠ¥å‘Šå¤±è´¥: {str(e)}",
                "success": False
            }
    
    async def _save_results_node(self, state: ResumeAnalysisState) -> Dict[str, Any]:
        """ä¿å­˜ç»“æœèŠ‚ç‚¹"""
        logger.info("ğŸ’¾ ä¿å­˜åˆ†æç»“æœ...")
        
        try:
            if not state.get("final_analysis_report"):
                raise Exception("æ²¡æœ‰æ‰¾åˆ°æœ€ç»ˆåˆ†ææŠ¥å‘Š")
            
            # ä¿å­˜åˆ°æ–‡ä»¶
            analysis_dir = Path("data/resume_analysis")
            analysis_dir.mkdir(parents=True, exist_ok=True)
            
            analysis_file = analysis_dir / f"{state['analysis_id']}.json"
            
            with open(analysis_file, 'w', encoding='utf-8') as f:
                json.dump(state["final_analysis_report"], f, ensure_ascii=False, indent=2)
            
            logger.info(f"âœ… åˆ†æç»“æœå·²ä¿å­˜: {analysis_file}")
            
            return {"success": True}
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜ç»“æœå¤±è´¥: {e}")
            return {
                "error_message": f"ä¿å­˜ç»“æœå¤±è´¥: {str(e)}",
                "success": False
            }
    
    def _calculate_overall_score(self, state: ResumeAnalysisState) -> float:
        """è®¡ç®—ç»¼åˆè¯„åˆ†"""
        try:
            scores = []
            
            # JDåŒ¹é…è¯„åˆ†
            jd_result = state.get("jd_match_result", {})
            if jd_result.get("overall_match"):
                scores.append(jd_result["overall_match"])
            
            # STARåŸåˆ™è¯„åˆ†
            star_result = state.get("star_principle_result", {})
            if star_result.get("overall_score"):
                scores.append(star_result["overall_score"])
            
            # å¥åº·åº¦è¯„åˆ†
            health_result = state.get("health_scan_result", {})
            if health_result.get("overall_health"):
                scores.append(health_result["overall_health"])
            
            if scores:
                return round(sum(scores) / len(scores), 2)
            else:
                return 75.0  # é»˜è®¤åˆ†æ•°
                
        except Exception as e:
            logger.error(f"è®¡ç®—ç»¼åˆè¯„åˆ†å¤±è´¥: {e}")
            return 75.0
    
    def _build_jd_matching_prompt(self, resume_data: Dict, jd_content: str) -> str:
        """æ„å»ºJDåŒ¹é…åˆ†ææç¤º"""
        # æå–ç®€å†å…³é”®ä¿¡æ¯
        basic_info = resume_data.get("basic_info", {})
        projects = resume_data.get("projects", [])
        skills = resume_data.get("skills", {})
        education = resume_data.get("education", {})
        
        prompt = f"""ä½ æ˜¯ä¸€åèµ„æ·±çš„HRä¸“å®¶ï¼Œè¯·åˆ†æç®€å†ä¸èŒä½æè¿°çš„åŒ¹é…åº¦ã€‚

ç®€å†ä¿¡æ¯ï¼š
- åŸºæœ¬ä¿¡æ¯ï¼š{json.dumps(basic_info, ensure_ascii=False)}
- é¡¹ç›®ç»å†ï¼š{json.dumps(projects, ensure_ascii=False)}
- æŠ€èƒ½æ¸…å•ï¼š{json.dumps(skills, ensure_ascii=False)}
- æ•™è‚²èƒŒæ™¯ï¼š{json.dumps(education, ensure_ascii=False)}

èŒä½æè¿°ï¼š
{jd_content if jd_content else "æœªæä¾›å…·ä½“èŒä½æè¿°ï¼Œè¯·åŸºäºç®€å†å†…å®¹è¿›è¡Œä¸€èˆ¬æ€§è¯„ä¼°"}

è¯·ä»ä»¥ä¸‹ç»´åº¦è¿›è¡Œåˆ†æå¹¶è¿”å›JSONæ ¼å¼ç»“æœï¼š
1. æŠ€èƒ½åŒ¹é…åº¦ï¼šæŠ€èƒ½ä¸èŒä½è¦æ±‚çš„åŒ¹é…ç¨‹åº¦(0-100åˆ†)
2. ç»éªŒåŒ¹é…åº¦ï¼šå·¥ä½œ/é¡¹ç›®ç»éªŒçš„åŒ¹é…ç¨‹åº¦(0-100åˆ†)
3. é¡¹ç›®ç›¸å…³æ€§ï¼šé¡¹ç›®ç»å†ä¸èŒä½çš„ç›¸å…³ç¨‹åº¦(0-100åˆ†)
4. æ•™è‚²åŒ¹é…åº¦ï¼šæ•™è‚²èƒŒæ™¯çš„åŒ¹é…ç¨‹åº¦(0-100åˆ†)
5. ç»¼åˆåŒ¹é…åº¦ï¼šæ•´ä½“åŒ¹é…åº¦è¯„åˆ†(0-100åˆ†)
6. æ ¸å¿ƒä¼˜åŠ¿ï¼šå€™é€‰äººçš„ä¸»è¦ä¼˜åŠ¿ç‚¹
7. æŠ€èƒ½ç¼ºå£ï¼šéœ€è¦æå‡çš„æŠ€èƒ½é¢†åŸŸ
8. æ”¹è¿›å»ºè®®ï¼šå…·ä½“çš„å‘å±•å»ºè®®

è¯·è¿”å›ä»¥ä¸‹JSONæ ¼å¼ï¼š
{{
    "overall_match": ç»¼åˆåŒ¹é…åº¦åˆ†æ•°,
    "skill_match": æŠ€èƒ½åŒ¹é…åº¦åˆ†æ•°,
    "experience_match": ç»éªŒåŒ¹é…åº¦åˆ†æ•°,
    "project_relevance": é¡¹ç›®ç›¸å…³æ€§åˆ†æ•°,
    "education_match": æ•™è‚²åŒ¹é…åº¦åˆ†æ•°,
    "strengths": ["ä¼˜åŠ¿1", "ä¼˜åŠ¿2"],
    "gaps": ["ç¼ºå£1", "ç¼ºå£2"],
    "suggestions": ["å»ºè®®1", "å»ºè®®2"],
    "analysis_timestamp": "{datetime.now().isoformat()}"
}}"""
        
        return prompt
    
    def _build_star_principle_prompt(self, resume_data: Dict) -> str:
        """æ„å»ºSTARåŸåˆ™æ£€æµ‹æç¤º"""
        projects = resume_data.get("projects", [])
        internship = resume_data.get("internship", [])
        
        prompt = f"""ä½ æ˜¯ä¸€åä¸“ä¸šçš„ç®€å†ä¼˜åŒ–ä¸“å®¶ï¼Œè¯·åˆ†æé¡¹ç›®/å®ä¹ ç»å†æ˜¯å¦ç¬¦åˆSTARåŸåˆ™ã€‚

é¡¹ç›®ç»å†ï¼š
{json.dumps(projects, ensure_ascii=False, indent=2)}

å®ä¹ ç»å†ï¼š
{json.dumps(internship, ensure_ascii=False, indent=2)}

STARåŸåˆ™è¯„åˆ†æ ‡å‡†ï¼š
- S (Situation æƒ…å¢ƒ): 0-100åˆ†
  * 80-100åˆ†ï¼šèƒŒæ™¯æè¿°æ¸…æ™°å®Œæ•´ï¼Œé¡¹ç›®èƒŒæ™¯ã€å›¢é˜Ÿè§„æ¨¡ã€æŠ€æœ¯ç¯å¢ƒç­‰ä¿¡æ¯æ˜ç¡®
  * 60-79åˆ†ï¼šèƒŒæ™¯æè¿°åŸºæœ¬æ¸…æ™°ï¼Œä½†ç¼ºå°‘éƒ¨åˆ†å…³é”®ä¿¡æ¯
  * 0-59åˆ†ï¼šèƒŒæ™¯æè¿°æ¨¡ç³Šæˆ–ç¼ºå¤±

- T (Task ä»»åŠ¡): 0-100åˆ†
  * 80-100åˆ†ï¼šä»»åŠ¡ç›®æ ‡æ˜ç¡®å…·ä½“ï¼Œé¢„æœŸæˆæœæ¸…æ™°ï¼Œé¢ä¸´çš„æŒ‘æˆ˜æè¿°è¯¦ç»†
  * 60-79åˆ†ï¼šä»»åŠ¡ç›®æ ‡è¾ƒä¸ºæ˜ç¡®ï¼Œä½†ç¼ºå°‘å…·ä½“ç»†èŠ‚
  * 0-59åˆ†ï¼šä»»åŠ¡ç›®æ ‡æ¨¡ç³Šæˆ–ä¸æ¸…æ™°

- A (Action è¡ŒåŠ¨): 0-100åˆ†
  * 80-100åˆ†ï¼šå…·ä½“è¡ŒåŠ¨æ­¥éª¤è¯¦ç»†ï¼ŒæŠ€æœ¯æ–¹æ¡ˆæ˜ç¡®ï¼Œä¸ªäººè´¡çŒ®çªå‡º
  * 60-79åˆ†ï¼šè¡ŒåŠ¨æè¿°è¾ƒä¸ºè¯¦ç»†ï¼Œä½†ç¼ºå°‘æŠ€æœ¯ç»†èŠ‚æˆ–ä¸ªäººè´¡çŒ®
  * 0-59åˆ†ï¼šè¡ŒåŠ¨æè¿°æ¨¡ç³Šæˆ–è¿‡äºç®€å•

- R (Result ç»“æœ): 0-100åˆ†
  * 80-100åˆ†ï¼šç»“æœé‡åŒ–å…·ä½“ï¼Œæœ‰æ˜ç¡®çš„æ•°æ®æ”¯æ’‘ï¼Œä¸šåŠ¡ä»·å€¼æ¸…æ™°
  * 60-79åˆ†ï¼šç»“æœæè¿°ç›¸å¯¹å…·ä½“ï¼Œæœ‰ä¸€å®šé‡åŒ–ï¼Œä½†ä¸å¤Ÿè¯¦ç»†
  * 0-59åˆ†ï¼šç»“æœæè¿°æ¨¡ç³Šï¼Œç¼ºä¹é‡åŒ–æ•°æ®

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›åˆ†æç»“æœï¼š
{{
    "overall_score": æ‰€æœ‰é¡¹ç›®STARè¯„åˆ†çš„å¹³å‡å€¼(æ•´æ•°),
    "star_items": [
        {{
            "name": "é¡¹ç›®åç§°",
            "situation_score": Sç»´åº¦è¯„åˆ†(æ•´æ•°ï¼Œ0-100),
            "task_score": Tç»´åº¦è¯„åˆ†(æ•´æ•°ï¼Œ0-100),
            "action_score": Aç»´åº¦è¯„åˆ†(æ•´æ•°ï¼Œ0-100),
            "result_score": Rç»´åº¦è¯„åˆ†(æ•´æ•°ï¼Œ0-100),
            "overall_score": è¯¥é¡¹ç›®å››ä¸ªç»´åº¦çš„å¹³å‡åˆ†(æ•´æ•°),
            "suggestions": ["å…·ä½“çš„æ”¹è¿›å»ºè®®1", "å…·ä½“çš„æ”¹è¿›å»ºè®®2"]
        }}
    ],
    "improvement_suggestions": [
        "æ•´ä½“ä¼˜åŒ–å»ºè®®1ï¼šå¦‚ä½•æå‡é¡¹ç›®æè¿°çš„STARå®Œæ•´æ€§",
        "æ•´ä½“ä¼˜åŒ–å»ºè®®2ï¼šå¦‚ä½•å¢å¼ºé¡¹ç›®æˆæœçš„é‡åŒ–è¡¨è¾¾",
        "æ•´ä½“ä¼˜åŒ–å»ºè®®3ï¼šå¦‚ä½•çªå‡ºä¸ªäººè´¡çŒ®å’ŒæŠ€æœ¯èƒ½åŠ›"
    ],
    "analysis_timestamp": "{datetime.now().isoformat()}"
}}

æ³¨æ„ï¼š
1. æ‰€æœ‰è¯„åˆ†éƒ½å¿…é¡»æ˜¯0-100çš„æ•´æ•°
2. æ¯ä¸ªé¡¹ç›®éƒ½å¿…é¡»åŒ…å«æ‰€æœ‰å››ä¸ªç»´åº¦çš„è¯„åˆ†
3. å»ºè®®è¦å…·ä½“å¯æ“ä½œï¼Œé’ˆå¯¹æ€§å¼º
4. å¦‚æœæ²¡æœ‰é¡¹ç›®ç»å†ï¼Œè¿”å›ç©ºçš„star_itemsæ•°ç»„ä½†ä¿ç•™ç»“æ„"""
        
        return prompt
    
    def _build_health_scan_prompt(self, resume_data: Dict) -> str:
        """æ„å»ºå¥åº·åº¦æ‰«ææç¤º"""
        prompt = f"""ä½ æ˜¯ä¸€åç®€å†å®¡æŸ¥ä¸“å®¶ï¼Œè¯·å¯¹ç®€å†è¿›è¡Œå…¨é¢çš„å¥åº·åº¦æ‰«æã€‚

ç®€å†æ•°æ®ï¼š
{json.dumps(resume_data, ensure_ascii=False, indent=2)}

è¯·ä»ä»¥ä¸‹ç»´åº¦è¯„ä¼°ç®€å†è´¨é‡(0-100åˆ†)ï¼š
1. æ ¼å¼è§„èŒƒï¼šæ’ç‰ˆã€ç»“æ„ã€å­—ä½“ç­‰
2. å†…å®¹å®Œæ•´æ€§ï¼šå„ä¸ªæ¿å—æ˜¯å¦é½å…¨
3. ä¿¡æ¯å‡†ç¡®æ€§ï¼šå†…å®¹çš„çœŸå®æ€§å’Œä¸€è‡´æ€§
4. æŠ€èƒ½åŒ¹é…åº¦ï¼šæŠ€èƒ½ä¸ç›®æ ‡å²—ä½çš„åŒ¹é…ç¨‹åº¦
5. é¡¹ç›®è´¨é‡ï¼šé¡¹ç›®çš„æŠ€æœ¯å«é‡å’Œå•†ä¸šä»·å€¼
6. æè¿°è´¨é‡ï¼šå†…å®¹çš„å…·ä½“æ€§å’Œå¯é‡åŒ–ç¨‹åº¦

è¿”å›JSONæ ¼å¼ï¼š
{{
    "overall_health": æ•´ä½“å¥åº·åº¦è¯„åˆ†,
    "health_checks": [
        {{
            "category": "æ£€æŸ¥ç±»åˆ«",
            "score": è¯¥ç±»åˆ«è¯„åˆ†,
            "status": "é€šè¿‡/è‰¯å¥½/å¾…æ”¹è¿›/ä¸é€šè¿‡",
            "details": "å…·ä½“è¯´æ˜",
            "suggestions": ["æ”¹è¿›å»ºè®®"]
        }}
    ],
    "analysis_timestamp": "{datetime.now().isoformat()}"
}}"""
        
        return prompt
    
    async def analyze_jd_matching(self, resume_id: str, resume_data: Dict[str, Any], 
                                 jd_content: str, analysis_id: str) -> Dict[str, Any]:
        """æ‰§è¡ŒJDåŒ¹é…åˆ†æï¼ˆå•ç‹¬åˆ†æï¼‰"""
        try:
            logger.info(f"ğŸ” å¯åŠ¨JDåŒ¹é…åˆ†æ: {analysis_id}")
            
            # æ„å»ºJDåŒ¹é…æç¤º
            prompt = self._build_jd_matching_prompt(resume_data, jd_content)
            
            # é™çº§æ•°æ®
            fallback_data = {
                "overall_match": 82,
                "skill_match": 85,
                "experience_match": 75,
                "project_relevance": 88,
                "education_match": 90,
                "strengths": ["æŠ€æœ¯åŸºç¡€æ‰å®", "é¡¹ç›®ç»éªŒä¸°å¯Œ"],
                "gaps": ["éœ€è¦åŠ å¼ºç³»ç»Ÿæ¶æ„èƒ½åŠ›"],
                "suggestions": ["å¤šå‚ä¸å¼€æºé¡¹ç›®", "åŠ å¼ºç³»ç»Ÿè®¾è®¡å­¦ä¹ "],
                "match_details": {
                    "æŠ€æœ¯èƒ½åŠ›": 85,
                    "é¡¹ç›®ç»éªŒ": 75,
                    "æ•™è‚²èƒŒæ™¯": 90,
                    "å·¥ä½œç»éªŒ": 45,
                    "è½¯æŠ€èƒ½": 88
                },
                "analysis_timestamp": datetime.now().isoformat()
            }
            
            # æ‰§è¡Œåˆ†æ
            result = await self._run_analysis_node(prompt, fallback_data, "JDåŒ¹é…")
            
            return {
                "success": True,
                "result": result
            }
            
        except Exception as e:
            logger.error(f"âŒ JDåŒ¹é…åˆ†æå¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e)
            }
    
    async def analyze_star_principle(self, resume_id: str, resume_data: Dict[str, Any], 
                                   analysis_id: str) -> Dict[str, Any]:
        """æ‰§è¡ŒSTARåŸåˆ™æ£€æµ‹ï¼ˆå•ç‹¬åˆ†æï¼‰"""
        try:
            logger.info(f"â­ å¯åŠ¨STARåŸåˆ™æ£€æµ‹: {analysis_id}")
            
            # æ„å»ºSTARæ£€æµ‹æç¤º
            prompt = self._build_star_principle_prompt(resume_data)
            
            # é™çº§æ•°æ®ï¼ˆåŒ¹é…å‰ç«¯æ¸²æŸ“æ ¼å¼ï¼‰
            fallback_data = {
                "overall_score": 76,
                "star_items": [
                    {
                        "name": "æ™ºèƒ½å­¦ä¹ ç®¡ç†ç³»ç»Ÿ",
                        "situation_score": 85,
                        "task_score": 90,
                        "action_score": 88,
                        "result_score": 65,
                        "overall_score": 82,
                        "suggestions": ["éœ€è¦è¡¥å……é‡åŒ–çš„é¡¹ç›®æˆæœ", "å¢åŠ å…·ä½“çš„æŠ€æœ¯éš¾ç‚¹æè¿°"]
                    },
                    {
                        "name": "ç”µå•†æ•°æ®åˆ†æå¹³å°", 
                        "situation_score": 75,
                        "task_score": 80,
                        "action_score": 85,
                        "result_score": 70,
                        "overall_score": 78,
                        "suggestions": ["å¼ºåŒ–æ•°æ®åˆ†æç»“æœçš„å•†ä¸šä»·å€¼", "æ·»åŠ æ€§èƒ½ä¼˜åŒ–çš„å…·ä½“æ•°æ®"]
                    }
                ],
                "improvement_suggestions": [
                    "æ•´ä½“ä¼˜åŒ–å»ºè®®1ï¼šå¢åŠ é¡¹ç›®ç»“æœçš„é‡åŒ–æ•°æ®ï¼Œå¦‚'æ€§èƒ½æå‡30%'ã€'ç”¨æˆ·æ»¡æ„åº¦è¾¾95%'",
                    "æ•´ä½“ä¼˜åŒ–å»ºè®®2ï¼šè¯¦ç»†æè¿°è§£å†³é—®é¢˜çš„å…·ä½“æŠ€æœ¯æ–¹æ¡ˆå’Œå®æ–½æ­¥éª¤",
                    "æ•´ä½“ä¼˜åŒ–å»ºè®®3ï¼šçªå‡ºä¸ªäººåœ¨å›¢é˜Ÿä¸­çš„å…³é”®ä½œç”¨å’Œæ ¸å¿ƒè´¡çŒ®"
                ],
                "analysis_timestamp": datetime.now().isoformat()
            }
            
            # æ‰§è¡Œåˆ†æ
            result = await self._run_analysis_node(prompt, fallback_data, "STARåŸåˆ™")
            
            return {
                "success": True,
                "result": result
            }
            
        except Exception as e:
            logger.error(f"âŒ STARåŸåˆ™æ£€æµ‹å¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e)
            }
    
    async def analyze_basic_parallel(self, resume_id: str, resume_data: Dict[str, Any], 
                                   user_data: Dict[str, Any]) -> Dict[str, Any]:
        """å¹¶è¡Œæ‰§è¡ŒåŸºç¡€åˆ†æï¼ˆSTARæ£€æµ‹ + ç”¨æˆ·ç”»åƒç”Ÿæˆï¼‰"""
        try:
            logger.info(f"ğŸš€ å¯åŠ¨å¹¶è¡ŒåŸºç¡€åˆ†æ: {resume_id}")
            
            # ç”Ÿæˆä»»åŠ¡ID
            star_analysis_id = f"star_analysis_{resume_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            profile_id = f"profile_{resume_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            
            # å¹¶è¡Œæ‰§è¡ŒSTARæ£€æµ‹å’Œç”¨æˆ·ç”»åƒç”Ÿæˆ
            import asyncio
            
            star_task = asyncio.create_task(
                self.analyze_star_principle(resume_id, resume_data, star_analysis_id)
            )
            
            profile_task = asyncio.create_task(
                self._generate_user_profile(profile_id, resume_id, resume_data, user_data)
            )
            
            logger.info(f"âš¡ å¹¶è¡Œä»»åŠ¡å·²å¯åŠ¨: STARæ£€æµ‹ + ç”¨æˆ·ç”»åƒç”Ÿæˆ")
            
            # ç­‰å¾…ä¸¤ä¸ªä»»åŠ¡å®Œæˆ
            star_result, profile_result = await asyncio.gather(star_task, profile_task)
            
            # æ±‡æ€»ç»“æœ
            results = {
                "star_analysis": star_result,
                "user_profile": profile_result,
                "parallel_execution": True,
                "completed_at": datetime.now().isoformat()
            }
            
            # æ£€æŸ¥æ˜¯å¦éƒ½æˆåŠŸ
            all_success = star_result.get("success", False) and profile_result.get("success", False)
            
            if all_success:
                logger.info(f"âœ… å¹¶è¡ŒåŸºç¡€åˆ†æå®Œæˆ: {resume_id}")
                return {
                    "success": True,
                    "results": results,
                    "star_analysis_id": star_analysis_id,
                    "profile_id": profile_id
                }
            else:
                failed_tasks = []
                if not star_result.get("success"):
                    failed_tasks.append("STARæ£€æµ‹")
                if not profile_result.get("success"):
                    failed_tasks.append("ç”¨æˆ·ç”»åƒç”Ÿæˆ")
                
                logger.warning(f"âš ï¸ éƒ¨åˆ†åŸºç¡€åˆ†æå¤±è´¥: {', '.join(failed_tasks)}")
                return {
                    "success": False,
                    "error": f"éƒ¨åˆ†ä»»åŠ¡å¤±è´¥: {', '.join(failed_tasks)}",
                    "results": results
                }
            
        except Exception as e:
            logger.error(f"âŒ å¹¶è¡ŒåŸºç¡€åˆ†æå¤±è´¥: {resume_id} - {e}")
            return {
                "success": False,
                "error": str(e)
            }
    
    async def _generate_user_profile(self, profile_id: str, resume_id: str, 
                                   resume_data: Dict[str, Any], user_data: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆç”¨æˆ·ç”»åƒ"""
        try:
            logger.info(f"ğŸ§  å¼€å§‹ç”Ÿæˆç”¨æˆ·ç”»åƒ: {profile_id}")
            
            basic_info = resume_data.get("basic_info", {})
            target_position = resume_data.get("target_position", "")
            
            # æ„å»ºç”¨æˆ·ç”»åƒæ•°æ®
            profile_data = {
                "profile_id": profile_id,
                "resume_id": resume_id,
                "profile_data": {
                    "basic_info": {
                        "name": user_data.get("user_name") or basic_info.get("name", "ç”¨æˆ·"),
                        "target_position": target_position,
                        "target_field": user_data.get("target_field", "æŠ€æœ¯")
                    },
                    "personalized_welcome": {
                        "greeting": f"æ‚¨å¥½ {user_data.get('user_name', 'æœ‹å‹')}ï¼å¾ˆé«˜å…´åœ¨ä»Šå¤©çš„é¢è¯•ä¸­ä¸æ‚¨ç›¸é‡ã€‚æˆ‘æ³¨æ„åˆ°æ‚¨åº”è˜çš„æ˜¯{target_position}èŒä½ï¼Œç›¸ä¿¡æ‚¨ä¸€å®šæœ‰å¾ˆå¤šç²¾å½©çš„ç»å†è¦åˆ†äº«ã€‚è®©æˆ‘ä»¬å¼€å§‹æ„‰å¿«çš„äº¤æµå§ï¼",
                        "tone": "friendly"
                    },
                    "metadata": {
                        "generated_at": datetime.now().isoformat(),
                        "generation_mode": "parallel_basic_analysis"
                    }
                },
                "status": "completed",
                "created_at": datetime.now().isoformat()
            }
            
            return {
                "success": True,
                "result": profile_data
            }
            
        except Exception as e:
            logger.error(f"âŒ ç”¨æˆ·ç”»åƒç”Ÿæˆå¤±è´¥: {profile_id} - {e}")
            return {
                "success": False,
                "error": str(e)
            }
    
    async def analyze_resume(self, resume_id: str, resume_data: Dict[str, Any], 
                           jd_content: str = "", analysis_id: str = None) -> Dict[str, Any]:
        """æ‰§è¡Œå®Œæ•´çš„ç®€å†åˆ†æå·¥ä½œæµï¼ˆä¿ç•™ç”¨äºå‘åå…¼å®¹ï¼‰"""
        
        if not LANGGRAPH_AVAILABLE:
            logger.warning("LangGraphä¸å¯ç”¨ï¼Œä½¿ç”¨é™çº§åˆ†æ")
            return await self._fallback_analysis(resume_id, resume_data, jd_content, analysis_id)
        
        try:
            # æ„å»ºåˆå§‹çŠ¶æ€
            initial_state = ResumeAnalysisState(
                resume_id=resume_id,
                resume_data=resume_data,
                jd_content=jd_content,
                analysis_id=analysis_id or f"analysis_{resume_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                
                # åˆå§‹åŒ–ç»“æœä¸ºNone
                jd_match_result=None,
                star_principle_result=None,
                health_scan_result=None,
                
                # ç§»é™¤å®ŒæˆçŠ¶æ€ - LangGraphè‡ªåŠ¨å¤„ç†å¹¶è¡Œç­‰å¾…
                jd_completed=False,
                star_completed=False,
                health_completed=False,
                
                final_analysis_report=None,
                error_message=None,
                success=False
            )
            
            logger.info(f"ğŸš€ å¯åŠ¨LangGraphç®€å†åˆ†æå·¥ä½œæµ: {initial_state['analysis_id']}")
            
            # æ‰§è¡Œå·¥ä½œæµ
            config = {"configurable": {"thread_id": initial_state["analysis_id"]}}
            final_state = await self.app.ainvoke(initial_state, config)
            
            if final_state.get("success") and final_state.get("final_analysis_report"):
                logger.info("âœ… LangGraphç®€å†åˆ†æå®Œæˆ")
                return {
                    "success": True,
                    "analysis_id": final_state["analysis_id"],
                    "result": final_state["final_analysis_report"]
                }
            else:
                error_msg = final_state.get("error_message", "æœªçŸ¥é”™è¯¯")
                logger.error(f"âŒ LangGraphåˆ†æå¤±è´¥: {error_msg}")
                return {
                    "success": False,
                    "error": error_msg,
                    "analysis_id": initial_state["analysis_id"]
                }
                
        except Exception as e:
            logger.error(f"âŒ LangGraphå·¥ä½œæµæ‰§è¡Œå¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e),
                "analysis_id": analysis_id or "unknown"
            }
    
    async def _fallback_analysis(self, resume_id: str, resume_data: Dict[str, Any], 
                                jd_content: str, analysis_id: str) -> Dict[str, Any]:
        """é™çº§åˆ†ææ–¹æ³•"""
        logger.info("ğŸ”„ ä½¿ç”¨é™çº§åˆ†ææ¨¡å¼")
        
        try:
            # ç›´æ¥è°ƒç”¨å„ä¸ªåˆ†æèŠ‚ç‚¹
            jd_state = await self._jd_matching_node({
                "resume_data": resume_data,
                "jd_content": jd_content
            })
            
            star_state = await self._star_principle_node({
                "resume_data": resume_data
            })
            
            health_state = await self._health_scan_node({
                "resume_data": resume_data
            })
            
            # æ‰‹åŠ¨æ±‡æ€»ç»“æœ
            combined_state = {
                "resume_id": resume_id,
                "analysis_id": analysis_id,
                "jd_match_result": jd_state["jd_match_result"],
                "star_principle_result": star_state["star_principle_result"],
                "health_scan_result": health_state["health_scan_result"]
            }
            
            report_state = await self._compile_report_node(combined_state)
            
            return {
                "success": True,
                "analysis_id": analysis_id,
                "result": report_state["final_analysis_report"]
            }
            
        except Exception as e:
            logger.error(f"âŒ é™çº§åˆ†æå¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e),
                "analysis_id": analysis_id
            }


# åˆ›å»ºå…¨å±€å·¥ä½œæµå®ä¾‹
_workflow_instance = None

def get_resume_analysis_workflow() -> ResumeAnalysisWorkflow:
    """è·å–ç®€å†åˆ†æå·¥ä½œæµå®ä¾‹"""
    global _workflow_instance
    
    if _workflow_instance is None:
        _workflow_instance = ResumeAnalysisWorkflow()
    
    return _workflow_instance
