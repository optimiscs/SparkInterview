"""
ç®€å†åˆ†æç›¸å…³çš„Celeryä»»åŠ¡
åŒ…æ‹¬JDåŒ¹é…ã€STARæ£€æµ‹ã€å¹¶è¡ŒåŸºç¡€åˆ†æç­‰
"""
import logging
from datetime import datetime
from typing import Dict, Any

from celery import current_task
from src.celery_app import celery_app
from src.data.resume_dao import get_resume_dao
from src.workflows.resume_analysis_workflow import get_resume_analysis_workflow

logger = logging.getLogger(__name__)

@celery_app.task(bind=True, name="src.celery_tasks.analysis_tasks.process_jd_matching_analysis")
def process_jd_matching_analysis(self, jd_analysis_id: str, resume_id: str, resume_data: Dict, jd_content: str):
    """å¼‚æ­¥å¤„ç†JDåŒ¹é…åˆ†æ - Celeryä»»åŠ¡ç‰ˆæœ¬"""
    try:
        logger.info(f"ğŸ” [Celery] å¼€å§‹JDåŒ¹é…åˆ†æ: {jd_analysis_id}")
        
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€
        self.update_state(
            state="PROGRESS",
            meta={"status": "æ­£åœ¨è¿›è¡ŒJDåŒ¹é…åˆ†æ...", "progress": 10}
        )
        
        # ä½¿ç”¨å·¥ä½œæµè¿›è¡ŒJDåŒ¹é…åˆ†æ
        workflow = get_resume_analysis_workflow()
        
        # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦åŒæ­¥è°ƒç”¨ï¼Œå› ä¸ºCeleryä»»åŠ¡æœ¬èº«æ˜¯åœ¨ç‹¬ç«‹è¿›ç¨‹ä¸­
        import asyncio
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        
        try:
            result = loop.run_until_complete(
                workflow.analyze_jd_matching(
                    resume_id=resume_id,
                    resume_data=resume_data,
                    jd_content=jd_content,
                    analysis_id=jd_analysis_id
                )
            )
        finally:
            loop.close()
        
        # æ›´æ–°è¿›åº¦
        self.update_state(
            state="PROGRESS", 
            meta={"status": "æ­£åœ¨ä¿å­˜åˆ†æç»“æœ...", "progress": 80}
        )
        
        # ä¿å­˜ç»“æœåˆ°æ•°æ®åº“
        dao = get_resume_dao()
        if result["success"]:
            # ä¿å­˜JDåŒ¹é…åˆ†æç»“æœ
            analysis_data = {
                "analysis_id": jd_analysis_id,
                "resume_id": resume_id,
                "analysis_type": "jd_matching",
                "status": "completed",
                "jd_matching": result["result"],
                "timestamp": datetime.now().isoformat(),
                "worker_info": {
                    "task_id": self.request.id,
                    "worker_hostname": self.request.hostname,
                    "processed_by": "celery_worker"
                }
            }
            dao.save_analysis(jd_analysis_id, analysis_data)
            
            logger.info(f"âœ… [Celery] JDåŒ¹é…åˆ†æå®Œæˆ: {jd_analysis_id}")
            
            return {
                "status": "completed",
                "analysis_id": jd_analysis_id,
                "result": result["result"],
                "task_id": self.request.id
            }
        else:
            # ä¿å­˜å¤±è´¥çŠ¶æ€
            error_data = {
                "analysis_id": jd_analysis_id,
                "resume_id": resume_id,
                "analysis_type": "jd_matching",
                "status": "failed",
                "error": result.get("error", "æœªçŸ¥é”™è¯¯"),
                "timestamp": datetime.now().isoformat(),
                "worker_info": {
                    "task_id": self.request.id,
                    "worker_hostname": self.request.hostname,
                    "processed_by": "celery_worker"
                }
            }
            dao.save_analysis(jd_analysis_id, error_data)
            
            logger.error(f"âŒ [Celery] JDåŒ¹é…åˆ†æå¤±è´¥: {jd_analysis_id}")
            raise Exception(f"JDåŒ¹é…åˆ†æå¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
        
    except Exception as e:
        logger.error(f"âŒ [Celery] JDåŒ¹é…åˆ†æå¼‚å¸¸: {jd_analysis_id} - {e}")
        
        # ä¿å­˜å¼‚å¸¸çŠ¶æ€
        dao = get_resume_dao()
        error_data = {
            "analysis_id": jd_analysis_id,
            "resume_id": resume_id,
            "analysis_type": "jd_matching",
            "status": "failed",
            "error": str(e),
            "timestamp": datetime.now().isoformat(),
            "worker_info": {
                "task_id": self.request.id,
                "worker_hostname": self.request.hostname,
                "processed_by": "celery_worker"
            }
        }
        dao.save_analysis(jd_analysis_id, error_data)
        
        # é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œè®©Celeryè®°å½•ä»»åŠ¡å¤±è´¥
        raise

@celery_app.task(bind=True, name="src.celery_tasks.analysis_tasks.process_star_analysis")
def process_star_analysis(self, star_analysis_id: str, resume_id: str, resume_data: Dict):
    """å¼‚æ­¥å¤„ç†STARåŸåˆ™æ£€æµ‹ - Celeryä»»åŠ¡ç‰ˆæœ¬"""
    try:
        logger.info(f"â­ [Celery] å¼€å§‹STARåŸåˆ™æ£€æµ‹: {star_analysis_id}")
        
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€
        self.update_state(
            state="PROGRESS",
            meta={"status": "æ­£åœ¨è¿›è¡ŒSTARåŸåˆ™æ£€æµ‹...", "progress": 10}
        )
        
        # ä½¿ç”¨å·¥ä½œæµè¿›è¡ŒSTARåˆ†æ
        workflow = get_resume_analysis_workflow()
        
        import asyncio
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        
        try:
            result = loop.run_until_complete(
                workflow.analyze_star_principle(
                    resume_id=resume_id,
                    resume_data=resume_data,
                    analysis_id=star_analysis_id
                )
            )
        finally:
            loop.close()
        
        # æ›´æ–°è¿›åº¦
        self.update_state(
            state="PROGRESS",
            meta={"status": "æ­£åœ¨ä¿å­˜æ£€æµ‹ç»“æœ...", "progress": 80}
        )
        
        # ä¿å­˜ç»“æœåˆ°æ•°æ®åº“
        dao = get_resume_dao()
        if result["success"]:
            # ä¿å­˜STARæ£€æµ‹ç»“æœ
            analysis_data = {
                "analysis_id": star_analysis_id,
                "resume_id": resume_id,
                "analysis_type": "star_principle",
                "status": "completed",
                "star_principle": result["result"],
                "timestamp": datetime.now().isoformat(),
                "worker_info": {
                    "task_id": self.request.id,
                    "worker_hostname": self.request.hostname,
                    "processed_by": "celery_worker"
                }
            }
            dao.save_analysis(star_analysis_id, analysis_data)
            
            logger.info(f"âœ… [Celery] STARåŸåˆ™æ£€æµ‹å®Œæˆ: {star_analysis_id}")
            
            return {
                "status": "completed",
                "analysis_id": star_analysis_id,
                "result": result["result"],
                "task_id": self.request.id
            }
        else:
            # ä¿å­˜å¤±è´¥çŠ¶æ€
            error_data = {
                "analysis_id": star_analysis_id,
                "resume_id": resume_id,
                "analysis_type": "star_principle",
                "status": "failed",
                "error": result.get("error", "æœªçŸ¥é”™è¯¯"),
                "timestamp": datetime.now().isoformat(),
                "worker_info": {
                    "task_id": self.request.id,
                    "worker_hostname": self.request.hostname,
                    "processed_by": "celery_worker"
                }
            }
            dao.save_analysis(star_analysis_id, error_data)
            
            logger.error(f"âŒ [Celery] STARåŸåˆ™æ£€æµ‹å¤±è´¥: {star_analysis_id}")
            raise Exception(f"STARåŸåˆ™æ£€æµ‹å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
        
    except Exception as e:
        logger.error(f"âŒ [Celery] STARåŸåˆ™æ£€æµ‹å¼‚å¸¸: {star_analysis_id} - {e}")
        
        # ä¿å­˜å¼‚å¸¸çŠ¶æ€
        dao = get_resume_dao()
        error_data = {
            "analysis_id": star_analysis_id,
            "resume_id": resume_id,
            "analysis_type": "star_principle",
            "status": "failed",
            "error": str(e),
            "timestamp": datetime.now().isoformat(),
            "worker_info": {
                "task_id": self.request.id,
                "worker_hostname": self.request.hostname,
                "processed_by": "celery_worker"
            }
        }
        dao.save_analysis(star_analysis_id, error_data)
        
        raise

@celery_app.task(bind=True, name="src.celery_tasks.analysis_tasks.process_parallel_basic_analysis")
def process_parallel_basic_analysis(self, basic_analysis_id: str, resume_id: str, resume_data: Dict, user_data: Dict):
    """å¼‚æ­¥å¤„ç†å¹¶è¡ŒåŸºç¡€åˆ†æ - Celeryä»»åŠ¡ç‰ˆæœ¬"""
    try:
        logger.info(f"ğŸš€ [Celery] å¼€å§‹å¹¶è¡ŒåŸºç¡€åˆ†æ: {basic_analysis_id}")
        
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€
        self.update_state(
            state="PROGRESS",
            meta={"status": "æ­£åœ¨è¿›è¡Œå¹¶è¡ŒåŸºç¡€åˆ†æ...", "progress": 10}
        )
        
        # æ›´æ–°ç®€å†åˆ†æçŠ¶æ€ä¸ºå¤„ç†ä¸­
        dao = get_resume_dao()
        dao.update_analysis_status(resume_id, "PROCESSING")
        
        # æ›´æ–°è¿›åº¦
        self.update_state(
            state="PROGRESS",
            meta={"status": "æ­£åœ¨æ‰§è¡ŒSTARæ£€æµ‹å’Œç”¨æˆ·ç”»åƒç”Ÿæˆ...", "progress": 30}
        )
        
        # ä½¿ç”¨å·¥ä½œæµè¿›è¡Œå¹¶è¡ŒåŸºç¡€åˆ†æ
        workflow = get_resume_analysis_workflow()
        
        import asyncio
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        
        try:
            result = loop.run_until_complete(
                workflow.analyze_basic_parallel(
                    resume_id=resume_id,
                    resume_data=resume_data,
                    user_data=user_data
                )
            )
        finally:
            loop.close()
        
        # æ›´æ–°è¿›åº¦
        self.update_state(
            state="PROGRESS",
            meta={"status": "æ­£åœ¨ä¿å­˜åˆ†æç»“æœ...", "progress": 80}
        )
        
        # ä¿å­˜ç»“æœåˆ°æ•°æ®åº“
        if result["success"]:
            # åˆ†åˆ«ä¿å­˜STARåˆ†æå’Œç”¨æˆ·ç”»åƒç»“æœ
            results = result["results"]
            
            # ä¿å­˜STARåˆ†æç»“æœ
            if results["star_analysis"]["success"]:
                star_data = {
                    "analysis_id": result.get("star_analysis_id"),
                    "resume_id": resume_id,
                    "analysis_type": "star_principle",
                    "status": "completed",
                    "star_principle": results["star_analysis"]["result"],
                    "timestamp": datetime.now().isoformat(),
                    "worker_info": {
                        "task_id": self.request.id,
                        "worker_hostname": self.request.hostname,
                        "processed_by": "celery_worker"
                    }
                }
                dao.save_analysis(result.get("star_analysis_id"), star_data)
            
            # ä¿å­˜ç”¨æˆ·ç”»åƒç»“æœ
            if results["user_profile"]["success"]:
                profile_data = results["user_profile"]["result"]
                profile_data["worker_info"] = {
                    "task_id": self.request.id,
                    "worker_hostname": self.request.hostname,
                    "processed_by": "celery_worker"
                }
                dao.save_profile(result.get("profile_id"), profile_data)
            
            # æ›´æ–°ç®€å†åˆ†æçŠ¶æ€ä¸ºå®Œæˆ
            dao.update_analysis_status(resume_id, "COMPLETED")
            
            logger.info(f"âœ… [Celery] å¹¶è¡ŒåŸºç¡€åˆ†æå®Œæˆ: {basic_analysis_id}")
            
            return {
                "status": "completed", 
                "basic_analysis_id": basic_analysis_id,
                "star_analysis_id": result.get("star_analysis_id"),
                "profile_id": result.get("profile_id"),
                "task_id": self.request.id
            }
        else:
            # ä¿å­˜å¤±è´¥çŠ¶æ€
            dao.update_analysis_status(resume_id, "FAILED")
            logger.error(f"âŒ [Celery] å¹¶è¡ŒåŸºç¡€åˆ†æå¤±è´¥: {basic_analysis_id}")
            
            raise Exception(f"å¹¶è¡ŒåŸºç¡€åˆ†æå¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
        
    except Exception as e:
        logger.error(f"âŒ [Celery] å¹¶è¡ŒåŸºç¡€åˆ†æå¼‚å¸¸: {basic_analysis_id} - {e}")
        
        # ä¿å­˜å¼‚å¸¸çŠ¶æ€
        dao = get_resume_dao()
        dao.update_analysis_status(resume_id, "FAILED")
        
        raise

# ä»»åŠ¡çŠ¶æ€æŸ¥è¯¢è¾…åŠ©å‡½æ•°
def get_task_info(task_id: str) -> Dict[str, Any]:
    """è·å–Celeryä»»åŠ¡ä¿¡æ¯"""
    try:
        from celery.result import AsyncResult
        task = AsyncResult(task_id, app=celery_app)
        
        return {
            "task_id": task_id,
            "status": task.status,
            "result": task.result,
            "info": task.info,
            "ready": task.ready(),
            "successful": task.successful() if task.ready() else False,
            "failed": task.failed() if task.ready() else False
        }
    except Exception as e:
        logger.error(f"è·å–ä»»åŠ¡ä¿¡æ¯å¤±è´¥: {task_id} - {e}")
        return {
            "task_id": task_id,
            "status": "UNKNOWN",
            "error": str(e)
        }
